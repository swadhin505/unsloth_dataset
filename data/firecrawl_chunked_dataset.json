{
  "data": [
    {
      "id": "3b83ae7f-fb36-438b-8d7e-54055a8d0b82",
      "source": "firecrawl\\docs\\.md",
      "content": "---\ntitle: Welcome | Unsloth Documentation\nurl: https://docs.unsloth.ai/\n---  \n[Unsloth](https://github.com/unslothai/unsloth) makes finetuning large language models like Llama-3, Mistral, Phi-4 and Gemma 2x faster, use 70% less memory, and with no degradation in accuracy!  \nOur docs will guide you through training your own custom model. It covers the essentials of [installing & updating](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-5.-installing--requirements) Unsloth, [creating datasets](https://docs.unsloth.ai/basics/datasets-101), running & [deploying](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-5.-running--saving-the-model) your model.",
      "metadata": {
        "title": "Welcome | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/"
      }
    },
    {
      "id": "73da547f-afe0-4c58-947a-e9ef4bc443e3",
      "source": "firecrawl\\docs\\.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/#get-started) [Get started](https://docs.unsloth.ai/get-started/beginner-start-here)\n\n[üí°Reasoning - GRPO & RL](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl) [‚ö°Tutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo) [üß¨Fine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide) [üìíUnsloth Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)",
      "metadata": {
        "title": "Welcome | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/"
      }
    },
    {
      "id": "7abd5303-b88c-4537-aff4-7e2669963371",
      "source": "firecrawl\\docs\\.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/#quickstart) Quickstart\n\n**Install locally with pip (recommended)** for Linux devices:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install unsloth\n```  \nFor Windows install instructions, see [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).",
      "metadata": {
        "title": "Welcome | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/"
      }
    },
    {
      "id": "d3cb8286-8024-4263-84a9-54cc0abf2bac",
      "source": "firecrawl\\docs\\.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/#what-is-finetuning-and-why) What is finetuning and why?\n\nFine-tuning an LLM customizes its behavior, enhances domain knowledge, and optimizes performance for specific tasks. Finetuning is the process of updating the actual \"brains\" of the language model through some process called back-propagation.  \nBy fine-tuning a pre-trained model (e.g. Llama-3.1-8B) on a specialized dataset, you can:  \n- **Update Knowledge**: Introduce new domain-specific information.  \n- **Customize Behavior**: Adjust the model‚Äôs tone, personality, or response style.  \n- **Optimize for Tasks**: Improve accuracy and relevance for specific use cases.  \n**Example usecases**:  \n- Train LLM to predict if a headline impacts a company positively or negatively.  \n- Use historical customer interactions for more accurate and custom responses.  \n- Fine-tune LLM on legal texts for contract analysis, case law research, and compliance.  \nYou can think of a fine-tuned model as a specialized agent designed to do specific tasks more effectively and efficiently. **Fine-tuning can replicate all of RAG's capabilities**, but not vice versa.  \n[ü§îFAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me)",
      "metadata": {
        "title": "Welcome | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/"
      }
    },
    {
      "id": "dd48c5dd-14b4-46c9-aca0-1817275f72c3",
      "source": "firecrawl\\docs\\.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/#how-to-use-unsloth) How to use Unsloth?\n\n[Unsloth](https://github.com/unslothai/unsloth) can be installed locally via Linux, Windows, Kaggle, or another GPU service like Google Colab. Most use Unsloth through the interface Google Colab which provides a free GPU to train with.  \n[üì•Installing + Updating](https://docs.unsloth.ai/get-started/installing-+-updating) [üõ†Ô∏èUnsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)  \n!  \n[NextBeginner? Start here!](https://docs.unsloth.ai/get-started/beginner-start-here)  \nLast updated 2 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Welcome | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/"
      }
    },
    {
      "id": "a4d3c425-a2b1-476d-a7e9-5da54d3ac6c2",
      "source": "firecrawl\\docs\\basics-chat-templates.md",
      "content": "---\ntitle: Chat Templates | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/chat-templates\n---",
      "metadata": {
        "title": "Chat Templates | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/chat-templates"
      }
    },
    {
      "id": "d114f363-f4d9-4900-b43d-2ff9f8b73f16",
      "source": "firecrawl\\docs\\basics-chat-templates.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/chat-templates#list-of-colab-chat-template-notebooks) List of Colab chat template notebooks:\n\n- [Conversational](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)  \n- [ChatML](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)  \n- [Ollama](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)  \n- [Text Classification](https://github.com/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb) by Timotheeee  \n- [Multiple Datasets](https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing) by Flail",
      "metadata": {
        "title": "Chat Templates | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/chat-templates"
      }
    },
    {
      "id": "07df7b32-e16c-423c-a0ab-7fc7e6d1b8c5",
      "source": "firecrawl\\docs\\basics-chat-templates.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/chat-templates#multi-turn-conversations) Multi turn conversations\n\nA bit issue if you didn't notice is the Alpaca dataset is single turn, whilst remember using ChatGPT was interactive and you can talk to it in multiple turns. For example, the left is what we want, but the right which is the Alpaca dataset only provides singular conversations. We want the finetuned language model to somehow learn how to do multi turn conversations just like ChatGPT.  \n!  \nSo we introduced the `conversation_extension` parameter, which essentially selects some random rows in your single turn dataset, and merges them into 1 conversation! For example, if you set it to 3, we randomly select 3 rows and merge them into 1! Setting them too long can make training slower, but could make your chatbot and final finetune much better!  \n!  \nThen set `output_column_name` to the prediction / output column. For the Alpaca dataset dataset, it would be the output column.  \nWe then use the `standardize_sharegpt` function to just make the dataset in a correct format for finetuning! Always call this!  \n!",
      "metadata": {
        "title": "Chat Templates | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/chat-templates"
      }
    },
    {
      "id": "f944a2b2-c2be-44d8-8898-e075f3c0937d",
      "source": "firecrawl\\docs\\basics-chat-templates.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/chat-templates#customizable-chat-templates) Customizable Chat Templates\n\nWe can now specify the chat template for finetuning itself. The very famous Alpaca format is below:  \n!  \nBut remember we said this was a bad idea because ChatGPT style finetunes require only 1 prompt? Since we successfully merged all dataset columns into 1 using Unsloth, we essentially can create the below style chat template with 1 input column (instruction) and 1 output:  \n!  \nWe just require you must put a `{INPUT}` field for the instruction and an `{OUTPUT}` field for the model's output field. We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT. For example, below are some cool examples which you can customize the chat template to be:  \n!  \nFor the ChatML format used in OpenAI models:  \n!  \nOr you can use the Llama-3 template itself (which only functions by using the instruct version of Llama-3): We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT.  \n!  \nOr in the Titanic prediction task where you had to predict if a passenger died or survived in this Colab notebook which includes CSV and Excel uploading: [https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)  \n!",
      "metadata": {
        "title": "Chat Templates | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/chat-templates"
      }
    },
    {
      "id": "78a18b1f-3e5c-490d-b7fd-a6ec6be62458",
      "source": "firecrawl\\docs\\basics-chat-templates.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/chat-templates#more-information) More Information\n\nAssuming your dataset is a list of list of dictionaries like the below:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n[\\\n[{'from': 'human', 'value': 'Hi there!'},\\\n{'from': 'gpt', 'value': 'Hi how can I help?'},\\\n{'from': 'human', 'value': 'What is 2+2?'}],\\\n[{'from': 'human', 'value': 'What's your name?'},\\\n{'from': 'gpt', 'value': 'I'm Daniel!'},\\\n{'from': 'human', 'value': 'Ok! Nice!'},\\\n{'from': 'gpt', 'value': 'What can I do for you?'},\\\n{'from': 'human', 'value': 'Oh nothing :)'},],\\\n]\n```  \nYou can use our `get_chat_template` to format it. Select `chat_template` to be any of `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth`, and use `mapping` to map the dictionary values `from`, `value` etc. `map_eos_token` allows you to map `<|im_end|>` to EOS without any training.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nfrom unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\ntokenizer,\nchat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\nmapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\nmap_eos_token = True, # Maps <|im_end|> to </s> instead\n)\n\ndef formatting_prompts_func(examples):\nconvos = examples[\"conversations\"]\ntexts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\nreturn { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n```  \nYou can also make your own custom chat templates! For example our internal chat template we use is below. You must pass in a `tuple` of `(custom_template, eos_token)` where the `eos_token` must be used inside the template.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nunsloth_template = \\\n\"{{ bos_token }}\"\\\n\"{{ 'You are a helpful assistant to the usern' }}\"\\\n\"</div>\"\\\n\"<div data-gb-custom-block data-tag=\"for\">\"\\\n\"<div data-gb-custom-block data-tag=\"if\" data-0='role' data-1='role' data-2='] == ' data-3='user'>\"\\\n\"{{ '>>> User: ' + message['content'] + 'n' }}\"\\\n\"<div data-gb-custom-block data-tag=\"elif\" data-0='role' data-1='role' data-2='] == ' data-3='assistant'></div>\"\\\n\"{{ '>>> Assistant: ' + message['content'] + eos_token + 'n' }}\"\\\n\"</div>\"\\\n\"</div>\"\\\n\"<div data-gb-custom-block data-tag=\"if\">\"\\\n\"{{ '>>> Assistant: ' }}\"\\\n\"</div>\"\nunsloth_eos_token = \"eos_token\"\n\ntokenizer = get_chat_template(\ntokenizer,\nchat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\nmapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\nmap_eos_token = True, # Maps <|im_end|> to </s> instead\n)\n```  \n[PreviousContinued Pretraining](https://docs.unsloth.ai/basics/continued-pretraining) [NextVision Fine-tuning](https://docs.unsloth.ai/basics/vision-fine-tuning)  \nLast updated 25 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Chat Templates | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/chat-templates"
      }
    },
    {
      "id": "7b21acca-4d4b-4d2a-956c-59148e7d30d1",
      "source": "firecrawl\\docs\\basics-continued-pretraining.md",
      "content": "---\ntitle: Continued Pretraining | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/continued-pretraining\n---  \n- The [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining/raw text.  \n- The [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language.  \nYou can read more about continued pretraining and our release in our [blog post](https://unsloth.ai/blog/contpretraining).",
      "metadata": {
        "title": "Continued Pretraining | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/continued-pretraining"
      }
    },
    {
      "id": "9d7b7212-2c16-4be8-a6e8-eab894b7b7b2",
      "source": "firecrawl\\docs\\basics-continued-pretraining.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining#what-is-continued-pretraining) What is Continued Pretraining?\n\nContinued or continual pretraining (CPT) is necessary to ‚Äústeer‚Äù the language model to understand new domains of knowledge, or out of distribution domains. Base models like Llama-3 8b or Mistral 7b are first pretrained on gigantic datasets of trillions of tokens (Llama-3 for e.g. is 15 trillion).  \nBut sometimes these models have not been well trained on other languages, or text specific domains, like law, medicine or other areas. So continued pretraining (CPT) is necessary to make the language model learn new tokens or datasets.",
      "metadata": {
        "title": "Continued Pretraining | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/continued-pretraining"
      }
    },
    {
      "id": "5118a632-1f45-4c6c-908f-fa4932ed5eb3",
      "source": "firecrawl\\docs\\basics-continued-pretraining.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining#advanced-features) Advanced Features: > [Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining#loading-lora-adapters-for-continued-finetuning) Loading LoRA adapters for continued finetuning\n\nIf you saved a LoRA adapter through Unsloth, you can also continue training using your LoRA weights. The optimizer state will be reset as well. To load even optimizer states to continue finetuning, see the next section.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name = \"LORA_MODEL_NAME\",\nmax_seq_length = max_seq_length,\ndtype = dtype,\nload_in_4bit = load_in_4bit,\n)\ntrainer = Trainer(...)\ntrainer.train()\n```",
      "metadata": {
        "title": "Continued Pretraining | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/continued-pretraining"
      }
    },
    {
      "id": "9b996bdd-9e5d-4b61-81d3-577fd318f936",
      "source": "firecrawl\\docs\\basics-continued-pretraining.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining#advanced-features) Advanced Features: > [Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining#continued-pretraining-and-finetuning-the-lm_head-and-embed_tokens-matrices) Continued Pretraining & Finetuning the `lm_head` and `embed_tokens` matrices\n\nAdd `lm_head` and `embed_tokens`. For Colab, sometimes you will go out of memory for Llama-3 8b. If so, just add `lm_head`.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel = FastLanguageModel.get_peft_model(\nmodel,\nr = 16,\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\\n\"gate_proj\", \"up_proj\", \"down_proj\",\\\n\"lm_head\", \"embed_tokens\",],\nlora_alpha = 16,\n)\n```  \nThen use 2 different learning rates - a 2-10x smaller one for the `lm_head` or `embed_tokens` like so:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n....\nargs = UnslothTrainingArguments(\n....\nlearning_rate = 5e-5,\nembedding_learning_rate = 5e-6, # 2-10x smaller than learning_rate\n),\n)\n```  \n[PreviousInference](https://docs.unsloth.ai/basics/running-and-saving-models/inference) [NextChat Templates](https://docs.unsloth.ai/basics/chat-templates)  \nLast updated 15 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Continued Pretraining | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/continued-pretraining"
      }
    },
    {
      "id": "5a467b97-7d2a-4e2d-a741-2fcd6899b4b7",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "---\ntitle: Datasets 101 | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/datasets-101\n---",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "64dff386-8a72-43bd-a7bf-78a61aebb5c1",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#what-is-a-dataset) What is a Dataset?\n\nFor LLMs, datasets are collections of data that can be used to train our models. In order to be useful for training, text data needs to be in a format that can be tokenized.",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "bf337acd-0443-4374-9a38-018bb03d5243",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#tokenization) Tokenization\n\nTokenization is the process of breaking text into units called **tokens.** These units can be representative of words, sub-words or even characters.  \nThe typical approach to tokenization with large language models is to tokenize text into sub-word chunks, this allows models to handle inputs that are out of vocabulary.  \nDuring training, tokens are embedded in a high-dimensional latent space. By utilizing attention mechanisms, the model fine-tunes these embeddings to produce contextually relevant outputs.  \n**In Summary:** Tokenization turns raw text into a format that is both machine-readable and retains meaningful information.",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "6b593e0f-4c4c-44cb-b054-37476c1327f3",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#data-format) Data Format\n\nTo enable the process of tokenization, datasets need to be in a format that can be read by a tokenizer.  \nFormat  \nDescription  \nTraining Type  \nRaw Corpus  \nRaw text from a source such as a website, book, or article.  \nContinued Pretraining  \nInstruct  \nInstructions for the model to follow and an example of the output to aim for.  \nSupervised fine-tuning (SFT)  \nConversation  \nMultiple-turn conversation between a user and an AI assistant.  \nSupervised fine-tuning (SFT)  \nRLHF  \nConversation between a user and an AI assistant, with the assistant's responses being ranked by a human evaluator.  \nReinforcement Learning  \nIt's worth noting that different styles of format exist for each of these types.",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "1ff5671e-ef10-4d0e-a4a8-fbabd0102ad6",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#getting-started) Getting Started\n\nBefore we format our data, we want to identify the following:  \n1  \nPurpose of dataset  \nKnowing the purpose of the dataset will help us determine what data we need and format to use.  \nThe purpose could be, adapting a model to a new task such as summarization or improving a model's ability to role-play a specific character.  \n2  \nStyle of output  \nThe style of output will let us know what sources of data we will use to reach our desired output.  \nFor example, the type of output you want to achieve could be JSON, HTML, text or code. Or perhaps you want it to be Spanish, English or German etc.  \n3  \nData source  \nWhen we know the purpose and style of the data we need, we can look for a data source to collect our data from.  \nThe Source of data can be a CSV file, PDF or even a website. You can also synthetically generate data but extra care is required to make sure each example is high quality and relevant.",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "a18c971d-27dd-4719-9c32-e0819ef25ac3",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#formatting-our-data) Formatting Our Data\n\nWhen we have identified the relevant criteria, and collected the necessary data, we can then format our data into a machine readable format that is ready for training.  \nFor continued pretraining, we use raw text format without specific structure:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n\"text\": \"Pasta carbonara is a traditional Roman pasta dish. The sauce is made by mixing raw eggs with grated Pecorino Romano cheese and black pepper. The hot pasta is then tossed with crispy guanciale (cured pork cheek) and the egg mixture, creating a creamy sauce from the residual heat. Despite popular belief, authentic carbonara never contains cream or garlic. The dish likely originated in Rome in the mid-20th century, though its exact origins are debated...\"\n```  \nThis format preserves natural language flow and allows the model to learn from continuous text.  \nIf we are adapting a model to a new task, and intend for the model to output text in a single turn based on a specific set of instructions, we can use **Instruction** format in [Alpaca style](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-6.-alpaca-dataset)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n\"Instruction\": \"Task we want the model to perform.\"\n\n\"Input\": \"Optional, but useful, it will essentially be the user's query.\"\n\n\"Output\": \"The expected result of the task and the output of the model.\"\n```  \nWhen we want multiple turns of conversation we can use sharegpt **conversational** format  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n{\n\"conversations\": [\\\n{\\\n\"from\": \"human\",\\\n\"value\": \"Can you help me make pasta carbonara?\"\\\n},\\\n{\\\n\"from\": \"assistant\",\\\n\"value\": \"Would you like the traditional Roman recipe, or a simpler version?\"\\\n},\\\n{\\\n\"from\": \"human\",\\\n\"value\": \"The traditional version please\"\\\n},\\\n{\\\n\"from\": \"assistant\",\\\n\"value\": \"The authentic Roman carbonara uses just a few ingredients: pasta, guanciale, eggs, Pecorino Romano, and black pepper. Would you like the detailed recipe?\"\\\n}\\\n]\n}\n```  \nEach message alternates between `human` and `assistant`, allowing for natural dialogue flow.  \n**Q:**How can I format my raw data into Alpaca instruct?  \n**A:** One approach is to create a Python script to process your raw data. If you're working on a summarization task, you can use a local LLM to generate instructions and outputs for each example.",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "d2cc6137-406e-4b47-a9c0-7c9196d1f19c",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#how-big-should-my-dataset-be) How big should my dataset be?\n\nWe generally recommend using at least 100 rows of data for fine-tuning to achieve reasonable results. For optimal performance, a dataset with over 300 rows is preferable, and in this case, more data usually leads to better outcomes. However, the effectiveness of your fine-tuned model depends heavily on the quality of the dataset, so be sure to thoroughly clean and prepare your data.",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "d26bbe47-a41b-45ed-a830-7266c22732f6",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#multiple-datasets) Multiple datasets\n\nIf you have multiple datasets for fine-tuning, you can either:  \n- Standardize the format of all datasets, combine them into a single dataset, and fine-tune on this unified dataset.  \n- Use the [Multiple Datasets](https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing) notebook to fine-tune on multiple datasets directly.",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "5b1edc86-f2e6-4559-a3d5-7906e6eb03f0",
      "source": "firecrawl\\docs\\basics-datasets-101.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#multiple-datasets) Multiple datasets > [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101#can-i-fine-tune-the-same-model-multiple-times) Can I fine-tune the same model multiple times?\n\nYou can fine-tune an already fine-tuned model multiple times, but it's best to combine all the datasets and perform the fine-tuning in a single process instead. Training an already fine-tuned model can potentially alter the quality and knowledge acquired during the previous fine-tuning process.  \n[PreviousTutorial: How to Finetune Llama-3 and Use In Ollama](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama) [NextRunning & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models)  \nLast updated 25 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Datasets 101 | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/datasets-101"
      }
    },
    {
      "id": "0dd6de2a-5db4-4b81-9142-6ef00b98be39",
      "source": "firecrawl\\docs\\basics-errors-troubleshooting.md",
      "content": "---\ntitle: Errors/Troubleshooting | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/errors-troubleshooting\n---",
      "metadata": {
        "title": "Errors/Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/errors-troubleshooting"
      }
    },
    {
      "id": "ad5a5a28-edb6-49d2-a42e-e2c58a9a3f54",
      "source": "firecrawl\\docs\\basics-errors-troubleshooting.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting#running-in-unsloth-works-well-but-after-exporting-and-running-on-other-platforms-the-results-are-poo) Running in Unsloth works well, but after exporting & running on other platforms, the results are poor\n\nYou might sometimes encounter an issue where your model runs and produces good results on Unsloth, but when you use it on another platform like Ollama or vLLM, the results are poor or you might get gibberish, endless/infinite generations _or_ repeated outputs **.**  \n- The most common cause of this error is using an incorrect chat template. It‚Äôs essential to use the SAME chat template that was used when training the model in Unsloth and later when you run it in another framework, such as llama.cpp or Ollama. When inferencing from a saved model, it's crucial to apply the correct template.  \n- It might also be because your inference engine adds an unnecessary \"start of sequence\" token (or the lack of thereof on the contrary) so ensure you check both hypotheses!",
      "metadata": {
        "title": "Errors/Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/errors-troubleshooting"
      }
    },
    {
      "id": "73719449-b647-4500-b943-50dfde848173",
      "source": "firecrawl\\docs\\basics-errors-troubleshooting.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting#saving-to-gguf-vllm-16bit-crashes) Saving to GGUF / vLLM 16bit crashes\n\nYou can try reducing the maximum GPU usage during saving by changing `maximum_memory_usage`.  \nThe default is `model.save_pretrained(..., maximum_memory_usage = 0.75)`. Reduce it to say 0.5 to use 50% of GPU peak memory or lower. This can reduce OOM crashes during saving.",
      "metadata": {
        "title": "Errors/Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/errors-troubleshooting"
      }
    },
    {
      "id": "eb2e9458-983a-42e4-a61e-b1ecbfb21335",
      "source": "firecrawl\\docs\\basics-errors-troubleshooting.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting#evaluation-loop-also-oom-or-crashing) Evaluation Loop - also OOM or crashing.\n\nA common issue when you OOM is because you set your batch size too high. Set it lower than 3 to use less VRAM.  \nFirst split your training dataset into a train and test split. Set the trainer settings for evaluation to:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nnew_dataset = dataset.train_test_split(test_size = 0.01)\nSFTTrainer(\nargs = TrainingArguments(\nfp16_full_eval = True,\nper_device_eval_batch_size = 2,\neval_accumulation_steps = 4,\neval_strategy = \"steps\",\neval_steps = 1,\n),\ntrain_dataset = new_dataset[\"train\"],\neval_dataset = new_dataset[\"test\"],\n```  \nThis will cause no OOMs and make it somewhat faster with no upcasting to float32. Validation set.",
      "metadata": {
        "title": "Errors/Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/errors-troubleshooting"
      }
    },
    {
      "id": "52a36e2d-47c3-460c-9b9b-6f768fd8bbbf",
      "source": "firecrawl\\docs\\basics-errors-troubleshooting.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting#notimplementederror-a-utf-8-locale-is-required.-got-ansi) NotImplementedError: A UTF-8 locale is required. Got ANSI\n\nSee https://github.com/googlecolab/colabtools/issues/3409  \nIn a new cell, run the below:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n```  \n[PreviousFinetuning from Last Checkpoint](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint) [NextUnsloth Benchmarks](https://docs.unsloth.ai/basics/unsloth-benchmarks)  \nLast updated 15 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Errors/Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/errors-troubleshooting"
      }
    },
    {
      "id": "7b280c05-b969-4a83-8a8c-b01c4d2604de",
      "source": "firecrawl\\docs\\basics-finetuning-from-last-checkpoint.md",
      "content": "---\ntitle: Finetuning from Last Checkpoint | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint\n---  \nYou must edit the `Trainer` first to add `save_strategy` and `save_steps`. Below saves a checkpoint every 50 steps to the folder `outputs`.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ntrainer = SFTTrainer(\n....\nargs = TrainingArguments(\n....\noutput_dir = \"outputs\",\nsave_strategy = \"steps\",\nsave_steps = 50,\n),\n)\n```  \nThen in the trainer do:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ntrainer_stats = trainer.train(resume_from_checkpoint = True)\n```  \nWhich will start from the latest checkpoint and continue training.",
      "metadata": {
        "title": "Finetuning from Last Checkpoint | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint"
      }
    },
    {
      "id": "33359439-b3df-421f-bcc5-7252ba578a1c",
      "source": "firecrawl\\docs\\basics-finetuning-from-last-checkpoint.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint#wandb-integration) Wandb Integration\n\nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n# Install library\n!pip install wandb --upgrade\n\n# Setting up Wandb\n!wandb login <token>\n\nimport os\n\nos.environ[\"WANDB_PROJECT\"] = \"<name>\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n```  \nThen in `TrainingArguments()` set  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nreport_to = \"wandb\",\nlogging_steps = 1, # Change if needed\nsave_steps = 100 # Change if needed\nrun_name = \"<name>\" # (Optional)\n```  \nTo train the model, do `trainer.train()`; to resume training, do  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nimport wandb\nrun = wandb.init()\nartifact = run.use_artifact('<username>/<Wandb-project-name>/<run-id>', type='model')\nartifact_dir = artifact.download()\ntrainer.train(resume_from_checkpoint=artifact_dir)\n```  \n[PreviousVision Fine-tuning](https://docs.unsloth.ai/basics/vision-fine-tuning) [NextErrors/Troubleshooting](https://docs.unsloth.ai/basics/errors-troubleshooting)  \nLast updated 8 months ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Finetuning from Last Checkpoint | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint"
      }
    },
    {
      "id": "8a4edd03-89c7-4f3b-a562-93dd8e3b3417",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-reinforcement-learning-dpo-orpo-and-kto.md",
      "content": "---\ntitle: Reinforcement Learning - DPO, ORPO & KTO | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto\n---  \nDPO (Direct Preference Optimization), ORPO (Odds Ratio Preference Optimization), PPO, KTO Reward Modelling all work with Unsloth.  \nWe have Google Colab notebooks for reproducing ORPO, DPO Zephyr, KTO and SimPO:  \n- [ORPO notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb)  \n- [DPO Zephyr notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)  \n- [KTO notebook](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing)  \n- [SimPO notebook](https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing)  \nWe're also in ü§óHugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).",
      "metadata": {
        "title": "Reinforcement Learning - DPO, ORPO & KTO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto"
      }
    },
    {
      "id": "a0b16c28-a017-488b-a0a6-c9b1ead10643",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-reinforcement-learning-dpo-orpo-and-kto.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto#dpo-code) DPO Code\n\nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npython\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Optional set GPU device ID\n\nfrom unsloth import FastLanguageModel, PatchDPOTrainer\nfrom unsloth import is_bfloat16_supported\nPatchDPOTrainer()\nimport torch\nfrom transformers import TrainingArguments\nfrom trl import DPOTrainer\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name = \"unsloth/zephyr-sft-bnb-4bit\",\nmax_seq_length = max_seq_length,\ndtype = None,\nload_in_4bit = True,\n)\n\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\nmodel,\nr = 64,\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\\n\"gate_proj\", \"up_proj\", \"down_proj\",],\nlora_alpha = 64,\nlora_dropout = 0, # Supports any, but = 0 is optimized\nbias = \"none\", # Supports any, but = \"none\" is optimized\n# [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\nuse_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\nrandom_state = 3407,\nmax_seq_length = max_seq_length,\n)\n\ndpo_trainer = DPOTrainer(\nmodel = model,\nref_model = None,\nargs = TrainingArguments(\nper_device_train_batch_size = 4,\ngradient_accumulation_steps = 8,\nwarmup_ratio = 0.1,\nnum_train_epochs = 3,\nfp16 = not is_bfloat16_supported(),\nbf16 = is_bfloat16_supported(),\nlogging_steps = 1,\noptim = \"adamw_8bit\",\nseed = 42,\noutput_dir = \"outputs\",\n),\nbeta = 0.1,\ntrain_dataset = YOUR_DATASET_HERE,\n# eval_dataset = YOUR_DATASET_HERE,\ntokenizer = tokenizer,\nmax_length = 1024,\nmax_prompt_length = 512,\n)\ndpo_trainer.train()\n```  \n[PreviousTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo) [NextTutorial: How to Run DeepSeek-R1 Locally](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally)  \nLast updated 14 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Reinforcement Learning - DPO, ORPO & KTO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto"
      }
    },
    {
      "id": "a3405b63-3aff-446b-8696-a87c5a75da9c",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "---\ntitle: Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\n---  \nDeepSeek developed [GRPO](https://unsloth.ai/blog/grpo) (Group Relative Policy Optimization) to train their R1 reasoning models.",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "92a5fc85-1708-4e66-bc3e-9ca11feaee60",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart\n\nThese instructions are for our pre-made Google Colab [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks). If you are installing Unsloth locally, you can also copy our notebooks inside your favorite code editor.",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "d4091f03-1638-4285-8a21-b48c8725321f",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#the-grpo-notebooks-we-are-using-llama-3.1-8b-phi-4-14b-and-qwen2.5-3b) The GRPO notebooks we are using: [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb), [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb) and [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb)\n\n1",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "a7fa96d4-9e45-4324-841f-a2e89ffd2edd",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#install-unsloth) Install Unsloth\n\nIf you're using our Colab notebook, click **Runtime > Run all**. We'd highly recommend you checking out our [Fine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide) before getting started.  \nIf installing locally, ensure you have the correct [requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements) and use `pip install unsloth ` on Linux or follow our [Windows install](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation) instructions.  \n!  \n2",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "239b0d67-a2f6-49b2-b553-d26e03a74d86",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#learn-about-grpo-and-reward-functions) Learn about GRPO & Reward Functions\n\nBefore we get started, it is recommended to learn more about GRPO, reward functions and how they work. Read more about them including [tips & tricks](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#basics-tips) [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#basics-tips).  \nYou will also need enough VRAM. In general, model parameters = amount of VRAM you will need. In Colab, we are using their free 16GB VRAM GPUs which can train any model up to 16B in parameters.  \n3",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "a0dee673-f9af-4a13-95da-801c9a2330e4",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#configure-desired-settings) Configure desired settings\n\nWe have pre-selected optimal settings for the best results for you already and you can change the model to whichever you want listed in our [supported models](https://docs.unsloth.ai/get-started/all-our-models). Would not recommend changing other settings if you're a beginner.  \n!  \n4",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "7337034e-16b7-404d-94c2-2642fba6af8e",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#select-your-dataset) Select your dataset\n\nWe have pre-selected OpenAI's [GSM8K](https://huggingface.co/datasets/openai/gsm8k) dataset already but you could change it to your own or any public one on Hugging Face. You can read more about [datasets here](https://docs.unsloth.ai/basics/datasets-101).  \nYour dataset should still have at least 2 columns for question and answer pairs. However the answer must not reveal the reasoning behind how it derived the answer from the question. See below for an example:  \n!  \n5",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "6a367dc3-721e-4cc5-97b3-b16586d1a054",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#reward-functions-verifier) Reward Functions/Verifier\n\n[Reward Functions/Verifiers](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) lets us know if the model is doing well or not according to the dataset you have provided. Each generation run will be assessed on how it performs to the score of the average of the rest of generations. You can create your own reward functions however we have already pre-selected them for you with [Will's GSM8K](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions) reward functions. With this, we have 5 different ways which we can reward each generation.  \nYou can input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, feed your generations into a LLM of your choice and set a rule: \"If the answer sounds too robotic, deduct 3 points.\" This helps refine outputs based on quality criteria. **See examples** of what they can look like [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples).  \n**Example Reward Function for an Email Automation Task:**  \n- **Question:** Inbound email  \n- **Answer:** Outbound email  \n- **Reward Functions:**  \n- If the answer contains a required keyword ‚Üí **+1**  \n- If the answer exactly matches the ideal response ‚Üí **+1**  \n- If the response is too long ‚Üí **-1**  \n- If the recipient's name is included ‚Üí **+1**  \n- If a signature block (phone, email, address) is present ‚Üí **+1**  \n!  \n6",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "fdddd519-f9d3-4f97-ac9d-0b930c9b5107",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#train-your-model) Train your model\n\nWe have pre-selected hyperparameters for the most optimal results however you could change them. Read all about [parameters here](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia).  \n!  \nYou should see the reward increase overtime. We would recommend you train for at least 300 steps which may take 30 mins however, for optimal results, you should train for longer.  \nYou will also see sample answers which allows you to see how the model is learning. Some may have steps, XML tags, attempts etc. and the idea is as trains it's going to get better and better because it's going to get scored higher and higher until we get the outputs we desire with long reasoning chains of answers.  \n!  \n7",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "28cd89c8-c4ff-46b9-904c-1e3e77b4769a",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#quickstart) Quickstart > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#run-and-save-your-model) Run & Save your model\n\nRun your model by clicking the play button. In the first example, there is usually no reasoning in the answer and in order to see the reasoning, we need to first save the LoRA we just trained with GRPO first.  \n!  \nThe first inference example run has no reasoning. You must load the LoRA and test it to reveal the reasoning.  \nThen we load the LoRA and test it. Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!  \nYou can then save your model to GGUF, Ollama etc. by following our [guide here](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-7.-running--saving-the-model).  \n!  \nIf you are still not getting any reasoning, you may have either trained for too less steps or your reward function/verifier was not optimal.",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "55c9a4ae-6430-4da1-91e0-c44ddd9191c5",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo#video-tutorials) Video Tutorials\n\nHere are some video tutorials created by amazing YouTubers who we think are fantastic!  \nTrain A DeepSeek Style Reasoning Model With UnslothAI (Local Tutorial) - YouTube  \nBijan Bowen  \n12.2K subscribers  \n[Train A DeepSeek Style Reasoning Model With UnslothAI (Local Tutorial)](https://www.youtube.com/watch?v=SoPE1cUz3Hs)  \nBijan Bowen  \nSearch  \nWatch later  \nShare  \nCopy link  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nMore videos",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "c90926e8-d759-4a89-8c8e-c4e645bb3b2e",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl-tutorial-train-your-own-reasoning-model-with-grpo.md",
      "content": "More videos\n\nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \n[Watch on](https://www.youtube.com/watch?v=SoPE1cUz3Hs&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)  \n0:00  \n0:00 / 34:47‚Ä¢Live  \n‚Ä¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=SoPE1cUz3Hs \"Watch on YouTube\")  \nLocal GRPO on your own device  \nDeepseek-R1 & Training Your Own Reasoning Model - YouTube  \nAI Makerspace  \n12K subscribers  \n[Deepseek-R1 & Training Your Own Reasoning Model](https://www.youtube.com/watch?v=bbFEYPx9Hpo)  \nAI Makerspace  \nSearch  \nWatch later  \nShare  \nCopy link  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nMore videos  \nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \n[Watch on](https://www.youtube.com/watch?t=3289&v=bbFEYPx9Hpo&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)  \n54:49  \n54:49 / 1:09:58‚Ä¢Live  \n‚Ä¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=bbFEYPx9Hpo \"Watch on YouTube\")  \nGreat to learn about how to prep your dataset and explanations behind Reinforcement Learning + GRPO basics  \nThis ONE TRICK Turns your LLM like DeepSeek R1üí• Train your own DeepLlama for Free! üí• - YouTube  \n1littlecoder  \n95.1K subscribers  \n[This ONE TRICK Turns your LLM like DeepSeek R1üí• Train your own DeepLlama for Free! üí•](https://www.youtube.com/watch?v=juOh1afy-IE)  \n1littlecoder  \nSearch  \nWatch later  \nShare  \nCopy link  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nMore videos  \nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \n[Watch on](https://www.youtube.com/watch?v=juOh1afy-IE&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)  \n0:00  \n0:00 / 22:32‚Ä¢Live  \n‚Ä¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=juOh1afy-IE \"Watch on YouTube\")  \nüöÄCreate the Aha Moment Locally | Turn any model into a Reasoning Model using Unsloth - YouTube  \nPrompt Engineer  \n15.8K subscribers  \n[üöÄCreate the Aha Moment Locally | Turn any model into a Reasoning Model using Unsloth](https://www.youtube.com/watch?v=oF0_eMhzRaQ)  \nPrompt Engineer  \nSearch  \nWatch later  \nShare  \nCopy link  \nInfo  \nShopping  \nTap to unmute  \nIf playback doesn't begin shortly, try restarting your device.  \nMore videos  \nYou're signed out  \nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.  \nCancelConfirm  \nShare  \nInclude playlist  \nAn error occurred while retrieving sharing information. Please try again later.  \n[Watch on](https://www.youtube.com/watch?v=oF0_eMhzRaQ&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)  \n0:00  \n0:00 / 15:43‚Ä¢Live  \n‚Ä¢  \n[Watch on YouTube](https://www.youtube.com/watch?v=oF0_eMhzRaQ \"Watch on YouTube\")  \n[PreviousReasoning - GRPO & RL](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl) [NextReinforcement Learning - DPO, ORPO & KTO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto)  \nLast updated 9 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Tutorial: Train your own Reasoning model with GRPO | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"
      }
    },
    {
      "id": "d341c2e3-95e7-45f7-8b9f-1a9ccda0e5ca",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "---\ntitle: Reasoning - GRPO & RL | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\n---  \nThis article covers everything you need to know about GRPO, Reinforcement Learning (RL) and reward functions, along with tips, and the basics of using GRPO with Unsloth. If you're looking for a quickstart tutorial for using GRPO, see our guide [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo):  \n[‚ö°Tutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "935d1284-2c64-49e2-9d13-2194040cd327",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-notebooks) GRPO notebooks:\n\n- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)  \n- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)  \n- [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb)  \nDeepSeek developed [GRPO](https://unsloth.ai/blog/grpo) (Group Relative Policy Optimization) to train their R1 reasoning models. This RL technique optimizes responses efficiently without a value function model, reducing memory and computational costs compared to PPO (Proximal Policy Optimization).  \n- Usecases for GRPO isn‚Äôt just for code or math‚Äîits reasoning process can enhance tasks like email automation, database retrieval, law, and medicine, greatly improving accuracy based on your dataset and reward function!  \n- With 15GB VRAM, Unsloth allows you to transform any model up to 17B parameters like Llama 3.1 (8B), Phi-4 (14B), Mistral (7B) or Qwen2.5 (7B) into a reasoning model  \n- **Minimum requirement:** Just 5GB VRAM is enough to train your own reasoning model locally (for any model with 1.5B parameters or less)  \n- If you're not getting any reasoning, make sure you have enough training steps and ensure your [reward function/verifier](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) is working. We provide examples for reward functions [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples).  \n- Previous demonstrations show that you could achieve your own \"aha\" moment with Qwen2.5 (3B) - but it required 2xA100 GPUs (160GB VRAM). Now, with Unsloth, you can achieve the same \"aha\" moment using just a single 5GB VRAM GPU.  \n- Previously, GRPO was only supported for full fine-tuning, but we've made it work with QLoRA and LoRA  \n- On [**20K context lengths**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) for example with 8 generations per prompt, Unsloth uses only 54.3GB of VRAM for Llama 3.1 (8B), whilst standard implementations (+ Flash Attention 2) take **510.8GB (90% less for Unsloth)**.  \n- Please note, this isn‚Äôt fine-tuning DeepSeek‚Äôs R1 distilled models or using distilled data from R1 for tuning which Unsloth already supported. This is converting a standard model into a full-fledged reasoning model using GRPO.  \nIn a test example, even though we only trained Phi-4 with 100 steps using GRPO, the results are already clear. The model without GRPO does not have the thinking token, whilst the one trained with GRPO does and also has the correct answer.  \n!",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "078a1d2f-fb3c-47d1-8bbd-2f022b8fe665",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#training-with-grpo) Training with GRPO\n\nFor a tutorial on how to transform any open LLM into a reasoning model using Unsloth & GRPO, [see here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo).",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "ce7b9ecc-689a-4a0b-aea6-c0aee3087985",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#training-with-grpo) Training with GRPO > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#how-grpo-trains-a-model) **How GRPO Trains a Model**\n\n1. For each question-answer pair, the model generates multiple possible responses (e.g., 8 variations).  \n2. Each response is evaluated using reward functions.  \n3. Training Steps:  \n- If you have 300 rows of data, that's 300 training steps (or 900 steps if trained for 3 epochs).  \n- You can increase the number of generated responses per question (e.g., from 8 to 16).  \n4. The model learns by updating its weights every step.",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "baadaccd-f2d2-4b2a-ba77-0967005b3dfb",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#training-with-grpo) Training with GRPO > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#basics-tips) Basics/Tips\n\n- Wait for at least **300 steps** for the reward to actually increase. In order to get decent results, you may need to trade for a minimum of 12 hours (this is how GRPO works), but keep in mind this isn't compulsory as you can stop at anytime.  \n- For optimal results have at least **500 rows of data**. You can try with even 10 rows of data but it's better to have more.  \n- Each training run will always be different depending on your model, data, reward function/verifier etc. so though 300 steps is what we wrote as the minimum, sometimes it might be 1000 steps or more. So, it depends on various factors.  \n- If you're using GRPO with Unsloth locally, please \"pip install diffusers\" as well if you get an error. Please also use the latest version of vLLM.  \n- It‚Äôs advised to apply GRPO to a model at least **1.5B in parameters** to correctly generate thinking tokens as smaller models may not.  \n- For GRPO's [**GPU VRAM requirements**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) **for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.  \n- **Continuous fine-tuning is** possible and you can just leave GRPO running in the background.  \n- In the example notebooks, we use the [**GSM8K dataset**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions), the current most popular choice for R1-style training.  \n- If you‚Äôre using a base model, ensure you have a chat template.  \n- The more you train with GRPO the better. The best part of GRPO is you don't even need that much data. All you need is a great reward function/verifier and the more time spent training, the better your model will get. Expect your reward vs step to increase as time progresses like this:  \n!  \n- Training loss tracking for GRPO is now built directly into Unsloth, eliminating the need for external tools like wandb etc. It contains full logging details for all reward functions now including the total aggregated reward function itself.  \n!",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "5f0cfb92-1719-4683-b0af-c0197d3109c7",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) Reward Functions / Verifier\n\nIn Reinforcement Learning a **Reward Function** and a **Verifier** serve distinct roles in evaluating a model‚Äôs output. In general, you could interpret them as the same thing however, technically they're not but it does not matter as much as they are usually used in conjunction with each other.  \n**Verifier**:  \n- Determines whether the generated response is correct or incorrect.  \n- It does not assign a numerical score‚Äîit simply verifies correctness.  \n- Example: If a model generates \"5\" for \"2+2\", the verifier checks and labels it as \"wrong\" (since the correct answer is 4).  \n- Verifiers can also execute code (e.g., in Python) to validate logic, syntax, and correctness without needing manual evaluation.  \n**Reward Function**:  \n- Converts verification results (or other criteria) into a numerical score.  \n- Example: If an answer is wrong, it might assign a penalty (-1, -2, etc.), while a correct answer could get a positive score (+1, +2).  \n- It can also penalize based on criteria beyond correctness, such as excessive length or poor readability.  \n**Key Differences**:  \n- A **Verifier** checks correctness but doesn‚Äôt score.  \n- A **Reward Function** assigns a score but doesn‚Äôt necessarily verify correctness itself.  \n- A Reward Function _can_ use a Verifier, but they are technically not the same.",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "3d0e4892-fd5b-4b8e-9c19-ed7ad0ce3127",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) Reward Functions / Verifier > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#understanding-reward-functions) **Understanding Reward Functions**\n\nGRPO's primary goal is to maximize reward and learn how an answer was derived, rather than simply memorizing and reproducing responses from its training data.  \n- With every training step, GRPO **adjusts model weights** to maximize the reward. This process fine-tunes the model incrementally.  \n- **Regular fine-tuning** (without GRPO) only **maximizes next-word prediction probability** but does not optimize for a reward. GRPO **optimizes for a reward function** rather than just predicting the next word.  \n- You can **reuse data** across multiple epochs.  \n- **Default reward functions** can be predefined to be used on a wide array of use cases or you can ask ChatGPT/local model to generate them for you.  \n- There‚Äôs no single correct way to design reward functions or verifiers - the possibilities are endless. However, they must be well-designed and meaningful, as poorly crafted rewards can unintentionally degrade model performance.",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "25723bd9-eafd-41ed-9b51-f9dd8d4b1912",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) Reward Functions / Verifier > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples) Reward Function Examples\n\nYou can refer to the examples below. You can input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, feed your generations into a LLM of your choice and set a rule: \"If the answer sounds too robotic, deduct 3 points.\" This helps refine outputs based on quality criteria",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "f8024639-e5e4-4c61-b7f7-6162b03b31d9",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) Reward Functions / Verifier > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples) Reward Function Examples > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#example-1-simple-arithmetic-task) **Example #1: Simple Arithmetic Task**\n\n- **Question:** `\"2 + 2\"`  \n- **Answer:** `\"4\"`  \n- **Reward Function 1:**  \n- If a number is detected ‚Üí **+1**  \n- If no number is detected ‚Üí **-1**  \n- **Reward Function 2:**  \n- If the number matches the correct answer ‚Üí **+3**  \n- If incorrect ‚Üí **-3**  \n- **Total Reward:** _Sum of all reward functions_",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "fc385034-dce8-4dfd-a4a2-244026207f17",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) Reward Functions / Verifier > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples) Reward Function Examples > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#example-2-email-automation-task) **Example #2: Email Automation Task**\n\n- **Question:** Inbound email  \n- **Answer:** Outbound email  \n- **Reward Functions:**  \n- If the answer contains a required keyword ‚Üí **+1**  \n- If the answer exactly matches the ideal response ‚Üí **+1**  \n- If the response is too long ‚Üí **-1**  \n- If the recipient's name is included ‚Üí **+1**  \n- If a signature block (phone, email, address) is present ‚Üí **+1**",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "df9839f7-4f11-4af1-a2b2-0166328ae707",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) Reward Functions / Verifier > [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions) GSM8K Reward Functions\n\nIn our examples, we've built on existing GSM8K reward functions by [@willccbb](https://x.com/willccbb) which is popular and shown to be quite effective:  \n- **correctness_reward_func** ‚Äì Rewards exact label matches.  \n- **int_reward_func** ‚Äì Encourages integer-only answers.  \n- **soft_format_reward_func** ‚Äì Checks structure but allows minor newline mismatches.  \n- **strict_format_reward_func** ‚Äì Ensures response structure matches the prompt, including newlines.  \n- **xmlcount_reward_func** ‚Äì Ensures exactly one of each XML tag in the response.",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "c869e973-5e35-4fbd-a4db-12bcd9bcec1e",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#using-vllm) Using vLLM\n\nYou can now use [vLLM](https://github.com/vllm-project/vllm/) directly in your finetuning stack, which allows for much more throughput and allows you to finetune and do inference on the model at the same time! On 1x A100 40GB, expect 4000 tokens / s or so with Unsloth‚Äôs dynamic 4bit quant of Llama 3.2 3B Instruct. On a 16GB Tesla T4 (free Colab GPU), you can get 300 tokens / s.  \nWe also magically removed double memory usage when loading vLLM and Unsloth together, allowing for savings of 5GB or so for Llama 3.1 8B and 3GB for Llama 3.2 3B. Unsloth could originally finetune Llama 3.3 70B Instruct in 1x 48GB GPU with Llama 3.3 70B weights taking 40GB of VRAM. If we do not remove double memory usage, then we‚Äôll need >= 80GB of VRAM when loading Unsloth and vLLM together.  \nBut with Unsloth, you can still finetune and get the benefits of fast inference in one package in under 48GB of VRAM! To use fast inference, first install vllm, and instantiate Unsloth with fast_inference:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install unsloth vllm\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name = \"unsloth/Llama-3.2-3B-Instruct\",\nfast_inference = True,\n)\nmodel.fast_generate([\"Hello!\"])\n```",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "1b361e3e-acd8-4773-8ee2-098b5d8b31a9",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) GRPO Requirement Guidelines\n\nWhen you‚Äôre using Unsloth to do GRPO, we smartly reduce VRAM usage by over 90% when compared to standard implementations with Flash Attention 2 by using multiple tricks! On 20K context lengths for example with 8 generations per prompt, Unsloth uses only **54.3GB of VRAM for Llama 3.1 8B**, whilst standard implementations take **510.8GB (90% less for Unsloth)**.  \n1. For GRPO's **GPU VRAM requirements for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.  \n2. Our new memory efficient linear kernels for GRPO slashes memory usage by 8x or more. This shaves 68.5GB of memory, whilst being actually faster through the help of torch.compile!  \n3. We leverage our smart [Unsloth gradient checkpointing](https://unsloth.ai/blog/long-context) algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. This shaves 52GB of memory.  \n4. Unsloth also uses the same GPU / CUDA memory space as the underlying inference engine (vLLM), unlike implementations in other packages. This shaves 16GB of memory.  \nMetrics  \nUnsloth  \nStandard + FA2  \nTraining Memory Cost (GB)  \n42GB  \n414GB  \nGRPO Memory Cost (GB)  \n9.8GB  \n78.3GB  \nInference Cost (GB)  \n0GB  \n16GB  \nInference KV Cache for 20K context length (GB)  \n2.5GB  \n2.5GB  \nTotal Memory Usage  \n54.33GB (90% less)  \n510.8GB  \nIn typical standard GRPO implementations, you need to create 2 logits of size (8. 20K) to calculate the GRPO loss. This takes 2 * 2 bytes * 8 (num generations) * 20K (context length) * 128256 (vocabulary size) = 78.3GB in VRAM.  \nUnsloth shaves 8x memory usage for long context GRPO, so we need only an extra 9.8GB in extra VRAM for 20K context lengths!  \nWe also need to from the KV Cache in 16bit. Llama 3.1 8B has 32 layers, and both K and V are 1024 in size. So memory usage for 20K context length = 2 * 2 bytes * 32 layers * 20K context length * 1024 = 2.5GB per batch. We would set the batch size for vLLM to 8, but we shall leave it at 1 for our calculations to save VRAM. Otherwise you will need 20GB for the KV cache.",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "81c7c7ed-b8a0-429b-8a4b-a7a19c03f0a0",
      "source": "firecrawl\\docs\\basics-reasoning-grpo-and-rl.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#how-grpo-works) How GRPO Works:\n\nDeepSeek‚Äôs researchers observed an \"aha moment\" when training R1-Zero with pure reinforcement learning (RL). The model learned to extend its thinking time by reevaluating its initial approach, without any human guidance or predefined instructions.  \n1. The model generates groups of responses.  \n2. Each response is scored based on correctness or another metric created by some set reward function rather than an LLM reward model.  \n3. The average score of the group is computed.  \n4. Each response's score is compared to the group average.  \n5. The model is reinforced to favor higher-scoring responses.  \nAs an example, assume we want a model to solve:  \nWhat is 1+1? >> Chain of thought/working out >> The answer is 2.  \nWhat is 2+2? >> Chain of thought/working out >> The answer is 4.  \nOriginally, one had to collect large swathes of data to fill the working out / chain of thought process. But GRPO (the algorithm DeepSeek uses) or other RL algorithms can steer the model to automatically exhibit reasoning capabilities and create the reasoning trace. Instead, we need to create good reward functions or verifiers. For example, if it gets the correct answer, give it a score of 1. If some words are mis-spelt, minus 0.1. And so on! We can provide many many functions to reward the process.  \n[PreviousFine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide) [NextTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)  \nLast updated 2 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Reasoning - GRPO & RL | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"
      }
    },
    {
      "id": "e0e37805-f60e-4941-81d8-0901cfb09b72",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-inference.md",
      "content": "---\ntitle: Inference | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/running-and-saving-models/inference\n---  \nUnsloth supports natively 2x faster inference. For our inference only notebook, click [here](https://colab.research.google.com/drive/1aqlNQi7MMJbynFDyOQteD2t0yVfjb9Zh?usp=sharing).  \nAll QLoRA, LoRA and non LoRA inference paths are 2x faster. This requires no change of code or any new dependencies.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\nmax_seq_length = max_seq_length,\ndtype = dtype,\nload_in_4bit = load_in_4bit,\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64)\n```",
      "metadata": {
        "title": "Inference | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/inference"
      }
    },
    {
      "id": "75fe62c7-09d6-4fe7-9186-3a96603a37df",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-inference.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/inference#notimplementederror-a-utf-8-locale-is-required.-got-ansi) NotImplementedError: A UTF-8 locale is required. Got ANSI\n\nSometimes when you execute a cell [this error](https://github.com/googlecolab/colabtools/issues/3409) can appear. To solve this, in a new cell, run the below:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n```  \n[PreviousTroubleshooting](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting) [NextContinued Pretraining](https://docs.unsloth.ai/basics/continued-pretraining)  \nLast updated 2 months ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Inference | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/inference"
      }
    },
    {
      "id": "124221b6-4771-4337-96fa-fac5f22cd17d",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-gguf.md",
      "content": "---\ntitle: Saving to GGUF | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf\n---  \nLocallyManual Saving  \nTo save to GGUF, use the below to save locally:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q4_k_m\")\nmodel.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q8_0\")\nmodel.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"f16\")\n```  \nFor to push to hub:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel.push_to_hub_gguf(\"hf_username/dir\", tokenizer, quantization_method = \"q4_k_m\")\nmodel.push_to_hub_gguf(\"hf_username/dir\", tokenizer, quantization_method = \"q8_0\")\n```  \nAll supported quantization options for `quantization_method` are listed below:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n# https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp#L19\n# From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\nALLOWED_QUANTS = \\\n{\n\"not_quantized\" : \"Recommended. Fast conversion. Slow inference, big files.\",\n\"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\n\"quantized\" : \"Recommended. Slow conversion. Fast inference, small files.\",\n\"f32\" : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\n\"f16\" : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\n\"q8_0\" : \"Fast conversion. High resource use, but generally acceptable.\",\n\"q4_k_m\" : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\n\"q5_k_m\" : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\n\"q2_k\" : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\n\"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\n\"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\n\"q3_k_s\" : \"Uses Q3_K for all tensors\",\n\"q4_0\" : \"Original quant method, 4-bit.\",\n\"q4_1\" : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\n\"q4_k_s\" : \"Uses Q4_K for all tensors\",\n\"q4_k\" : \"alias for q4_k_m\",\n\"q5_k\" : \"alias for q5_k_m\",\n\"q5_0\" : \"Higher accuracy, higher resource usage and slower inference.\",\n\"q5_1\" : \"Even higher accuracy, resource usage and slower inference.\",\n\"q5_k_s\" : \"Uses Q5_K for all tensors\",\n\"q6_k\" : \"Uses Q8_K for all tensors\",\n\"iq2_xxs\" : \"2.06 bpw quantization\",\n\"iq2_xs\" : \"2.31 bpw quantization\",\n\"iq3_xxs\" : \"3.06 bpw quantization\",\n\"q3_k_xs\" : \"3-bit extra small quantization\",\n}\n```  \nFirst save your model to 16bit:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)\n```  \nThen use the terminal and do:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ngit clone --recursive https://github.com/ggerganov/llama.cpp\nmake clean -C llama.cpp\nmake all -j -C llama.cpp\npip install gguf protobuf\n\npython llama.cpp/convert-hf-to-gguf.py FOLDER --outfile OUTPUT --outtype f16\n```  \nOr follow the steps at https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model using the model name \"merged_model\" to merge to GGUF.  \n[PreviousRunning & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models) [NextSaving to Ollama](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama)  \nLast updated 8 months ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Saving to GGUF | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf"
      }
    },
    {
      "id": "65f38f29-5772-421a-8d88-005754d7ef4b",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-ollama.md",
      "content": "---\ntitle: Saving to Ollama | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama\n---  \nSee our guide below for the complete process on how to save to [Ollama](https://github.com/ollama/ollama):  \n[ü¶ôTutorial: How to Finetune Llama-3 and Use In Ollama](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama)",
      "metadata": {
        "title": "Saving to Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama"
      }
    },
    {
      "id": "be0b15e0-1df0-4f62-bbf3-f3b7bdd059e2",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama#saving-on-google-colab) Saving on Google Colab\n\nYou can save the finetuned model as a small 100MB file called a LoRA adapter like below. You can instead push to the Hugging Face hub as well if you want to upload your model! Remember to get a Hugging Face token via: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and add your token!  \n!  \nAfter saving the model, we can again use Unsloth to run the model itself! Use `FastLanguageModel` again to call it for inference!  \n!",
      "metadata": {
        "title": "Saving to Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama"
      }
    },
    {
      "id": "a76d5db0-6d3b-46ac-bbd3-76c5b6d110ef",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama#exporting-to-ollama) Exporting to Ollama\n\nFinally we can export our finetuned model to Ollama itself! First we have to install Ollama in the Colab notebook:  \n!  \nThen we export the finetuned model we have to llama.cpp's GGUF formats like below:  \n!  \nReminder to convert `False` to `True` for 1 row, and not change every row to `True`, or else you'll be waiting for a very time! We normally suggest the first row getting set to `True`, so we can export the finetuned model quickly to `Q8_0` format (8 bit quantization). We also allow you to export to a whole list of quantization methods as well, with a popular one being `q4_k_m`.  \nHead over to [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) to learn more about GGUF. We also have some manual instructions of how to export to GGUF if you want here: [https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf](https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf)  \nYou will see a long list of text like below - please wait 5 to 10 minutes!!  \n!  \nAnd finally at the very end, it'll look like below:  \n!  \nThen, we have to run Ollama itself in the background. We use `subprocess` because Colab doesn't like asynchronous calls, but normally one just runs `ollama serve` in the terminal / command prompt.  \n!",
      "metadata": {
        "title": "Saving to Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama"
      }
    },
    {
      "id": "6c9be3b4-eb25-49ac-81b3-fbbea45d8f39",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama#automatic-modelfile-creation) Automatic `Modelfile` creation\n\nThe trick Unsloth provides is we automatically create a `Modelfile` which Ollama requires! This is a just a list of settings and includes the chat template which we used for the finetune process! You can also print the `Modelfile` generated like below:  \n!  \nWe then ask Ollama to create a model which is Ollama compatible, by using the `Modelfile`  \n!",
      "metadata": {
        "title": "Saving to Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama"
      }
    },
    {
      "id": "9b00f9ae-66e2-4f67-918b-d90426bf7059",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama#ollama-inference) Ollama Inference\n\nAnd we can now call the model for inference if you want to do call the Ollama server itself which is running on your own local machine / in the free Colab notebook in the background. Remember you can edit the yellow underlined part.  \n!",
      "metadata": {
        "title": "Saving to Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama"
      }
    },
    {
      "id": "e945c697-a763-4bd0-8b6c-e4952f8bdda3",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama#undefined)\n\n[PreviousSaving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf) [NextSaving to VLLM](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm)  \nLast updated 8 months ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Saving to Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama"
      }
    },
    {
      "id": "a3628660-98c3-4f34-b659-954d02b03b84",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-saving-to-vllm.md",
      "content": "---\ntitle: Saving to VLLM | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm\n---  \nTo save to 16bit for VLLM, use:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\nmodel.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n```  \nTo merge to 4bit to load on HuggingFace, first call `merged_4bit`. Then use `merged_4bit_forced` if you are certain you want to merge to 4bit. I highly discourage you, unless you know what you are going to do with the 4bit model (ie for DPO training for eg or for HuggingFace's online inference engine)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\nmodel.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n```  \nTo save just the LoRA adapters, either use:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel.save_pretrained(...) AND tokenizer.save_pretrained(...)\n```  \nOr just use our builtin function to do that:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\nmodel.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")\n```  \n[PreviousSaving to Ollama](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama) [NextTroubleshooting](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting)  \nLast updated 8 months ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Saving to VLLM | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm"
      }
    },
    {
      "id": "32ff3137-13a9-4992-a9e5-f5472f4e3267",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-troubleshooting.md",
      "content": "---\ntitle: Troubleshooting | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting\n---",
      "metadata": {
        "title": "Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting"
      }
    },
    {
      "id": "9e977742-1bba-4b55-9232-d0df6467e5a7",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-troubleshooting.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting#running-in-unsloth-works-well-but-after-exporting-and-running-on-other-platforms-the-results-are-poo) Running in Unsloth works well, but after exporting & running on other platforms, the results are poor\n\nYou might sometimes encounter an issue where your model runs and produces good results on Unsloth, but when you use it on another platform like Ollama or vLLM, the results are poor or you might get gibberish, endless/infinite generations _or_ repeated outputs **.**  \n- The most common cause of this error is using an incorrect chat template. It‚Äôs essential to use the SAME chat template that was used when training the model in Unsloth and later when you run it in another framework, such as llama.cpp or Ollama. When inferencing from a saved model, it's crucial to apply the correct template.  \n- It might also be because your inference engine adds an unnecessary \"start of sequence\" token (or the lack of thereof on the contrary) so ensure you check both hypotheses!",
      "metadata": {
        "title": "Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting"
      }
    },
    {
      "id": "144ed017-68ad-4529-ac0b-11d98de7ef3b",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-troubleshooting.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting#saving-to-safetensors-not-bin-format-in-colab) Saving to `safetensors`, not `bin` format in Colab\n\nWe save to `.bin` in Colab so it's like 4x faster, but set `safe_serialization = None` to force saving to `.safetensors`. So `model.save_pretrained(..., safe_serialization = None)` or `model.push_to_hub(..., safe_serialization = None)`",
      "metadata": {
        "title": "Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting"
      }
    },
    {
      "id": "ff192f7f-b968-4c24-ab36-29ab0fd34763",
      "source": "firecrawl\\docs\\basics-running-and-saving-models-troubleshooting.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting#if-saving-to-gguf-or-vllm-16bit-crashes) If saving to GGUF or vLLM 16bit crashes\n\nYou can try reducing the maximum GPU usage during saving by changing `maximum_memory_usage`.  \nThe default is `model.save_pretrained(..., maximum_memory_usage = 0.75)`. Reduce it to say 0.5 to use 50% of GPU peak memory or lower. This can reduce OOM crashes during saving.  \n[PreviousSaving to VLLM](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm) [NextInference](https://docs.unsloth.ai/basics/running-and-saving-models/inference)  \nLast updated 10 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Troubleshooting | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting"
      }
    },
    {
      "id": "dddb7294-1439-46e9-b904-c2fd51bf6cf2",
      "source": "firecrawl\\docs\\basics-running-and-saving-models.md",
      "content": "---\ntitle: Running & Saving Models | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/running-and-saving-models\n---  \nYou can also run your fine-tuned models by using [Unsloth's 2x faster inference](https://docs.unsloth.ai/basics/running-and-saving-models/inference).  \n[Saving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf)  \n[Saving to Ollama](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama)  \n[Saving to VLLM](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm)  \n[Troubleshooting](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting)  \n[Inference](https://docs.unsloth.ai/basics/running-and-saving-models/inference)  \n[PreviousDatasets 101](https://docs.unsloth.ai/basics/datasets-101) [NextSaving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf)  \nLast updated 24 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Running & Saving Models | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/running-and-saving-models"
      }
    },
    {
      "id": "efc65ebe-e5fb-4a2e-a120-5141e4c1dd5c",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "---\ntitle: Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\n---  \nBy the end of this tutorial, you will create a custom chatbot by **finetuning Llama-3** with [**Unsloth**](https://github.com/unslothai/unsloth) for free. It can run locally via [**Ollama**](https://github.com/ollama/ollama) on your PC, or in a free GPU instance through [**Google Colab**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb). You will be able to interact with the chatbot interactively like below:  \n!  \n**Unsloth** makes finetuning much easier, and can automatically export the finetuned model to **Ollama** with integrated automatic `Modelfile` creation! If you need help, you can join our Discord server: [https://discord.com/invite/unsloth](https://discord.com/invite/unsloth)",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "b3e959ac-b4ae-46d5-a638-a8ef4d9f3568",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-1.-what-is-unsloth) 1. What is Unsloth?\n\n[Unsloth](https://github.com/unslothai/unsloth) makes finetuning LLMs like Llama-3, Mistral, Phi-3 and Gemma 2x faster, use 70% less memory, and with no degradation in accuracy! We will be using Google Colab which provides a free GPU during this tutorial. You can access our free notebooks below:  \n- [Ollama Llama-3 Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb) (notebook which we will be using)  \n- [CSV/Excel Ollama Guide](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "8a8ec16d-888b-4c1e-9876-dd0520fb0ef0",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-1.-what-is-unsloth) 1. What is Unsloth? > [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#you-will-also-need-to-login-into-your-google-account) _**You will also need to login into your Google account!**_\n\n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "2451fb0e-e95e-41d6-8c0a-d753c85a6989",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-2.-what-is-ollama) 2. What is Ollama?\n\n[Ollama](https://github.com/ollama/ollama) allows you to run language models from your own computer in a quick and simple way! It quietly launches a program which can run a language model like Llama-3 in the background. If you suddenly want to ask the language model a question, you can simply submit a request to Ollama, and it'll quickly return the results to you! We'll be using Ollama as our inference engine!  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "eb1e0e40-99f2-4c28-a14a-daf74438c4e4",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-3.-install-unsloth) 3. Install Unsloth\n\n!  \nIf you have never used a Colab notebook, a quick primer on the notebook itself:  \n1. **Play Button at each \"cell\".** Click on this to run that cell's code. You must not skip any cells and you must run every cell in chronological order. If you encounter any errors, simply rerun the cell you did not run before. Another option is to click CTRL + ENTER if you don't want to click the play button.  \n2. **Runtime Button in the top toolbar.** You can also use this button and hit \"Run all\" to run the entire notebook in 1 go. This will skip all the customization steps, and can be a good first try.  \n3. **Connect / Reconnect T4 button.** You can click here for more advanced system statistics.  \nThe first installation cell looks like below: Remember to click the PLAY button in the brackets [ ]. We grab our open source Github package, and install some other packages.  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "af6449ef-a7a1-4563-a246-541331681e7e",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-4.-selecting-a-model-to-finetune) 4. Selecting a model to finetune\n\nLet's now select a model for finetuning! We defaulted to Llama-3 from Meta / Facebook which was trained on a whopping 15 trillion \"tokens\". Assume a token is like 1 English word. That's approximately 350,000 thick Encyclopedias worth! Other popular models include Mistral, Phi-3 (trained using GPT-4 output) and Gemma from Google (13 trillion tokens!).  \nUnsloth supports these models and more! In fact, simply type a model from the Hugging Face model hub to see if it works! We'll error out if it doesn't work.  \n!  \nThere are 3 other settings which you can toggle:  \n1. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmax_seq_length = 2048\n```  \nThis determines the context length of the model. Gemini for example has over 1 million context length, whilst Llama-3 has 8192 context length. We allow you to select ANY number - but we recommend setting it 2048 for testing purposes. Unsloth also supports very long context finetuning, and we show we can provide 4x longer context lengths than the best.  \n2. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ndtype = None\n```  \nKeep this as None, but you can select torch.float16 or torch.bfloat16 for newer GPUs.  \n3. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nload_in_4bit = True\n```  \nWe do finetuning in 4 bit quantization. This reduces memory usage by 4x, allowing us to actually do finetuning in a free 16GB memory GPU. 4 bit quantization essentially converts weights into a limited set of numbers to reduce memory usage. A drawback of this is there is a 1-2% accuracy degradation. Set this to False on larger GPUs like H100s if you want that tiny extra accuracy.  \n!  \nIf you run the cell, you will get some print outs of the Unsloth version, which model you are using, how much memory your GPU has, and some other statistics. Ignore this for now.",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "42831ff8-47cb-4634-a576-2abe7e9f35f3",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-5.-parameters-for-finetuning) 5. Parameters for finetuning\n\n!  \nNow to customize your finetune, you can edit the numbers above, but you can ignore it, since we already select quite reasonable numbers.  \nThe goal is to change these numbers to increase accuracy, but also **counteract over-fitting**. Over-fitting is when you make the language model memorize a dataset, and not be able to answer novel new questions. We want to a final model to answer unseen questions, and not do memorization.  \n1. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nr = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n```  \nThe rank of the finetuning process. A larger number uses more memory and will be slower, but can increase accuracy on harder tasks. We normally suggest numbers like 8 (for fast finetunes), and up to 128. Too large numbers can causing over-fitting, damaging your model's quality.  \n2. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\\n\"gate_proj\", \"up_proj\", \"down_proj\",],\n```  \nWe select all modules to finetune. You can remove some to reduce memory usage and make training faster, but we highly do not suggest this. Just train on all modules!  \n3. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nlora_alpha = 16,\n```  \nThe scaling factor for finetuning. A larger number will make the finetune learn more about your dataset, but can promote over-fitting. We suggest this to equal to the rank `r`, or double it.  \n4. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nlora_dropout = 0, # Supports any, but = 0 is optimized\n```  \nLeave this as 0 for faster training! Can reduce over-fitting, but not that much.  \n5. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nbias = \"none\", # Supports any, but = \"none\" is optimized\n```  \nLeave this as 0 for faster and less over-fit training!  \n6. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nuse_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n```  \nOptions include `True`, `False ` and `\"unsloth\"`. We suggest `\"unsloth\"` since we reduce memory usage by an extra 30% and support extremely long context finetunes.You can read up here: [https://unsloth.ai/blog/long-context](https://unsloth.ai/blog/long-context) for more details.  \n7. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nrandom_state = 3407,\n```  \nThe number to determine deterministic runs. Training and finetuning needs random numbers, so setting this number makes experiments reproducible.  \n8. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nuse_rslora = False, # We support rank stabilized LoRA\n```  \nAdvanced feature to set the `lora_alpha = 16` automatically. You can use this if you want!  \n9. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nloftq_config = None, # And LoftQ\n```  \nAdvanced feature to initialize the LoRA matrices to the top r singular vectors of the weights. Can improve accuracy somewhat, but can make memory usage explode at the start.",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "d98132dd-96ac-413d-a134-7f77f04f7144",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-6.-alpaca-dataset) 6. Alpaca Dataset\n\n!  \nWe will now use the Alpaca Dataset created by calling GPT-4 itself. It is a list of 52,000 instructions and outputs which was very popular when Llama-1 was released, since it made finetuning a base LLM be competitive with ChatGPT itself.  \nYou can access the GPT4 version of the Alpaca dataset here: [https://huggingface.co/datasets/vicgalle/alpaca-gpt4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4). An older first version of the dataset is here: [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca). Below shows some examples of the dataset:  \n!  \nYou can see there are 3 columns in each row - an instruction, and input and an output. We essentially combine each row into 1 large prompt like below. We then use this to finetune the language model, and this made it very similar to ChatGPT. We call this process **supervised instruction finetuning**.  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "608060ca-d780-4a42-af0a-3c294122df7b",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-7.-multiple-columns-for-finetuning) 7. Multiple columns for finetuning\n\nBut a big issue is for ChatGPT style assistants, we only allow 1 instruction / 1 prompt, and not multiple columns / inputs. For example in ChatGPT, you can see we must submit 1 prompt, and not multiple prompts.  \n!  \nThis essentially means we have to \"merge\" multiple columns into 1 large prompt for finetuning to actually function!  \nFor example the very famous Titanic dataset has many many columns. Your job was to predict whether a passenger has survived or died based on their age, passenger class, fare price etc. We can't simply pass this into ChatGPT, but rather, we have to \"merge\" this information into 1 large prompt.  \n!  \nFor example, if we ask ChatGPT with our \"merged\" single prompt which includes all the information for that passenger, we can then ask it to guess or predict whether the passenger has died or survived.  \n!  \nOther finetuning libraries require you to manually prepare your dataset for finetuning, by merging all your columns into 1 prompt. In Unsloth, we simply provide the function called `to_sharegpt` which does this in 1 go!  \nTo access the Titanic finetuning notebook or if you want to upload a CSV or Excel file, go here: [https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)  \n!  \nNow this is a bit more complicated, since we allow a lot of customization, but there are a few points:  \n- You must enclose all columns in curly braces `{}`. These are the column names in the actual CSV / Excel file.  \n- Optional text components must be enclosed in `[[]]`. For example if the column \"input\" is empty, the merging function will not show the text and skip this. This is useful for datasets with missing values.  \n- Select the output or target / prediction column in `output_column_name`. For the Alpaca dataset, this will be `output`.  \nFor example in the Titanic dataset, we can create a large merged prompt format like below, where each column / piece of text becomes optional.  \n!  \nFor example, pretend the dataset looks like this with a lot of missing data:  \nEmbarked  \nAge  \nFare  \nS  \n23  \n18  \n7.25  \nThen, we do not want the result to be:  \n1. The passenger embarked from S. Their age is 23. Their fare is **EMPTY**.  \n2. The passenger embarked from **EMPTY**. Their age is 18. Their fare is $7.25.  \nInstead by optionally enclosing columns using `[[]]`, we can exclude this information entirely.  \n1. [[The passenger embarked from S.]] [[Their age is 23.]] [[Their fare is **EMPTY**.]]  \n2. [[The passenger embarked from **EMPTY**.]] [[Their age is 18.]] [[Their fare is $7.25.]]  \nbecomes:  \n1. The passenger embarked from S. Their age is 23.  \n2. Their age is 18. Their fare is $7.25.",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "4b707897-e11b-4407-a6e2-9d807edf3370",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-8.-multi-turn-conversations) 8. Multi turn conversations\n\nA bit issue if you didn't notice is the Alpaca dataset is single turn, whilst remember using ChatGPT was interactive and you can talk to it in multiple turns. For example, the left is what we want, but the right which is the Alpaca dataset only provides singular conversations. We want the finetuned language model to somehow learn how to do multi turn conversations just like ChatGPT.  \n!  \nSo we introduced the `conversation_extension` parameter, which essentially selects some random rows in your single turn dataset, and merges them into 1 conversation! For example, if you set it to 3, we randomly select 3 rows and merge them into 1! Setting them too long can make training slower, but could make your chatbot and final finetune much better!  \n!  \nThen set `output_column_name` to the prediction / output column. For the Alpaca dataset dataset, it would be the output column.  \nWe then use the `standardize_sharegpt` function to just make the dataset in a correct format for finetuning! Always call this!  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "ef237994-1966-4812-b1bb-e25aece88657",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-9.-customizable-chat-templates) 9. Customizable Chat Templates\n\nWe can now specify the chat template for finetuning itself. The very famous Alpaca format is below:  \n!  \nBut remember we said this was a bad idea because ChatGPT style finetunes require only 1 prompt? Since we successfully merged all dataset columns into 1 using Unsloth, we essentially can create the below style chat template with 1 input column (instruction) and 1 output:  \n!  \nWe just require you must put a `{INPUT}` field for the instruction and an `{OUTPUT}` field for the model's output field. We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT. For example, below are some cool examples which you can customize the chat template to be:  \n!  \nFor the ChatML format used in OpenAI models:  \n!  \nOr you can use the Llama-3 template itself (which only functions by using the instruct version of Llama-3): We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT.  \n!  \nOr in the Titanic prediction task where you had to predict if a passenger died or survived in this Colab notebook which includes CSV and Excel uploading: [https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "65a8a8d8-d244-4636-b0c7-5c91d3526746",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-10.-train-the-model) 10. Train the model\n\nLet's train the model now! We normally suggest people to not edit the below, unless if you want to finetune for longer steps or want to train on large batch sizes.  \n!  \nWe do not normally suggest changing the parameters above, but to elaborate on some of them:  \n1. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nper_device_train_batch_size = 2,\n```  \nIncrease the batch size if you want to utilize the memory of your GPU more. Also increase this to make training more smooth and make the process not over-fit. We normally do not suggest this, since this might make training actually slower due to padding issues. We normally instead ask you to increase `gradient_accumulation_steps` which just does more passes over the dataset.  \n2. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ngradient_accumulation_steps = 4,\n```  \nEquivalent to increasing the batch size above itself, but does not impact memory consumption! We normally suggest people increasing this if you want smoother training loss curves.  \n3. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmax_steps = 60, # num_train_epochs = 1,\n```  \nWe set steps to 60 for faster training. For full training runs which can take hours, instead comment out `max_steps`, and replace it with `num_train_epochs = 1`. Setting it to 1 means 1 full pass over your dataset. We normally suggest 1 to 3 passes, and no more, otherwise you will over-fit your finetune.  \n4. Copy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nlearning_rate = 2e-4,\n```  \nReduce the learning rate if you want to make the finetuning process slower, but also converge to a higher accuracy result most likely. We normally suggest 2e-4, 1e-4, 5e-5, 2e-5 as numbers to try.  \n!  \nYou will see a log of some numbers! This is the training loss, and your job is to set parameters to make this go to as close to 0.5 as possible! If your finetune is not reaching 1, 0.8 or 0.5, you might have to adjust some numbers. If your loss goes to 0, that's probably not a good sign as well!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "bef9dbe7-7072-476c-b82f-779b4c16c810",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-11.-inference-running-the-model) 11. Inference / running the model\n\n!  \nNow let's run the model after we completed the training process! You can edit the yellow underlined part! In fact, because we created a multi turn chatbot, we can now also call the model as if it saw some conversations in the past like below:  \n!  \nReminder Unsloth itself provides **2x faster inference** natively as well, so always do not forget to call `FastLanguageModel.for_inference(model)`. If you want the model to output longer responses, set `max_new_tokens = 128` to some larger number like 256 or 1024. Notice you will have to wait longer for the result as well!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "8ba43d41-9864-43ef-b602-9b1f5394ceaa",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-12.-saving-the-model) 12. Saving the model\n\nWe can now save the finetuned model as a small 100MB file called a LoRA adapter like below. You can instead push to the Hugging Face hub as well if you want to upload your model! Remember to get a Hugging Face token via [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and add your token!  \n!  \nAfter saving the model, we can again use Unsloth to run the model itself! Use `FastLanguageModel` again to call it for inference!  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "8beb527a-9c92-4ed9-80e1-3f8a6486ea8b",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-13.-exporting-to-ollama) 13. Exporting to Ollama\n\nFinally we can export our finetuned model to Ollama itself! First we have to install Ollama in the Colab notebook:  \n!  \nThen we export the finetuned model we have to llama.cpp's GGUF formats like below:  \n!  \nReminder to convert `False` to `True` for 1 row, and not change every row to `True`, or else you'll be waiting for a very time! We normally suggest the first row getting set to `True`, so we can export the finetuned model quickly to `Q8_0` format (8 bit quantization). We also allow you to export to a whole list of quantization methods as well, with a popular one being `q4_k_m`.  \nHead over to [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) to learn more about GGUF. We also have some manual instructions of how to export to GGUF if you want here: [https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf](https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf)  \nYou will see a long list of text like below - please wait 5 to 10 minutes!!  \n!  \nAnd finally at the very end, it'll look like below:  \n!  \nThen, we have to run Ollama itself in the background. We use `subprocess` because Colab doesn't like asynchronous calls, but normally one just runs `ollama serve` in the terminal / command prompt.  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "486ee4f8-e4d6-4145-89a3-83eae6228deb",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-14.-automatic-modelfile-creation) 14. Automatic `Modelfile` creation\n\nThe trick Unsloth provides is we automatically create a `Modelfile` which Ollama requires! This is a just a list of settings and includes the chat template which we used for the finetune process! You can also print the `Modelfile` generated like below:  \n!  \nWe then ask Ollama to create a model which is Ollama compatible, by using the `Modelfile`  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "845c320a-dcad-445a-ae63-8037f52be51a",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-15.-ollama-inference) 15. Ollama Inference\n\nAnd we can now call the model for inference if you want to do call the Ollama server itself which is running on your own local machine / in the free Colab notebook in the background. Remember you can edit the yellow underlined part.  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "66d20b43-51ba-4c74-b0ec-7a6f85500d9e",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-16.-interactive-chatgpt-style) 16. Interactive ChatGPT style\n\nBut to actually run the finetuned model like a ChatGPT, we have to do a bit more! First click the terminal icon! and a Terminal will pop up. It's on the left sidebar.  \n!  \nThen, you might have to press ENTER twice to remove some weird output in the Terminal window. Wait a few seconds and type `ollama run unsloth_model` then hit ENTER.  \n!  \nAnd finally, you can interact with the finetuned model just like an actual ChatGPT! Hit CTRL + D to exit the system, and hit ENTER to converse with the chatbot!  \n!",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "c61542f5-d38d-49df-aaf2-56450dbdb81b",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-finetune-llama-3-and-use-in-ollama.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#youve-done-it) You've done it!\n\nYou've successfully finetuned a language model and exported it to Ollama with Unsloth 2x faster and with 70% less VRAM! And all this for free in a Google Colab notebook!  \nIf you want to learn how to do reward modelling, do continued pretraining, export to vLLM or GGUF, do text completion, or learn more about finetuning tips and tricks, head over to our [Github](https://github.com/unslothai/unsloth#-finetune-for-free).  \nIf you need any help on finetuning, you can also join our Discord server [here](https://discord.gg/unsloth). If you want help with Ollama, you can also join their server [here](https://discord.gg/ollama).  \nAnd finally, we want to thank you for reading and following this far! We hope this made you understand some of the nuts and bolts behind finetuning language models, and we hope this was useful!  \nTo access our Alpaca dataset example click [here](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing), and our CSV / Excel finetuning guide is [here](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing).  \n[PreviousTutorial: How to Run Gemma 3 effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively) [NextDatasets 101](https://docs.unsloth.ai/basics/datasets-101)  \nLast updated 25 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama"
      }
    },
    {
      "id": "37f4060d-3881-4316-a263-b3a34c2c9734",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally-deepseek-r1-dynamic-1.58-bit.md",
      "content": "---\ntitle: DeepSeek-R1 Dynamic 1.58-bit | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\n---  \nRead our full DeepSeek-R1 blogpost here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)",
      "metadata": {
        "title": "DeepSeek-R1 Dynamic 1.58-bit | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit"
      }
    },
    {
      "id": "581c3231-1ed3-47da-8cb9-acac4671e6a3",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally-deepseek-r1-dynamic-1.58-bit.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit#id-1-bit-small-dynamic-vs.-basic) 1-bit (Small) - Dynamic vs. Basic\n\nGGUF Type  \nQuant  \nSize (GB)  \nSeed  \nPygame  \nBackground  \nAccelerate SPACE  \nBird shape  \nLand  \nTop right score  \nPipes  \nBest Score  \nQuit  \nRunnable  \nScore  \nAvg Score  \nErrors  \nNotes  \nDynamic  \nIQ1_S  \n131  \n3407  \n1  \n0.5  \n1  \n0.5  \n0.5  \n1  \n0.5  \n1  \n1  \n0  \n7  \nscore =!inc SyntaxError: invalid syntax  \nSelects random shapes and colors at the start, but doesn't rotate across trials  \nDynamic  \nIQ1_S  \n131  \n3408  \n1  \n1  \n0.25  \n1  \n0.5  \n1  \n0.5  \n1  \n1  \n0  \n7.25  \nscore =B4 NameError: name 'B4' is not defined  \nBetter - selects pipe colors randomnly, but all are just 1 color - should be different. Dropping to ground fails to reset acceleration.  \nDynamic  \nIQ1_S  \n131  \n3409  \n1  \n0.5  \n0.5  \n0.5  \n0  \n1  \n1  \n1  \n1  \n0  \n6.5  \n6.92  \nscore =3D 0 SyntaxError: invalid decimal literal  \nToo hard to play - acceleration too fast. Pipe colors now are random, but bird shape not changing. Land collison fails.  \nBasic  \nIQ1_S  \n133  \n3407  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \nNo code  \nFully failed. Repeats \"with Dark Colurs\" forever  \nBasic  \nIQ1_S  \n133  \n3408  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \nNo code  \nFully failed. Repeats \"Pygame's\" forever  \nBasic  \nIQ1_S  \n133  \n3409  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \nNo code  \nFully failed. Repeats \"pipe_x = screen_height\npipe_x = screen_height\npipe_height = screen_height - Pipe_height\" forever.",
      "metadata": {
        "title": "DeepSeek-R1 Dynamic 1.58-bit | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit"
      }
    },
    {
      "id": "acdd5009-267a-4055-b057-513509ebc928",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally-deepseek-r1-dynamic-1.58-bit.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit#id-1-bit-medium-dynamic-vs.-basic) 1-bit (Medium) - Dynamic vs. Basic\n\nGGUF Type  \nQuant  \nSize (GB)  \nSeed  \nPygame  \nBackground  \nAccelerate SPACE  \nBird shape  \nLand  \nTop right score  \nPipes  \nBest Score  \nQuit  \nRunnable  \nScore  \nAvg Score  \nErrors  \nNotes  \nDynamic  \nIQ1_M  \n158  \n3407  \n1  \n1  \n0.75  \n1  \n1  \n1  \n1  \n1  \n1  \n1  \n9.75  \nNone  \nA bit fast and hard to play.  \nDynamic  \nIQ1_M  \n158  \n3408  \n1  \n1  \n0.5  \n1  \n1  \n1  \n1  \n1  \n1  \n1  \n9.5  \nNone  \nVery good - land should be clearer. Acceleration should be slower.  \nDynamic  \nIQ1_M  \n158  \n3409  \n1  \n0.5  \n1  \n0.5  \n0.5  \n1  \n0.5  \n1  \n1  \n1  \n8  \n9.08  \nNone  \nBackground color does not change across trials.Pipes do not touch the top. No land is seen.  \nBasic  \nIQ1_M  \n149  \n3407  \n1  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n1  \n0  \n2  \nif game_over: NameError: name 'game_over' is not defined  \nFully failed. Black screen only  \nBasic  \nIQ1_M  \n149  \n3408  \n1  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n1  \n0  \n2  \nNo code  \nFully failed. Black screen then closes.  \nBasic  \nIQ1_M  \n149  \n3409  \n1  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n0  \n1  \n1.67  \nwindow.fill((100, 100, 255)) Light Blue SyntaxError: invalid syntax && main() NameError: name 'main' is not defined.  \nFully failed.",
      "metadata": {
        "title": "DeepSeek-R1 Dynamic 1.58-bit | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit"
      }
    },
    {
      "id": "ff235e1b-e695-498e-b7db-08a049b59ee9",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally-deepseek-r1-dynamic-1.58-bit.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit#id-2-bit-extra-extra-small-dynamic-vs.-basic) 2-bit (Extra extra Small) - Dynamic vs. Basic\n\nGGUF Type  \nQuant  \nSize (GB)  \nSeed  \nPygame  \nBackground  \nAccelerate SPACE  \nBird shape  \nLand  \nTop right score  \nPipes  \nBest Score  \nQuit  \nRunnable  \nScore  \nAvg Score  \nErrors  \nNotes  \nDynamic  \nIQ2_XXS  \n183  \n3407  \n1  \n1  \n0.5  \n1  \n1  \n1  \n1  \n1  \n1  \n1  \n9.5  \nNone  \nToo hard to play - acceleration too slow. Lags  \nDynamic  \nIQ2_XXS  \n183  \n3408  \n1  \n1  \n1  \n1  \n1  \n1  \n0.5  \n0.5  \n1  \n0  \n8  \nglobal best_score SyntaxError: name 'best_score' is assigned to before global declaration  \nHad to edit 2 lines - remove global best_score, and set pipe_list = []  \nDynamic  \nIQ2_XXS  \n183  \n3409  \n1  \n1  \n1  \n1  \n1  \n1  \n1  \n1  \n1  \n1  \n10  \n9.17  \nNone  \nExtremely good. Even makes pipes have random distances between them.  \nBasic  \nIQ2_XXS  \n175  \n3407  \n1  \n0.5  \n0.5  \n0.5  \n1  \n0  \n0.5  \n1  \n0  \n0  \n5  \npipe_color = random.choice([(34, 139, 34), (139, 69, 19), (47, 47, 47)) SyntaxError: closing parenthesis ')' does not match opening parenthesis '[' && pygame.draw.polygon(screen, bird_color, points) ValueError: points argument must contain more than 2 points\\\n\\\nFails quiting. Same color. Collison detection a bit off. No score\\\n\\\nBasic\\\n\\\nIQ2_XXS\\\n\\\n175\\\n\\\n3408\\\n\\\n1\\\n\\\n0.5\\\n\\\n0.5\\\n\\\n0.5\\\n\\\n1\\\n\\\n1\\\n\\\n0.5\\\n\\\n1\\\n\\\n0\\\n\\\n0\\\n\\\n6\\\n\\\npipes.append({'x': SCREEN_WIDTH, 'gap_y': random.randint(50, SCREEN_HEIGHT - 150)) SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\\\n\\\nAcceleration weird. Chooses 1 color per round. Cannot quit.\\\n\\\nBasic\\\n\\\nIQ2_XXS\\\n\\\n175\\\n\\\n3409\\\n\\\n1\\\n\\\n1\\\n\\\n1\\\n\\\n1\\\n\\\n1\\\n\\\n1\\\n\\\n1\\\n\\\n0\\\n\\\n0.5\\\n\\\n0\\\n\\\n7.5\\\n\\\n6.17\\\n\\\nscreen = pygame.display.set_mode((SCREEN_WIDTH, SCREENHEIGHT)) NameError: name 'SCREENHEIGHT' is not defined. Did you mean: 'SCREEN_HEIGHT'?\\\n\\\nOK. Colors change. Best score does not update. Quit only ESC not Q.\\\n\\",
      "metadata": {
        "title": "DeepSeek-R1 Dynamic 1.58-bit | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit"
      }
    },
    {
      "id": "754337a4-954f-4cae-aee2-a57601ffcc7c",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally-deepseek-r1-dynamic-1.58-bit.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit#dynamic-quantization-trial-output) **Dynamic Quantization trial output**\\\n\n\\\nIQ1_S codeIQ1_M codeIQ2_XXS code\\\n\\\n[12KB\\\n\\\ninference_UD-IQ1_S_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FqpBdpW55h5mNAzVoTxPI%2Finference_UD-IQ1_S_3407.txt?alt=media&token=37b19689-73e5-46d0-98be-352e515dfdf8)\\\n\\\n[11KB\\\n\\\ninference_UD-IQ1_S_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FTdIrJSqc2VbNJy1bf3w5%2Finference_UD-IQ1_S_3408.txt?alt=media&token=e11f73bb-80be-49e5-91e2-f3a1f5495dcd)\\\n\\\n[10KB\\\n\\\ninference_UD-IQ1_S_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FBk2ZwEIcLmvZQ3jlMLzw%2Finference_UD-IQ1_S_3409.txt?alt=media&token=052885f5-bee9-420d-a9c0-827412ac17c8)\\\n\\\n[10KB\\\n\\\ninference_UD-IQ1_M_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Ft7YmT1H3Nflcy5kAp1LE%2Finference_UD-IQ1_M_3407.txt?alt=media&token=6f62f911-3364-4f92-b311-c1fa9b759370)\\\n\\\n[30KB\\\n\\\ninference_UD-IQ1_M_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FH6BCTeWlJpUkfeEmeqpu%2Finference_UD-IQ1_M_3408.txt?alt=media&token=7727a999-8c0a-4baf-8542-be8686a01630)\\\n\\\n[9KB\\\n\\\ninference_UD-IQ1_M_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FvVJI0H2F9KTNj5kwUCtC%2Finference_UD-IQ1_M_3409.txt?alt=media&token=0f863d41-53d6-4c94-8d57-bf1eeb79ead5)\\\n\\\n[29KB\\\n\\\ninference_UD-IQ2_XXS_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F26jxRY5mWuon67OfvGtq%2Finference_UD-IQ2_XXS_3407.txt?alt=media&token=daf9bf7d-245e-4b54-b0c0-a6273833835a)\\\n\\\n[34KB\\\n\\\ninference_UD-IQ2_XXS_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FEhjjYN7vAh7gbmR8oXbS%2Finference_UD-IQ2_XXS_3408.txt?alt=media&token=4b50d6dd-2798-44c7-aa92-7e67c09868a4)\\\n\\\n[42KB\\\n\\\ninference_UD-IQ2_XXS_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FXwCSfIf16nTwHzcWepoV%2Finference_UD-IQ2_XXS_3409.txt?alt=media&token=2f7539c9-026d-41e7-b7c7-5738a89ae5d4)\\\n\\",
      "metadata": {
        "title": "DeepSeek-R1 Dynamic 1.58-bit | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit"
      }
    },
    {
      "id": "cc620bff-c6fb-4aff-bce0-942c687aa61d",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally-deepseek-r1-dynamic-1.58-bit.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit#non-dynamic-quantization-trial-output) Non Dynamic Quantization trial output\\\n\n\\\nIQ1_S basic codeIQ1_M basic codeIQ2_XXS basic code\\\n\\\n[25KB\\\n\\\ninference_basic-IQ1_S_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FFtAMzAucSfKMkkmXItTj%2Finference_basic-IQ1_S_3407.txt?alt=media&token=76bfcf47-e1ce-442b-af49-6bfb6af7d046)\\\n\\\n[15KB\\\n\\\ninference_basic-IQ1_S_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F4NhjCVFMwCwT2OCj0IJ5%2Finference_basic-IQ1_S_3408.txt?alt=media&token=d4715674-3347-400b-9eb6-ae5d4470feeb)\\\n\\\n[14KB\\\n\\\ninference_basic-IQ1_S_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fb0ZW3xs7R7IMryO7n7Yp%2Finference_basic-IQ1_S_3409.txt?alt=media&token=64b8825b-7103-4708-9d12-12770e43b546)\\\n\\\n[7KB\\\n\\\ninference_basic-IQ1_M_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FmZ2TsQEzoGjhGlqUjtmj%2Finference_basic-IQ1_M_3407.txt?alt=media&token=975a30d6-2d90-47eb-9d68-b50fd47337f7)\\\n\\\n[7KB\\\n\\\ninference_basic-IQ1_M_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FIx9TQ99Qpmk7BViNLFBl%2Finference_basic-IQ1_M_3408.txt?alt=media&token=b88e1e5b-4535-4d93-bd67-f81def7377d5)\\\n\\\n[12KB\\\n\\\ninference_basic-IQ1_M_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FDX7XYpJPxXKAMZeGhSrr%2Finference_basic-IQ1_M_3409.txt?alt=media&token=6da9127e-272b-4e74-b990-6657e25eea6b)\\\n\\\n[25KB\\\n\\\ninference_basic-IQ2_XXS_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FajsVHsVqlWpwHk7mY32t%2Finference_basic-IQ2_XXS_3407.txt?alt=media&token=cbbf36a2-0d6a-4a87-8232-45b0b7fcc588)\\\n\\\n[34KB\\\n\\\ninference_basic-IQ2_XXS_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F4vjncPu2r2D7F5jVOC7I%2Finference_basic-IQ2_XXS_3408.txt?alt=media&token=9ed635a2-bf97-4f49-b26f-6e985d0ab1b7)\\\n\\\n[34KB\\\n\\\ninference_basic-IQ2_XXS_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FJmVOFgrRyXjY4lYZXE96%2Finference_basic-IQ2_XXS_3409.txt?alt=media&token=faad5bff-ba7f-41f1-abd5-7896f17a5b25)\\\n\\\n[PreviousTutorial: How to Run DeepSeek-R1 Locally](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally) [NextTutorial: How to Run QwQ-32B effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively)\\\n\\\nLast updated 1 month ago\\\n\\\nWas this helpful?\\\n\\\n* * *",
      "metadata": {
        "title": "DeepSeek-R1 Dynamic 1.58-bit | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit"
      }
    },
    {
      "id": "00ff676c-3e43-4315-a93c-8332fd9f81b3",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally.md",
      "content": "---\ntitle: Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\n---  \nA guide on how you can run our 1.58-bit Dynamic Quants for DeepSeek-R1 using llama.cpp.",
      "metadata": {
        "title": "Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally"
      }
    },
    {
      "id": "52d4143a-83c9-4ddb-8495-2148eeaeb632",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally#using-llama.cpp-recommended) Using llama.cpp (recommended)\n\n1. Do not forget about `<ÔΩúUserÔΩú>` and `<ÔΩúAssistantÔΩú>` tokens! - Or use a chat template formatter  \n2. Obtain the latest `llama.cpp` at: [github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp). You can follow the build instructions below as well:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\napt-get update\napt-get install build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```  \n1. It's best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.  \n2. Download the model via:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n# pip install huggingface_hub hf_transfer\n# import os # Optional for faster downloading\n# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\nrepo_id = \"unsloth/DeepSeek-R1-GGUF\",\nlocal_dir = \"DeepSeek-R1-GGUF\",\nallow_patterns = [\"*UD-IQ1_S*\"], # Select quant type UD-IQ1_S for 1.58bit\n)\n```  \n1. Example with Q4_0 K quantized cache **Notice -no-cnv disables auto conversation mode**  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli \\\n--model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n--cache-type-k q4_0 \\\n--threads 12 -no-cnv --prio 2 \\\n--temp 0.6 \\\n--ctx-size 8192 \\\n--seed 3407 \\\n--prompt \"<ÔΩúUserÔΩú>Create a Flappy Bird game in Python.<ÔΩúAssistantÔΩú>\"\n```  \nExample output:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n<think>\nOkay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\nLet me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\nWait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\nI also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\nIs there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\n```  \n1. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli \\\n--model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n--cache-type-k q4_0 \\\n--threads 12 -no-cnv --prio 2 \\\n--n-gpu-layers 7 \\\n--temp 0.6 \\\n--ctx-size 8192 \\\n--seed 3407 \\\n--prompt \"<ÔΩúUserÔΩú>Create a Flappy Bird game in Python.<ÔΩúAssistantÔΩú>\"\n```  \n1. If you want to merge the weights together, use this script:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-gguf-split --merge \\\nDeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\nmerged_file.gguf\n```  \n1. DeepSeek R1 has 61 layers. For example with a 24GB GPU or 80GB GPU, you can expect to offload after rounding down (reduce by 1 if it goes out of memory):  \nQuant  \nFile Size  \n24GB GPU  \n80GB GPU  \n2x80GB GPU  \n1.58bit  \n131GB  \n7  \n33  \nAll layers 61  \n1.73bit  \n158GB  \n5  \n26  \n57  \n2.22bit  \n183GB  \n4  \n22  \n49  \n2.51bit  \n212GB  \n2  \n19  \n32",
      "metadata": {
        "title": "Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally"
      }
    },
    {
      "id": "97c8b1a7-3119-4251-bd26-e5606d544d72",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally#using-llama.cpp-recommended) Using llama.cpp (recommended) > [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally#running-on-mac-apple-devices) Running on Mac / Apple devices\n\nFor Apple Metal devices, be careful of --n-gpu-layers. If you find the machine going out of memory, reduce it. For a 128GB unified memory machine, you should be able to offload 59 layers or so.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli \\\n--model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n--cache-type-k q4_0 \\\n--threads 16 \\\n--prio 2 \\\n--temp 0.6 \\\n--ctx-size 8192 \\\n--seed 3407 \\\n--n-gpu-layers 59 \\\n-no-cnv \\\n--prompt \"<ÔΩúUserÔΩú>Create a Flappy Bird game in Python.<ÔΩúAssistantÔΩú>\"\n```",
      "metadata": {
        "title": "Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally"
      }
    },
    {
      "id": "af2c277b-7822-4375-ab8b-364aff77af7c",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally#using-llama.cpp-recommended) Using llama.cpp (recommended) > [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally#run-in-ollama-open-webui) Run in Ollama/Open WebUI\n\nOpen WebUI has made an step-by-step tutorial on how to run R1 here: [docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/](https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/)  \nIf you want to use Ollama for inference on GGUFs, you need to first merge the 3 GGUF split files into 1 like the code below. Then you will need to run the model locally.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-gguf-split --merge \\\nDeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\nmerged_file.gguf\n```",
      "metadata": {
        "title": "Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally"
      }
    },
    {
      "id": "72865edf-3f38-44d8-bc93-392fc0843429",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally#deepseek-chat-template) DeepSeek Chat Template\n\nAll distilled versions and the main 671B R1 model use the same chat template:  \n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>It's 2.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Explain more!<ÔΩúAssistantÔΩú>`  \nA BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call _tokenizer.encode(..., add_special_tokens = False)_ since the chat template auto adds a BOS token as well.\nFor llama.cpp / GGUF inference, you should skip the BOS since it‚Äôll auto add it.  \n`<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>`  \nThe <think> and </think> tokens get their own designated tokens. For the distilled versions for Qwen and Llama, some tokens are re-mapped, whilst Qwen for example did not have a BOS token, so <|object_ref_start|> had to be used instead.\n**Tokenizer ID Mappings:**  \nToken  \nR1  \nDistill Qwen  \nDistill Llama  \n<think>  \n128798  \n151648  \n128013  \n</think>  \n128799  \n151649  \n128014  \n<|begin_of_sentence|>  \n0  \n151646  \n128000  \n<|end_of_sentence|>  \n1  \n151643  \n128001  \n<|User|>  \n128803  \n151644  \n128011  \n<|Assistant|>  \n128804  \n151645  \n128012  \nPadding token  \n2  \n151654  \n128004  \nOriginal tokens in models:  \nToken  \nQwen 2.5 32B Base  \nLlama 3.3 70B Instruct  \n<think>  \n<|box_start|>  \n<|reserved_special_token_5|>  \n</think>  \n<|box_end|>  \n<|reserved_special_token_6|>  \n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>  \n<|object_ref_start|>  \n<|begin_of_text|>  \n<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>  \n<|endoftext|>  \n<|end_of_text|>  \n<ÔΩúUserÔΩú>  \n<|im_start|>  \n<|reserved_special_token_3|>  \n<ÔΩúAssistantÔΩú>  \n<|im_end|>  \n<|reserved_special_token_4|>  \nPadding token  \n<|vision_pad|>  \n<|finetune_right_pad_id|>  \nAll Distilled and the original R1 versions seem to have accidentally assigned the padding token to <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>, which is mostly not a good idea, especially if you want to further finetune on top of these reasoning models. This will cause endless infinite generations, since most frameworks will mask the EOS token out as -100.  \nWe fixed all distilled and the original R1 versions with the correct padding token (Qwen uses <|vision_pad|>, Llama uses <|finetune_right_pad_id|>, and R1 uses <ÔΩú‚ñÅpad‚ñÅÔΩú> or our own added <ÔΩúPAD‚ñÅTOKENÔΩú>.",
      "metadata": {
        "title": "Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally"
      }
    },
    {
      "id": "16508c39-29ec-480b-939d-50f8a18b5e41",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-deepseek-r1-locally.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally#gguf-r1-table) GGUF R1 Table\n\nMoE Bits  \nType  \nDisk Size  \nAccuracy  \nLink  \nDetails  \n1.58bit  \nUD-IQ1_S  \n**131GB**  \nFair  \n[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S)  \nMoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit  \n1.73bit  \nUD-IQ1_M  \n**158GB**  \nGood  \n[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M)  \nMoE all 1.56bit. `down_proj` in MoE left at 2.06bit  \n2.22bit  \nUD-IQ2_XXS  \n**183GB**  \nBetter  \n[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS)  \nMoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit  \n2.51bit  \nUD-Q2_K_XL  \n**212GB**  \nBest  \n[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL)  \nMoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit  \n[PreviousReinforcement Learning - DPO, ORPO & KTO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto) [NextDeepSeek-R1 Dynamic 1.58-bit](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit)  \nLast updated 1 month ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Tutorial: How to Run DeepSeek-R1 Locally | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally"
      }
    },
    {
      "id": "d7b9ab25-c9ea-4a22-a6fc-4b5ec91088e6",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-gemma-3-effectively.md",
      "content": "---\ntitle: Tutorial: How to Run Gemma 3 effectively | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively\n---  \nGoogle released Gemma 3 in 4 sizes - 1B, 4B, 12B and 27B models! The smallest 1B model is text only, whilst the rest are capable of vision and text input! We provide GGUFs, and a guide of how to run it effectively! View all versions of Gemma 3 on [Hugging Face here](https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b).  \nAccording to the Gemma team, the optimal config for inference is\n`temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0`  \nFor Ollama only, use `temperature = 0.1` instead of 1.0. For any other framework like llama.cpp, Open WebUI etc. use `temperature = 1.0`  \n**Unsloth Gemma 3 uploads with optimal configs:**  \nGGUF  \nBnB 4-bit Instruct  \n16-bit Instruct  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b-it-GGUF)  \n- [4B](https://huggingface.co/unsloth/gemma-3-12b-it-GGUF)  \n- [12B](https://huggingface.co/unsloth/gemma-2-12b-it-GGUF)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b-it-GGUF)  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b-it-bnb-4bit)  \n- [4B](https://huggingface.co/unsloth/gemma-3-4b-it-bnb-4bit)  \n- [12B](https://huggingface.co/unsloth/gemma-3-27b-it-unsloth-bnb-4bit)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b-it-bnb-4bit)  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b)  \n- [4B](https://huggingface.co/unsloth/gemma-3-4b)  \n- [12B](https://huggingface.co/unsloth/gemma-3-12b)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b)  \nWe fixed an issue with our Gemma 3 GGUF uploads where previously they did not support vision. Now they do.",
      "metadata": {
        "title": "Tutorial: How to Run Gemma 3 effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"
      }
    },
    {
      "id": "feafcf10-b683-42e1-bfe5-bdf537127c1c",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-gemma-3-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively#official-recommended-settings) ‚öôÔ∏è Official Recommended Settings\n\nAccording to the Gemma team, the official recommended settings for inference is:  \n- Temperature of 1.0  \n- Top_K of 64  \n- Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)  \n- Top_P of 0.95  \n- Repetition Penalty of 1.0. (1.0 means disabled in llama.cpp and transformers)  \n- Chat template:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\n<bos><start_of_turn>usernHello!<end_of_turn>n<start_of_turn>modelnHey there!<end_of_turn>n<start_of_turn>usernWhat is 1+1?<end_of_turn>n<start_of_turn>modeln\n```  \n- Chat template with `n` newlines rendered (except for the last)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\n<bos><start_of_turn>user\nHello!<end_of_turn>\n<start_of_turn>model\nHey there!<end_of_turn>\n<start_of_turn>user\nWhat is 1+1?<end_of_turn>\n<start_of_turn>modeln\n```  \nllama.cpp an other inference engines auto add a <bos> - DO NOT add TWO <bos> tokens! You should ignore the <bos> when prompting the model!",
      "metadata": {
        "title": "Tutorial: How to Run Gemma 3 effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"
      }
    },
    {
      "id": "eb2568c0-6bf7-4296-a649-fd8acea1debf",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-gemma-3-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively#tutorial-how-to-run-gemma-3-27b-in-ollama) ü¶ô Tutorial: How to Run Gemma 3 27B in Ollama\n\n1. For Ollama use a `temperature = 0.1` instead of 1.0. Install `ollama` if you haven't already!  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\napt-get update\napt-get install pciutils -y\ncurl -fsSL https://ollama.com/install.sh | sh\n```  \n1. Run run the model! Note you can call `ollama serve` in another terminal if it fails! We include all our fixes and suggested parameters (temperature etc) in `params` in our Hugging Face upload!  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nollama run hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_M\n```",
      "metadata": {
        "title": "Tutorial: How to Run Gemma 3 effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"
      }
    },
    {
      "id": "c92b972e-d6fe-4897-8067-5b77cfaa2dbb",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-gemma-3-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively#tutorial-how-to-run-gemma-3-27b-in-llama.cpp) üìñ Tutorial: How to Run Gemma 3 27B in llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\napt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n-DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```  \n1. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose Q4_K_M, or other quantized versions (like BF16 full precision). More versions at: [https://huggingface.co/unsloth/gemma-3-27b-it-GGUF](https://huggingface.co/unsloth/gemma-3-27b-it-GGUF)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n# !pip install huggingface_hub hf_transfer\nimport os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\nrepo_id = \"unsloth/gemma-3-27b-it-GGUF\",\nlocal_dir = \"unsloth/gemma-3-27b-it-GGUF\",\nallow_patterns = [\"*Q4_K_M*\"], # For Q4_K_M\n)\n```  \n1. Run Unsloth's Flappy Bird test  \n2. Edit `--threads 32` for the number of CPU threads, `--ctx-size 16384` for context length (Gemma 3 supports 128K context length!), `--n-gpu-layers 99` for GPU offloading on how many layers. Try adjusting it if your GPU goes out of memory. Also remove it if you have CPU only inference.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli \\\n--model unsloth/gemma-3-27b-it-GGUF/gemma-3-27b-it-Q4_K_M.gguf \\\n--threads 32 \\\n--ctx-size 16384 \\\n--n-gpu-layers 99 \\\n--seed 3407 \\\n--prio 2 \\\n--temp 1.0 \\\n--repeat-penalty 1.0 \\\n--min-p 0.01 \\\n--top-k 64 \\\n--top-p 0.95 \\\n-no-cnv \\\n--prompt \"<start_of_turn>usernCreate a Flappy Bird game in Python. You must include these things:n1. You must use pygame.n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.n3. Pressing SPACE multiple times will accelerate the bird.n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<end_of_turn>n<start_of_turn>modeln\"\n```  \nThe full input from our [https://unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic) 1.58bit blog is:  \nRemember to remove <bos> since Gemma 3 auto adds a <bos>!  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\n<start_of_turn>user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for error\n```  \n[PreviousTutorial: How to Run QwQ-32B effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively) [NextTutorial: How to Finetune Llama-3 and Use In Ollama](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama)  \nLast updated 20 minutes ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Tutorial: How to Run Gemma 3 effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"
      }
    },
    {
      "id": "76f80b08-155f-464a-b4c3-a780c3f49ada",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "---\ntitle: Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\n---  \nQwen released QwQ-32B - a reasoning model with performance comparable to DeepSeek-R1 on many [benchmarks](https://qwenlm.github.io/blog/qwq-32b/). However, people have been experiencing **infinite generations**, **many repetitions**, <think> token issues and finetuning issues. We hope this guide will help debug and fix most issues!  \nOur model uploads with our bug fixes work great for fine-tuning, vLLM and Transformers. If you're using llama.cpp and engines that use llama.cpp as backend, follow our [instructions here](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#tutorial-how-to-run-qwq-32b) to fix endless generations.  \n**Unsloth QwQ-32B uploads with our bug fixes:**  \n[GGUF](https://huggingface.co/unsloth/QwQ-32B-GGUF)  \n[Dynamic 4-bit](https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit)  \n[BnB 4-bit](https://huggingface.co/unsloth/QwQ-32B-bnb-4bit)  \n[16-bit](https://huggingface.co/unsloth/QwQ-32B)",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "e59d9d89-db32-4ae8-aeaf-4c86c478afd8",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#official-recommended-settings) ‚öôÔ∏è Official Recommended Settings\n\nAccording to [Qwen](https://huggingface.co/Qwen/QwQ-32B), these are the recommended settings for inference:  \n- Temperature of 0.6  \n- Top_K of 40 (or 20 to 40)  \n- Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)  \n- Top_P of 0.95  \n- Repetition Penalty of 1.0. (1.0 means disabled in llama.cpp and transformers)  \n- Chat template: `<|im_start|>usernCreate a Flappy Bird game in Python.<|im_end|>n<|im_start|>assistantn<think>n`  \n`llama.cpp` uses `min_p = 0.1` by default, which might cause issues. Force it to 0.0.",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "ed1e5f19-d2e6-4dff-b0e1-c0f9fce99582",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#recommended-settings-for-llama.cpp) üëç Recommended settings for llama.cpp\n\nWe noticed many people use a `Repetition Penalty` greater than 1.0. For example 1.1 to 1.5. This actually interferes with llama.cpp's sampling mechanisms. The goal of a repetition penalty is to penalize repeated generations, but we found this doesn't work as expected.  \nTurning off `Repetition Penalty` also works (ie setting it to 1.0), but we found using it to be useful to penalize endless generations.  \nTo use it, we found you must also edit the ordering of samplers in llama.cpp to before applying `Repetition Penalty`, otherwise there will be endless generations. So add this:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n--samplers \"top_k;top_p;min_p;temperature;dry;typ_p;xtc\"\n```  \nBy default, llama.cpp uses this ordering:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n--samplers \"dry;top_k;typ_p;top_p;min_p;xtc;temperature\"\n```  \nWe reorder essentially temperature and dry, and move min_p forward. This means we apply samplers in this order:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ntop_k=40\ntop_p=0.95\nmin_p=0.0\ntemperature=0.6\ndry\ntyp_p\nxtc\n```  \nIf you still encounter issues, you can increase the `--repeat-penalty 1.0 to 1.2 or 1.3.`  \nCourtesy to [@krist486](https://x.com/krist486/status/1897885598196654180) for bringing llama.cpp sampling directions to my attention.",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "8c5ccf08-84ea-438d-84ae-81a6d9c87642",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#dry-repetition-penalty) ‚òÄÔ∏è Dry Repetition Penalty\n\nWe investigated usage of `dry penalty` as suggested in [https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md](https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md) using a value of 0.8, but we actually found this to **rather cause syntax issues especially for coding**. If you still encounter issues, you can increase the `dry penalty to 0.8.`  \nUtilizing our swapped sampling ordering can also help if you decide to use `dry penalty`.",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "efe57b57-9625-4ffb-9c09-6b93df615558",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#tutorial-how-to-run-qwq-32b-in-ollama) ü¶ô Tutorial: How to Run QwQ-32B in Ollama\n\n1. Install `ollama` if you haven't already!  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\napt-get update\napt-get install pciutils -y\ncurl -fsSL https://ollama.com/install.sh | sh\n```  \n1. Run run the model! Note you can call `ollama serve` in another terminal if it fails! We include all our fixes and suggested parameters (temperature, min_p etc) in `param` in our Hugging Face upload!  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nollama run hf.co/unsloth/QwQ-32B-GGUF:Q4_K_M\n```",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "7e56c7b4-d680-4eb8-ac64-e8c1a6d6c48f",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#tutorial-how-to-run-qwq-32b-in-llama.cpp) üìñ Tutorial: How to Run QwQ-32B in llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\napt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n-DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```  \n1. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose Q4_K_M, or other quantized versions (like BF16 full precision). More versions at: [https://huggingface.co/unsloth/QwQ-32B-GGUF](https://huggingface.co/unsloth/QwQ-32B-GGUF)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n# !pip install huggingface_hub hf_transfer\nimport os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\nrepo_id = \"unsloth/QwQ-32B-GGUF\",\nlocal_dir = \"unsloth-QwQ-32B-GGUF\",\nallow_patterns = [\"*Q4_K_M*\"], # For Q4_K_M\n)\n```  \n1. Run Unsloth's Flappy Bird test, which will save the output to `Q4_K_M_yes_samplers.txt`  \n2. Edit `--threads 32` for the number of CPU threads, `--ctx-size 16384` for context length, `--n-gpu-layers 99` for GPU offloading on how many layers. Try adjusting it if your GPU goes out of memory. Also remove it if you have CPU only inference.  \n3. We use `--repeat-penalty 1.1` and `--dry-multiplier 0.5` which you can adjust.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli \\\n--model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\\n--threads 32 \\\n--ctx-size 16384 \\\n--n-gpu-layers 99 \\\n--seed 3407 \\\n--prio 2 \\\n--temp 0.6 \\\n--repeat-penalty 1.1 \\\n--dry-multiplier 0.5 \\\n--min-p 0.01 \\\n--top-k 40 \\\n--top-p 0.95 \\\n-no-cnv \\\n--samplers \"top_k;top_p;min_p;temperature;dry;typ_p;xtc\" \\\n--prompt \"<|im_start|>usernCreate a Flappy Bird game in Python. You must include these things:n1. You must use pygame.n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.n3. Pressing SPACE multiple times will accelerate the bird.n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>n<|im_start|>assistantn<think>n\" \\\n2>&1 | tee Q4_K_M_yes_samplers.txt\n```  \nThe full input from our [https://unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic) 1.58bit blog is:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n<|im_start|>user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\n<|im_start|>assistant\n<think>\n```  \nThe beginning and the end of the final Python output after removing the thinking parts:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nimport pygame\nimport random\nimport sys\n\npygame.init()\n### Continues\n\nclass Bird:\ndef __init__(self):\n### Continues\n\ndef main():\nbest_score = 0\ncurrent_score = 0\ngame_over = False\npipes = []\nfirst_time = True # Track first game play\n\n# Initial setup\nbackground_color = (173, 216, 230) # Light blue initially\nland_color = random.choice(land_colors)\nbird = Bird()\n\nwhile True:\nfor event in pygame.event.get():\n### Continues\n\nif not game_over:\n# Update bird and pipes\nbird.update()\n### Continues\n\n# Drawing\n### Continues\npygame.display.flip()\nclock.tick(60)\n\nif __name__ == \"__main__\":\nmain()\n```  \nFull final Python output (removed thinking parts): [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#full-final-python-output-removed-thinking-parts)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nimport pygame\nimport random\nimport sys\n\npygame.init()\nWIDTH, HEIGHT = 800, 600\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption(\"Flappy Bird Clone\")\nclock = pygame.time.Clock()\n\n# Colors\npipe_colors = [(0, 100, 0), (210, 180, 140), (50, 50, 50)]\nland_colors = [(139, 69, 19), (255, 255, 0)]\n\n# Game constants\nGRAVITY = 0.5\nPIPE_SPEED = 5\nBIRD_SIZE = 30\nLAND_HEIGHT = 50\nPIPE_WIDTH = 50\nPIPE_GAP = 150\n\nclass Bird:\ndef __init__(self):\nself.x = WIDTH // 2\nself.y = HEIGHT // 2\nself.velocity = 0\nself.shape = random.choice(['square', 'circle', 'triangle'])\nself.color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100))\nself.rect = pygame.Rect(self.x - BIRD_SIZE//2, self.y - BIRD_SIZE//2, BIRD_SIZE, BIRD_SIZE)\n\ndef update(self):\nself.velocity += GRAVITY\nself.y += self.velocity\nself.rect.y = self.y - BIRD_SIZE//2\nself.rect.x = self.x - BIRD_SIZE//2 # Keep x centered\n\ndef draw(self):\nif self.shape == 'square':\npygame.draw.rect(screen, self.color, self.rect)\nelif self.shape == 'circle':\npygame.draw.circle(screen, self.color, (self.rect.centerx, self.rect.centery), BIRD_SIZE//2)\nelif self.shape == 'triangle':\npoints = [\\\n(self.rect.centerx, self.rect.top),\\\n(self.rect.left, self.rect.bottom),\\\n(self.rect.right, self.rect.bottom)\\\n]\npygame.draw.polygon(screen, self.color, points)\n\ndef spawn_pipe():\npipe_x = WIDTH\ntop_height = random.randint(50, HEIGHT - PIPE_GAP - LAND_HEIGHT)\nrect_top = pygame.Rect(pipe_x, 0, PIPE_WIDTH, top_height)\nbottom_y = top_height + PIPE_GAP\nbottom_height = (HEIGHT - LAND_HEIGHT) - bottom_y\nrect_bottom = pygame.Rect(pipe_x, bottom_y, PIPE_WIDTH, bottom_height)\ncolor = random.choice(pipe_colors)\nreturn {\n'rect_top': rect_top,\n'rect_bottom': rect_bottom,\n'color': color,\n'scored': False\n}\n\ndef main():\nbest_score = 0\ncurrent_score = 0\ngame_over = False\npipes = []\nfirst_time = True # Track first game play\n\n# Initial setup\nbackground_color = (173, 216, 230) # Light blue initially\nland_color = random.choice(land_colors)\nbird = Bird()\n\nwhile True:\nfor event in pygame.event.get():\nif event.type == pygame.QUIT:\npygame.quit()\nsys.exit()\nif event.type == pygame.KEYDOWN:\nif event.key == pygame.K_ESCAPE or event.key == pygame.K_q:\npygame.quit()\nsys.exit()\nif event.key == pygame.K_SPACE:\nif game_over:\n# Reset the game\nbird = Bird()\npipes.clear()\ncurrent_score = 0\nif first_time:\n# First restart after initial game over\nbackground_color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255))\nfirst_time = False\nelse:\nbackground_color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255))\nland_color = random.choice(land_colors)\ngame_over = False\nelse:\n# Jump the bird\nbird.velocity = -15 # Initial upward velocity\n\nif not game_over:\n# Update bird and pipes\nbird.update()\n\n# Move pipes left\nremove_pipes = []\nfor pipe in pipes:\npipe['rect_top'].x -= PIPE_SPEED\npipe['rect_bottom'].x -= PIPE_SPEED\n# Check if bird passed the pipe\nif not pipe['scored'] and bird.rect.x > pipe['rect_top'].right:\ncurrent_score += 1\npipe['scored'] = True\n# Check if pipe is offscreen\nif pipe['rect_top'].right < 0:\nremove_pipes.append(pipe)\n# Remove offscreen pipes\nfor p in remove_pipes:\npipes.remove(p)\n\n# Spawn new pipe if needed\nif not pipes or pipes[-1]['rect_top'].x < WIDTH - 200:\npipes.append(spawn_pipe())\n\n# Check collisions\nland_rect = pygame.Rect(0, HEIGHT - LAND_HEIGHT, WIDTH, LAND_HEIGHT)\nbird_rect = bird.rect\n# Check pipes\nfor pipe in pipes:\nif bird_rect.colliderect(pipe['rect_top']) or bird_rect.colliderect(pipe['rect_bottom']):\ngame_over = True\nbreak\n# Check land and top\nif bird_rect.bottom >= land_rect.top or bird_rect.top <= 0:\ngame_over = True\n\nif game_over:\nif current_score > best_score:\nbest_score = current_score\n\n# Drawing\nscreen.fill(background_color)\n# Draw pipes\nfor pipe in pipes:\npygame.draw.rect(screen, pipe['color'], pipe['rect_top'])\npygame.draw.rect(screen, pipe['color'], pipe['rect_bottom'])\n# Draw land\npygame.draw.rect(screen, land_color, (0, HEIGHT - LAND_HEIGHT, WIDTH, LAND_HEIGHT))\n# Draw bird\nbird.draw()\n# Draw score\nfont = pygame.font.SysFont(None, 36)\nscore_text = font.render(f'Score: {current_score}', True, (0, 0, 0))\nscreen.blit(score_text, (WIDTH - 150, 10))\n# Game over screen\nif game_over:\nover_text = font.render('Game Over!', True, (255, 0, 0))\nbest_text = font.render(f'Best: {best_score}', True, (255, 0, 0))\nrestart_text = font.render('Press SPACE to restart', True, (255, 0, 0))\nscreen.blit(over_text, (WIDTH//2 - 70, HEIGHT//2 - 30))\nscreen.blit(best_text, (WIDTH//2 - 50, HEIGHT//2 + 10))\nscreen.blit(restart_text, (WIDTH//2 - 100, HEIGHT//2 + 50))\n\npygame.display.flip()\nclock.tick(60)\n\nif __name__ == \"__main__\":\nmain()\n```  \n1. When running it, we get a runnable game!  \n!  \n1. Now try the same without our fixes! So remove `--samplers \"top_k;top_p;min_p;temperature;dry;typ_p;xtc\"` This will save the output to `Q4_K_M_no_samplers.txt`  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli \\\n--model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\\n--threads 32 \\\n--ctx-size 16384 \\\n--n-gpu-layers 99 \\\n--seed 3407 \\\n--prio 2 \\\n--temp 0.6 \\\n--repeat-penalty 1.1 \\\n--dry-multiplier 0.5 \\\n--min-p 0.01 \\\n--top-k 40 \\\n--top-p 0.95 \\\n-no-cnv \\\n--prompt \"<|im_start|>usernCreate a Flappy Bird game in Python. You must include these things:n1. You must use pygame.n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.n3. Pressing SPACE multiple times will accelerate the bird.n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>n<|im_start|>assistantn<think>n\" \\\n2>&1 | tee Q4_K_M_no_samplers.txt\n```  \nYou will get some looping, but **problematically incorrect Python syntax** and many other issues. For example the below looks correct, but is wrong! Ie line 39 `pipes.clear() ### <<< NameError: name 'pipes' is not defined. Did you forget to import 'pipes'?`  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\nimport pygame\nimport random\n\npygame.init()\n\n# Constants\nWIDTH, HEIGHT = 800, 600\nGROUND_HEIGHT = 20\nGRAVITY = 0.7\nPIPE_SPEED = -3\nBIRD_SIZE = 45\nMIN_GAP = 130\nMAX_GAP = 200\nPIPE_COLORS = [(0, 96, 0), (205, 133, 63), (89, 97, 107)]\nDARK_BROWN = (94, 72, 4)\nYELLOW = (252, 228, 6)\n\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\nclock = pygame.time.Clock()\n\ndef random_light_color():\nreturn (\nrandom.randint(180, 230),\nrandom.randint(190, 300),\nrandom.randint(250, 255)\n)\n\ndef reset_game():\nglobal bird_x, bird_y\nglobal pipes, score\nglobal background_color, land_color\nglobal bird_shape, bird_color\n\n# Bird properties\nbird_x = WIDTH * 0.3\nbird_y = HEIGHT // 2\nbird_vel = -5 # Initial upward thrust\n\npipes.clear() ### <<< NameError: name 'pipes' is not defined. Did you forget to import 'pipes'?\n```  \n1. If you use `--repeat-penalty 1.5`, it gets even worse and more obvious, with actually totally incorrect syntax.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nimport pygame\nfrom random import randint # For generating colors/shapes/positions randomly\npygame.init()\n\n# Constants:\nWIDTH, HEIGHT =456 ,702 #\nBACKGROUND_COLOR_LIGHTS=['lightskyblue']\nGAP_SIZE=189 #\n\nBIRD_RADIUS=3.\nPIPE_SPEED=- ( ) ?\nclass Game():\ndef __init__(self):\nself.screen_size=( )\n\ndef reset_game_vars():\nglobal current_scor e\n# set to zero and other initial states.\n\n# Main game loop:\nwhile running :\nfor event in pygame.event.get() :\nif quit ... etc\n\npygame.quit()\nprint(\"Code is simplified. Due time constraints, full working version requires further implementation.\")\n```  \n1. You might be wondering maybe it's Q4_K_M? B16 ie full precision should work fine right? Incorrect - the outputs again fail if we do not use our fix of - `-samplers \"top_k;top_p;min_p;temperature;dry;typ_p;xtc\"` when using a Repetition Penalty.",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "863261a3-e40e-45c7-a6ac-9c747573afc8",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#still-doesnt-work-try-min_p-0.1-temperature-1.5) üåÑ Still doesn't work? Try Min_p = 0.1, Temperature = 1.5\n\nAccording to the Min_p paper [https://arxiv.org/pdf/2407.01082](https://arxiv.org/pdf/2407.01082), for more creative and diverse outputs, and if you still see repetitions, try disabling top_p and top_k!  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\\n--threads 32 --n-gpu-layers 99 \\\n--ctx-size 16384 \\\n--temp 1.5 \\\n--min-p 0.1 \\\n--top-k 0 \\\n--top-p 1.0 \\\n-no-cnv \\\n--prompt \"<|im_start|>usernCreate a Flappy Bird game in Python. You must include these things:n1. You must use pygame.n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.n3. Pressing SPACE multiple times will accelerate the bird.n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>n<|im_start|>assistantn<think>n\"\n```  \nAnother approach is to disable `min_p` directly, since llama.cpp by default uses `min_p = 0.1`!  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n./llama.cpp/llama-cli --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\\n--threads 32 --n-gpu-layers 99 \\\n--ctx-size 16384 \\\n--temp 0.6 \\\n--min-p 0.0 \\\n--top-k 40 \\\n--top-p 0.95 \\\n-no-cnv \\\n--prompt \"<|im_start|>usernCreate a Flappy Bird game in Python. You must include these things:n1. You must use pygame.n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.n3. Pressing SPACE multiple times will accelerate the bird.n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>n<|im_start|>assistantn<think>n\"\n```",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "28ae70b2-1ade-414d-b991-5adc877f351b",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#less-than-think-greater-than-token-not-shown) ü§î <think> token not shown?\n\nSome people are reporting that because <think> is default added in the chat template, some systems are not outputting the thinking traces correctly. You will have to manually edit the Jinja template from:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\n{%- if tools %} {{- '<|im_start|>systemn' }} {%- if messages[0]['role'] == 'system' %} {{- messages[0]['content'] }} {%- else %} {{- '' }} {%- endif %} {{- \"nn# ToolsnnYou may call one or more functions to assist with the user query.nnYou are provided with function signatures within <tools></tools> XML tags:n<tools>\" }} {%- for tool in tools %} {{- \"n\" }} {{- tool | tojson }} {%- endfor %} {{- \"n</tools>nnFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:n<tool_call>n{\"name\": <function-name>, \"arguments\": <args-json-object>}n</tool_call><|im_end|>n\" }} {%- else %} {%- if messages[0]['role'] == 'system' %} {{- '<|im_start|>systemn' + messages[0]['content'] + '<|im_end|>n' }} {%- endif %} {%- endif %} {%- for message in messages %} {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %} {{- '<|im_start|>' + message.role + 'n' + message.content + '<|im_end|>' + 'n' }} {%- elif message.role == \"assistant\" and not message.tool_calls %} {%- set content = message.content.split('</think>')[-1].lstrip('n') %} {{- '<|im_start|>' + message.role + 'n' + content + '<|im_end|>' + 'n' }} {%- elif message.role == \"assistant\" %} {%- set content = message.content.split('</think>')[-1].lstrip('n') %} {{- '<|im_start|>' + message.role }} {%- if message.content %} {{- 'n' + content }} {%- endif %} {%- for tool_call in message.tool_calls %} {%- if tool_call.function is defined %} {%- set tool_call = tool_call.function %} {%- endif %} {{- 'n<tool_call>n{\"name\": \"' }} {{- tool_call.name }} {{- '\", \"arguments\": ' }} {{- tool_call.arguments | tojson }} {{- '}n</tool_call>' }} {%- endfor %} {{- '<|im_end|>n' }} {%- elif message.role == \"tool\" %} {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %} {{- '<|im_start|>user' }} {%- endif %} {{- 'n<tool_response>n' }} {{- message.content }} {{- 'n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %} {{- '<|im_end|>n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<|im_start|>assistantn<think>n' }} {%- endif %}\n```  \nto another by removing the `<think>n` at the end. The model will now have to manually add `<think>n` during inference, which might not always succeed. DeepSeek also edited all models to default add a `<think>` token to force the model to go into reasoning model.  \nSo change `{%- if add_generation_prompt %} {{- '<|im_start|>assistantn<think>n' }} {%- endif %} ` to `{%- if add_generation_prompt %} {{- '<|im_start|>assistantn' }} {%- endif %}` ie remove `<think>n`  \nFull jinja template with removed <think>\\n part [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#full-jinja-template-with-removed-less-than-think-greater-than-n-part)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\n{%- if tools %} {{- '<|im_start|>systemn' }} {%- if messages[0]['role'] == 'system' %} {{- messages[0]['content'] }} {%- else %} {{- '' }} {%- endif %} {{- \"nn# ToolsnnYou may call one or more functions to assist with the user query.nnYou are provided with function signatures within <tools></tools> XML tags:n<tools>\" }} {%- for tool in tools %} {{- \"n\" }} {{- tool | tojson }} {%- endfor %} {{- \"n</tools>nnFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:n<tool_call>n{\"name\": <function-name>, \"arguments\": <args-json-object>}n</tool_call><|im_end|>n\" }} {%- else %} {%- if messages[0]['role'] == 'system' %} {{- '<|im_start|>systemn' + messages[0]['content'] + '<|im_end|>n' }} {%- endif %} {%- endif %} {%- for message in messages %} {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %} {{- '<|im_start|>' + message.role + 'n' + message.content + '<|im_end|>' + 'n' }} {%- elif message.role == \"assistant\" and not message.tool_calls %} {%- set content = message.content.split('</think>')[-1].lstrip('n') %} {{- '<|im_start|>' + message.role + 'n' + content + '<|im_end|>' + 'n' }} {%- elif message.role == \"assistant\" %} {%- set content = message.content.split('</think>')[-1].lstrip('n') %} {{- '<|im_start|>' + message.role }} {%- if message.content %} {{- 'n' + content }} {%- endif %} {%- for tool_call in message.tool_calls %} {%- if tool_call.function is defined %} {%- set tool_call = tool_call.function %} {%- endif %} {{- 'n<tool_call>n{\"name\": \"' }} {{- tool_call.name }} {{- '\", \"arguments\": ' }} {{- tool_call.arguments | tojson }} {{- '}n</tool_call>' }} {%- endfor %} {{- '<|im_end|>n' }} {%- elif message.role == \"tool\" %} {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %} {{- '<|im_start|>user' }} {%- endif %} {{- 'n<tool_response>n' }} {{- message.content }} {{- 'n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %} {{- '<|im_end|>n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<|im_start|>assistantn' }} {%- endif %}\n```",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "7b00fbe0-7804-4a66-8385-279568bee77c",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#extra-notes) Extra Notes\n\nWe first thought maybe:  \n1. QwQ's context length was not natively 128K, but rather 32K with YaRN extension. For example in the readme file for [https://huggingface.co/Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B), we see:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\n```  \nWe tried overriding llama.cpp's YaRN handling, but nothing changed.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\n--override-kv qwen2.context_length=int:131072 \\\n--override-kv qwen2.rope.scaling.type=str:yarn \\\n--override-kv qwen2.rope.scaling.factor=float:4 \\\n--override-kv qwen2.rope.scaling.original_context_length=int:32768 \\\n--override-kv qqwen2.rope.scaling.attn_factor=float:1.13862943649292 \\\n```  \n1. We also thought maybe the RMS Layernorm epsilon was wrong - not 1e-5 but maybe 1e-6. For example [this](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct/blob/main/config.json) has `rms_norm_eps=1e-06`, whilst [this](https://huggingface.co/Qwen/Qwen2.5-32B/blob/main/config.json) has `rms_norm_eps=1e-05` . We also overrided it, but it did not work:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap\n--override-kv qwen2.attention.layer_norm_rms_epsilon=float:0.000001 \\\n```  \n1. We also tested if tokenizer IDs matched between llama.cpp and normal Transformers courtesy of [@kalomaze](https://x.com/kalomaze/status/1897875332230779138). They matched, so this was not the culprit.  \nWe provide our experimental results below:  \n[61KB\\\n\\\nfile_BF16_no_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FeABgnEXerhmNw1jzUmrr%2Ffile_BF16_no_samplers.txt?alt=media&token=d11aa8f8-0ff7-4370-9412-6129bd980a42)  \nBF16 full precision with no sampling fix  \n[55KB\\\n\\\nfile_BF16_yes_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fv01qqEwj6PHVE9VvPzfg%2Ffile_BF16_yes_samplers.txt?alt=media&token=d8ecf5bf-b4f2-4abe-a0b4-26d7e8e862f9)  \nBF16 full precision with sampling fix  \n[71KB\\\n\\\nfinal_Q4_K_M_no_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fi3eSz0NWvc44CkRUanrY%2Ffinal_Q4_K_M_no_samplers.txt?alt=media&token=deca70bd-fc21-44a9-b42c-87837ac3a8ce)  \nQ4_K_M precision with no sampling fix  \n[65KB\\\n\\\nfinal_Q4_K_M_yes_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FBtdJmKQjMZVlpO1HfWE7%2Ffinal_Q4_K_M_yes_samplers.txt?alt=media&token=f266d668-71ab-436d-8c05-b720e56e348e)  \nQ4_K_M precision with sampling fix",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "80093573-e136-41d0-b5e5-98d5e8a1f49d",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#tokenizer-bug-fixes) ‚úèÔ∏è Tokenizer Bug Fixes\n\n- We found a few issues as well specifically impacting finetuning! The EOS token is correct, but the PAD token should probably rather be `\"<|vision_pad|>`\" We updated it in: [https://huggingface.co/unsloth/QwQ-32B/blob/main/tokenizer_config.json](https://huggingface.co/unsloth/QwQ-32B/blob/main/tokenizer_config.json)  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n\"eos_token\": \"<|im_end|>\",\n\"pad_token\": \"<|endoftext|>\",\n```",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "43f07dfc-c6f1-4699-92a0-c62ad10706fa",
      "source": "firecrawl\\docs\\basics-tutorial-how-to-run-qwq-32b-effectively.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#dynamic-4-bit-quants) üõ†Ô∏è Dynamic 4-bit Quants\n\nWe also uploaded dynamic 4bit quants which increase accuracy vs naive 4bit quantizations! We attach the QwQ quantization error plot analysis for both activation and weight quantization errors:  \n!  \nWe uploaded dynamic 4-bit quants to: [https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit](https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit)  \nSince vLLM 0.7.3 (2025 February 20th) [https://github.com/vllm-project/vllm/releases/tag/v0.7.3](https://github.com/vllm-project/vllm/releases/tag/v0.7.3), vLLM now supports loading Unsloth dynamic 4bit quants!  \nAll our GGUFs are at [https://huggingface.co/unsloth/QwQ-32B-GGUF](https://huggingface.co/unsloth/QwQ-32B-GGUF)!  \n[PreviousDeepSeek-R1 Dynamic 1.58-bit](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit) [NextTutorial: How to Run Gemma 3 effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively)  \nLast updated 4 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Tutorial: How to Run QwQ-32B effectively | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"
      }
    },
    {
      "id": "85d4e7bc-9d5a-438c-9e09-3cf272b04bda",
      "source": "firecrawl\\docs\\basics-unsloth-benchmarks.md",
      "content": "---\ntitle: Unsloth Benchmarks | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/unsloth-benchmarks\n---  \n- For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).  \n- Benchmarking of Unsloth was also conducted by [ü§óHugging Face](https://huggingface.co/blog/unsloth-trl).  \nWe tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):  \nModel  \nVRAM  \nü¶•Unsloth speed  \nü¶•VRAM reduction  \nü¶•Longer context  \nüòäHugging Face + FA2  \nLlama 3.3 (70B)  \n80GB  \n2x  \n>75%  \n13x longer  \n1x  \nLlama 3.1 (8B)  \n80GB  \n2x  \n>70%  \n12x longer  \n1x",
      "metadata": {
        "title": "Unsloth Benchmarks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/unsloth-benchmarks"
      }
    },
    {
      "id": "a87f8380-6819-4024-a604-5785f18f8a05",
      "source": "firecrawl\\docs\\basics-unsloth-benchmarks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks#context-length-benchmarks) Context length benchmarks\n\nThe more data you have, the less VRAM Unsloth uses due to our [gradient checkpointing](https://unsloth.ai/blog/long-context) algorithm + Apple's CCE algorithm!",
      "metadata": {
        "title": "Unsloth Benchmarks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/unsloth-benchmarks"
      }
    },
    {
      "id": "1ed48404-4066-4865-8bbd-990656bc77e9",
      "source": "firecrawl\\docs\\basics-unsloth-benchmarks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks#context-length-benchmarks) Context length benchmarks > [Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks#llama-3.1-8b-max.-context-length) **Llama 3.1 (8B) max. context length**\n\nWe tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.  \nGPU VRAM  \nü¶•Unsloth context length  \nHugging Face + FA2  \n8 GB  \n2,972  \nOOM  \n12 GB  \n21,848  \n932  \n16 GB  \n40,724  \n2,551  \n24 GB  \n78,475  \n5,789  \n40 GB  \n153,977  \n12,264  \n48 GB  \n191,728  \n15,502  \n80 GB  \n342,733  \n28,454",
      "metadata": {
        "title": "Unsloth Benchmarks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/unsloth-benchmarks"
      }
    },
    {
      "id": "82c914ab-52ea-41ba-b84e-20a6e2b04631",
      "source": "firecrawl\\docs\\basics-unsloth-benchmarks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks#context-length-benchmarks) Context length benchmarks > [Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks#llama-3.3-70b-max.-context-length) **Llama 3.3 (70B) max. context length**\n\nWe tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.  \nGPU VRAM  \nü¶•Unsloth context length  \nHugging Face + FA2  \n48 GB  \n12,106  \nOOM  \n80 GB  \n89,389  \n6,916  \n[PreviousErrors/Troubleshooting](https://docs.unsloth.ai/basics/errors-troubleshooting)  \nLast updated 1 month ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Unsloth Benchmarks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/unsloth-benchmarks"
      }
    },
    {
      "id": "f085648c-d9e9-42b4-ae08-f05faa7452e9",
      "source": "firecrawl\\docs\\basics-vision-fine-tuning.md",
      "content": "---\ntitle: Vision Fine-tuning | Unsloth Documentation\nurl: https://docs.unsloth.ai/basics/vision-fine-tuning\n---  \nFine-tuning vision models has numerous use cases across various industries, enabling models to adapt to specific tasks and datasets. We provided 3 example notebooks for vision finetuning.  \n1. **Llama 3.2 Vision** finetuning for radiography: [Notebook](https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing)\nHow can we assist medical professionals in analyzing Xrays, CT Scans & ultrasounds faster.  \n2. **Qwen 2 VL** finetuning for converting handwriting to LaTeX: [Notebook](https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing)\nThis allows complex math formulas to be easily transcribed as LaTeX without manually writing it.  \n3. **Pixtral 12B 2409** vision finetuning for general Q&A: [Notebook](https://colab.research.google.com/drive/1K9ZrdwvZRE96qGkCq_e88FgV3MLnymQq?usp=sharing)\nOne can concatenate general Q&A datasets with more niche datasets to make the finetune not forget base model skills.  \nIt is best to ensure your dataset has images of all the same size/dimensions. Use dimensions of 300-1000px to ensure your training does not take too long or use too many resources.  \nTo finetune vision models, we now allow you to select which parts of the mode to finetune. You can select to only finetune the vision layers, or the language layers, or the attention / MLP layers! We set them all on by default!  \n!  \n[PreviousChat Templates](https://docs.unsloth.ai/basics/chat-templates) [NextFinetuning from Last Checkpoint](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint)  \nLast updated 25 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Vision Fine-tuning | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/basics/vision-fine-tuning"
      }
    },
    {
      "id": "073a0909-644b-4eab-bf5d-ce44c3cf7d87",
      "source": "firecrawl\\docs\\get-started-all-our-models.md",
      "content": "---\ntitle: All Our Models | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/all-our-models\n---  \nSee the table below for all GGUF, 16-bit and 4-bit bnb uploaded models on [Hugging Face](https://huggingface.co/unsloth).  \n- GGUFs can be used to run in your favorite places like Ollama, Open WebUI and llama.cpp.  \n- 4-bit and 16-bit models can be used for inference serving or fine-tuning.  \n‚Ä¢ GGUF + 4-bit ‚Ä¢ 16-bit original  \nHere's a table of all our GGUF + 4-bit model uploads:  \nModel  \nGGUF  \nInstruct (4-bit)  \nBase (4-bit)  \n[Gemma 3](https://docs.unsloth.ai/) (new)  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b-it-GGUF)  \n- [4B](https://huggingface.co/unsloth/gemma-3-12b-it-GGUF)  \n- [12B](https://huggingface.co/unsloth/gemma-2-12b-it-GGUF)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b-it-GGUF)  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b-it-unsloth-bnb-4bit)  \n- [4B](https://huggingface.co/unsloth/gemma-3-4b-it-unsloth-bnb-4bit)  \n- [12B](https://huggingface.co/unsloth/gemma-3-12b-it-unsloth-bnb-4bit)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b-it-unsloth-bnb-4bit)  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b-pt-bnb-4bit)  \n- [4B](https://huggingface.co/unsloth/gemma-3-4b-pt-bnb-4bit)  \n- [12B](https://huggingface.co/unsloth/gemma-3-12b-pt-bnb-4bit)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b-pt-bnb-4bit)  \n[DeepSeek-R1](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5) (new)  \n- [R1](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)  \n- [R1 Zero](https://huggingface.co/unsloth/DeepSeek-R1-Zero-GGUF)  \n- [Llama 3 (8B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF)  \n- [Llama 3.3 (70B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF)  \n- [Qwen 2.5 (14B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF)  \n- [Qwen 2.5 (32B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF)  \n- [Qwen 2.5 (1.5B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF)  \n- [Qwen 2.5 (7B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF)  \n- [Llama 3 (8B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit)  \n- [Llama 3.3 (70B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit)  \n- [Qwen 2.5 (14B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit)  \n- [Qwen 2.5 (32B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit)  \n- [Qwen 2.5 (1.5B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit)  \n- [Qwen 2.5 (7B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit)  \n[Phi-4](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa) (new)  \n- [Phi-4](https://huggingface.co/unsloth/phi-4-GGUF)  \n- [Phi-4-mini](https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF)  \n- [Phi-4](https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit)  \n- [Phi-4-mini](https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit)  \n[QwQ-32B](https://huggingface.co/collections/unsloth/qwen-qwq-32b-collection-676b3b29c20c09a8c71a6235) (new)  \n- [32B](https://huggingface.co/unsloth/QwQ-32B-GGUF)  \n- [32B](https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit)  \n[Llama 3.2](https://huggingface.co/collections/unsloth/llama-32-66f46afde4ca573864321a22)  \n- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF)  \n- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF)  \n- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-bnb-4bit)  \n- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-bnb-4bit)  \n- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit)  \n- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit)  \n- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-bnb-4bit)  \n- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-bnb-4bit)  \n- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-unsloth-bnb-4bit)  \n- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision-bnb-4bit)  \n[Llama 3.3](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f)  \n- [70B](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF)  \n- [70B](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-bnb-4bit)  \n[Llama 3.1](https://huggingface.co/collections/unsloth/llama-31-collection-6753dca76f47d9ce1696495f)  \n- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit)  \n- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit)  \n- [405B](https://huggingface.co/unsloth/Meta-Llama-3.1-405B-Instruct-bnb-4bit/)  \n- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit)  \n- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B-bnb-4bit)  \n- [405B](https://huggingface.co/unsloth/Meta-Llama-3.1-405B-bnb-4bit)  \nMistral  \n- [Small 2501 (24B)](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501-GGUF) - new  \n- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Instruct-2407-GGUF)  \n- [Small 2501 (24B)](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit) - new  \n- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit)  \n- [Small 2409 (22B)](https://huggingface.co/unsloth/Mistral-Small-Instruct-2409-bnb-4bit)  \n- [Large 2407](https://huggingface.co/unsloth/Mistral-Large-Instruct-2407-bnb-4bit)  \n- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.3-bnb-4bit)  \n- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.2-bnb-4bit)  \n- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-2409-bnb-4bit)  \n- [Small 2501 (24B)](https://huggingface.co/unsloth/Mistral-Small-24B-Base-2501-unsloth-bnb-4bit) - new  \n- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Base-2407-bnb-4bit)  \n- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-v0.3-bnb-4bit)  \n- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-v0.2-bnb-4bit)  \n- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-2409-unsloth-bnb-4bit)  \n[DeepSeek V3](https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c)  \n- [2 to 8-bit](https://huggingface.co/unsloth/DeepSeek-V3-GGUF)  \n[Qwen2.5-VL](https://huggingface.co/collections/unsloth/qwen25-vl-all-versions-679ca6c784fad5bd976a05a1) (new)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit)  \n- [72B](https://huggingface.co/unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct)  \n- [72B](https://huggingface.co/unsloth/Qwen2.5-VL-72B-Instruct)  \n[Qwen 2.5](https://huggingface.co/collections/unsloth/qwen-25-66fe4c08fb9ada518e8a0d3f)  \n- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-3B-Instruct-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-bnb-4bit)  \n- [14B](https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-bnb-4bit)  \n- [32B](https://huggingface.co/unsloth/Qwen2.5-32B-Instruct-bnb-4bit)  \n- [72B](https://huggingface.co/unsloth/Qwen2-72B-Instruct-bnb-4bit)  \n- [QwQ](https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit)  \n- [QVQ](https://huggingface.co/unsloth/QVQ-72B-Preview-bnb-4bit)  \n- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B-bnb-4bit)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B-bnb-4bit)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-3B-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-7B-bnb-4bit)  \n- [14B](https://huggingface.co/unsloth/Qwen2.5-14B-bnb-4bit)  \n- [32B](https://huggingface.co/unsloth/Qwen2.5-32B-bnb-4bit)  \n- [72B](https://huggingface.co/unsloth/Qwen2.5-72B-bnb-4bit)  \nGemma 2  \n- [All variants](https://huggingface.co/unsloth/gemma-2-it-GGUF)  \n- [2B](https://huggingface.co/unsloth/gemma-2-2b-it-bnb-4bit)  \n- [9B](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit)  \n- [27B](https://huggingface.co/unsloth/gemma-2-27b-it-bnb-4bit)  \n- [2B](https://huggingface.co/unsloth/gemma-2-2b-bnb-4bit)  \n- [9B](https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit)  \n- [27B](https://huggingface.co/unsloth/gemma-2-27b-bnb-4bit)  \nPhi-3.5  \n- [mini](https://huggingface.co/unsloth/Phi-3.5-mini-instruct-bnb-4bit)  \nPhi-3  \n- [mini](https://huggingface.co/unsloth/Phi-3-mini-4k-instruct-bnb-4bit)  \n- [medium](https://huggingface.co/unsloth/Phi-3-medium-4k-instruct-bnb-4bit)  \nLlama 3  \n- [8B](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)  \n- [70B](https://huggingface.co/unsloth/llama-3-70b-bnb-4bit)  \n- [8B](https://huggingface.co/unsloth/llama-3-8b-bnb-4bit)  \n- [70B](https://huggingface.co/unsloth/llama-3-70b-bnb-4bit)  \nLlava  \n- [1.5 (7B)](https://huggingface.co/unsloth/llava-1.5-7b-hf-bnb-4bit)  \n- [1.6 Mistral (7B)](https://huggingface.co/unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit)  \n[Qwen 2.5 Coder](https://huggingface.co/collections/unsloth/qwen-25-coder-6732bc833ed65dd1964994d4)  \n- [0.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-0.5B-Instruct-128K-GGUF)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-1.5B-Instruct-128K-GGUF)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-Coder-3B-Instruct-128K-GGUF)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF)  \n- [14B](https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-128K-GGUF)  \n- [32B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-Instruct-128K-GGUF)  \n- [0.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit)  \n- [14B](https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit)  \n- [32B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit)  \n- [0.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-0.5B-bnb-4bit)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-1.5B-bnb-4bit)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-Coder-3B-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-Coder-7B-bnb-4bit)  \n- [14B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-bnb-4bit)  \n- [32B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-bnb-4bit)  \nQwen2 VL  \n- [2B](https://huggingface.co/unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit)  \n- [72B](https://huggingface.co/unsloth/Qwen2-VL-72B-Instruct-bnb-4bit)  \nLlama 2  \n- [7B](https://huggingface.co/unsloth/llama-2-7b-chat-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/llama-2-7b-bnb-4bit)  \n- [13B](https://huggingface.co/unsloth/llama-2-13b-bnb-4bit)  \nSmolLM2  \n- [135M](https://huggingface.co/unsloth/SmolLM2-135M-Instruct-GGUF)  \n- [360M](https://huggingface.co/unsloth/SmolLM2-360M-Instruct-GGUF)  \n- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-Instruct-GGUF)  \n- [135M](https://huggingface.co/unsloth/SmolLM2-135M-Instruct-bnb-4bit)  \n- [360M](https://huggingface.co/unsloth/SmolLM2-360M-Instruct-bnb-4bit)  \n- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-Instruct-bnb-4bit)  \n- [135M](https://huggingface.co/unsloth/SmolLM2-135M-bnb-4bit)  \n- [360M](https://huggingface.co/unsloth/SmolLM2-360M-bnb-4bit)  \n- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-bnb-4bit)  \nTinyLlama  \n- [Instruct](https://huggingface.co/unsloth/tinyllama-chat-bnb-4bit)  \n- [Base](https://huggingface.co/unsloth/tinyllama-bnb-4bit)  \nQwen2  \n- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B-Instruct-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2-7B-Instruct-bnb-4bit)  \n- [72B](https://huggingface.co/unsloth/Qwen2-72B-Instruct-bnb-4bit)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B-bnb-4bit)  \n- [7B](https://huggingface.co/unsloth/Qwen2-7B-bnb-4bit)  \n- [72B](https://huggingface.co/unsloth/Qwen2-7B-bnb-4bit)  \nZephyr SFT  \n- [Instruct](https://huggingface.co/unsloth/zephyr-sft-bnb-4bit)  \nCodeLlama  \n- [7B](https://huggingface.co/unsloth/codellama-7b-bnb-4bit)  \n- [13B](https://huggingface.co/unsloth/codellama-13b-bnb-4bit)  \n- [34B](https://huggingface.co/unsloth/codellama-34b-bnb-4bit)  \nYi  \n- [34B](https://huggingface.co/unsloth/yi-34b-chat-bnb-4bit)  \n- [6B (v 1.5)](https://huggingface.co/unsloth/Yi-1.5-6B-bnb-4bit)  \n- [6B](https://huggingface.co/unsloth/yi-6b-bnb-4bit)  \n- [34B](https://huggingface.co/unsloth/yi-34b-bnb-4bit)  \nHere's a table of all our 16-bit or 8-bit original model uploads:  \nModel  \nInstruct  \nBase  \n[Gemma 3](https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b) (new)  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b-it)  \n- [4B](https://huggingface.co/unsloth/gemma-3-4b-it)  \n- [12B](https://huggingface.co/unsloth/gemma-3-12b-it)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b-it)  \n- [1B](https://huggingface.co/unsloth/gemma-3-1b-pt)  \n- [4B](https://huggingface.co/unsloth/gemma-3-4b-pt)  \n- [12B](https://huggingface.co/unsloth/gemma-3-12b-pt)  \n- [27B](https://huggingface.co/unsloth/gemma-3-27b-pt)  \n[DeepSeek-R1 (new)](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5)  \n- [R1](https://huggingface.co/unsloth/DeepSeek-R1)  \n- [R1 Zero](https://huggingface.co/unsloth/DeepSeek-R1-Zero)  \n- [Llama 3 (8B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B)  \n- [Llama 3.3 (70B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B)  \n- [Qwen 2.5 (14B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B)  \n- [Qwen 2.5 (32B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B)  \n- [Qwen 2.5 (1.5B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B)  \n- [Qwen 2.5 (7B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B)  \n- [R1 (bf16)](https://huggingface.co/unsloth/DeepSeek-R1-BF16)  \n[Llama 3.2](https://huggingface.co/collections/unsloth/llama-32-66f46afde4ca573864321a22)  \n- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct)  \n- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-bnb-4bit)  \n- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-Instruct)  \n- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision-Instruct)  \n- [1B](https://huggingface.co/unsloth/Llama-3.2-1B)  \n- [3B](https://huggingface.co/unsloth/Llama-3.2-3B)  \n- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision)  \n- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision)  \n[Llama 3.3](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f)  \n- [70B](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct)  \n[Phi-4](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa) (new)  \n- [Phi-4](https://huggingface.co/unsloth/phi-4)  \n- [Phi-4-mini](https://huggingface.co/unsloth/Phi-4-mini-instruct)  \n[Llama 3.1](https://huggingface.co/collections/unsloth/llama-31-collection-6753dca76f47d9ce1696495f)  \n- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct)  \n- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B-Instruct)  \n- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B)  \n- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B)  \nMistral  \n- [Mistral Small 2501 (24B) - new](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501)  \n- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Instruct-2407)  \n- [Small 2409 (22B)](https://huggingface.co/unsloth/Mistral-Small-Instruct-2409)  \n- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.3)  \n- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.2)  \n- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-2409)  \n- [Mistral Small 2501 (24B) - new](https://huggingface.co/unsloth/Mistral-Small-24B-Base-2501)  \n- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Base-2407)  \n- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-v0.3)  \n- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-v0.2)  \n- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-Base-2409)  \nGemma 2  \n- [2B](https://huggingface.co/unsloth/gemma-2b-it)  \n- [9B](https://huggingface.co/unsloth/gemma-9b-it)  \n- [27B](https://huggingface.co/unsloth/gemma-27b-it)  \n- [2B](https://huggingface.co/unsloth/gemma-2-2b)  \n- [9B](https://huggingface.co/unsloth/gemma-2-9b)  \n- [27B](https://huggingface.co/unsloth/gemma-2-27b)  \nDeepSeek V3  \n- [bf16](https://huggingface.co/unsloth/DeepSeek-V3-bf16)  \n- [original 8-bit](https://huggingface.co/unsloth/DeepSeek-V3)  \nPhi-3.5  \n- [mini](https://huggingface.co/unsloth/Phi-3.5-mini-instruct)  \nPhi-3  \n- [mini](https://huggingface.co/unsloth/Phi-3-mini-4k-instruct)  \n- [medium](https://huggingface.co/unsloth/Phi-3-medium-4k-instruct)  \nLlama 3  \n- [8B](https://huggingface.co/unsloth/llama-3-8b-Instruct)  \n- [8B](https://huggingface.co/unsloth/llama-3-8b)  \n[Qwen 2.5](https://huggingface.co/collections/unsloth/qwen-25-66fe4c08fb9ada518e8a0d3f)  \n- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B-Instruct)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B-Instruct)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-3B-Instruct)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct)  \n- [14B](https://huggingface.co/unsloth/Qwen2.5-14B-Instruct)  \n- [32B](https://huggingface.co/unsloth/Qwen2.5-32B-Instruct)  \n- [72B](https://huggingface.co/unsloth/Qwen2.5-72B-Instruct)  \n- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B)  \n- [3B](https://huggingface.co/unsloth/Qwen2.5-3B)  \n- [7B](https://huggingface.co/unsloth/Qwen2.5-7B)  \n- [14B](https://huggingface.co/unsloth/Qwen2.5-14B)  \n- [32B](https://huggingface.co/unsloth/Qwen2.5-32B)  \n- [72B](https://huggingface.co/unsloth/Qwen2.5-72B)  \nLlava  \n- [1.5 (7B)](https://huggingface.co/unsloth/llava-1.5-7b-hf)  \n- [1.6 Mistral (7B)](https://huggingface.co/unsloth/llava-v1.6-mistral-7b-hf)  \nQwen2 VL  \n- [2B](https://huggingface.co/unsloth/Qwen2-VL-2B-Instruct)  \n- [7B](https://huggingface.co/unsloth/Qwen2-VL-7B-Instruct)  \n- [72B](https://huggingface.co/unsloth/Qwen2-VL-72B-Instruct)  \nLlama 2  \n- [7B](https://huggingface.co/unsloth/llama-2-7b-chat)  \n- [7B](https://huggingface.co/unsloth/llama-2-7b)  \n- [13B](https://huggingface.co/unsloth/llama-2-13b)  \nSmolLM2  \n- [135M](https://huggingface.co/unsloth/SmolLM2-135M-Instruct)  \n- [360M](https://huggingface.co/unsloth/SmolLM2-360M-Instruct)  \n- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-Instruct)  \n- [135M](https://huggingface.co/unsloth/SmolLM2-135M)  \n- [360M](https://huggingface.co/unsloth/SmolLM2-360M)  \n- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B)  \nTinyLlama  \n- [Instruct](https://huggingface.co/unsloth/tinyllama-chat)  \n- [Base](https://huggingface.co/unsloth/tinyllama)  \nQwen2  \n- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B-Instruct)  \n- [7B](https://huggingface.co/unsloth/Qwen2-7B-Instruct)  \n- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B)  \n- [7B](https://huggingface.co/unsloth/Qwen2-7B)  \nZephyr SFT  \n- [Instruct](https://huggingface.co/unsloth/zephyr-sft)  \n[PreviousUnsloth Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) [NextInstalling + Updating](https://docs.unsloth.ai/get-started/installing-+-updating)  \nLast updated 22 minutes ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "All Our Models | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/all-our-models"
      }
    },
    {
      "id": "b81908bf-8f8e-4162-9ca5-45716d7049c0",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "---\ntitle: FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\n---",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "6d2d8e13-fdb9-47ca-aaf4-5d03c9b4794a",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#understanding-fine-tuning) Understanding Fine-Tuning\n\nFine-tuning an LLM customizes its behavior, deepens its domain expertise, and optimizes its performance for specific tasks. By refining a pre-trained model (e.g. _Llama-3.1-8B_) with specialized data, you can:  \n- **Update Knowledge** ‚Äì Introduce new, domain-specific information that the base model didn‚Äôt originally include.  \n- **Customize Behavior** ‚Äì Adjust the model‚Äôs tone, personality, or response style to fit specific needs or a brand voice.  \n- **Optimize for Tasks** ‚Äì Improve accuracy and relevance on particular tasks or queries your use-case requires.  \nThink of fine-tuning as creating a specialized expert out of a generalist model. Some debate whether to use Retrieval-Augmented Generation (RAG) instead of fine-tuning, but fine-tuning can incorporate knowledge and behaviors directly into the model in ways RAG cannot. In practice, combining both approaches yields the best results - leading to greater accuracy, better usability, and fewer hallucinations.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "8c337d55-b7e8-41e9-b8f6-63fe78fe520d",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#understanding-fine-tuning) Understanding Fine-Tuning > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#real-world-applications-of-fine-tuning) Real-World Applications of Fine-Tuning\n\nFine-tuning can be applied across various domains and needs. Here are a few practical examples of how it makes a difference:  \n- **Sentiment Analysis for Finance** ‚Äì Train an LLM to determine if a news headline impacts a company positively or negatively, tailoring its understanding to financial context.  \n- **Customer Support Chatbots** ‚Äì Fine-tune on past customer interactions to provide more accurate and personalized responses in a company‚Äôs style and terminology.  \n- **Legal Document Assistance** ‚Äì Fine-tune on legal texts (contracts, case law, regulations) for tasks like contract analysis, case law research, or compliance support, ensuring the model uses precise legal language.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "23174609-95f6-4d2d-8577-2095ed4c28b9",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#the-benefits-of-fine-tuning) The Benefits of Fine-Tuning\n\nFine-tuning offers several notable benefits beyond what a base model or a purely retrieval-based system can provide:",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "8e80c8a6-6c85-4570-a3ac-94ee1c7a3c5b",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#the-benefits-of-fine-tuning) The Benefits of Fine-Tuning > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#fine-tuning-vs.-rag-whats-the-difference) Fine-Tuning vs. RAG: What‚Äôs the Difference?\n\nFine-tuning can do mostly everything RAG can - but not the other way around. During training, fine-tuning embeds external knowledge directly into the model. This allows the model to handle niche queries, summarize documents, and maintain context without relying on an outside retrieval system. That‚Äôs not to say RAG lacks advantages as it is excels at accessing up-to-date information from external databases. It is in fact possible to retrieve fresh data with fine-tuning as well, however it is better to combine RAG with fine-tuning for efficiency.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "803f4030-a9d0-4ed6-a107-ba3ba91d0a2a",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#the-benefits-of-fine-tuning) The Benefits of Fine-Tuning > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#task-specific-mastery) Task-Specific Mastery\n\nFine-tuning deeply integrates domain knowledge into the model. This makes it highly effective at handling structured, repetitive, or nuanced queries, scenarios where RAG-alone systems often struggle. In other words, a fine-tuned model becomes a specialist in the tasks or content it was trained on.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "05014d17-76e5-4df7-ab29-71131e70e134",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#the-benefits-of-fine-tuning) The Benefits of Fine-Tuning > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#independence-from-retrieval) Independence from Retrieval\n\nA fine-tuned model has no dependency on external data sources at inference time. It remains reliable even if a connected retrieval system fails or is incomplete, because all needed information is already within the model‚Äôs own parameters. This self-sufficiency means fewer points of failure in production.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "196e1442-be4b-4a5b-b39d-4c416429d2b4",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#the-benefits-of-fine-tuning) The Benefits of Fine-Tuning > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#faster-responses) Faster Responses\n\nFine-tuned models don‚Äôt need to call out to an external knowledge base during generation. Skipping the retrieval step means they can produce answers much more quickly. This speed makes fine-tuned models ideal for time-sensitive applications where every second counts.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "a9826ca3-9f0c-452b-bc64-13f213f26c6f",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#the-benefits-of-fine-tuning) The Benefits of Fine-Tuning > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#custom-behavior-and-tone) Custom Behavior and Tone\n\nFine-tuning allows precise control over how the model communicates. This ensures the model‚Äôs responses stay consistent with a brand‚Äôs voice, adhere to regulatory requirements, or match specific tone preferences. You get a model that not only knows _what_ to say, but _how_ to say it in the desired style.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "6fafa9b7-200c-432c-b817-e1b10dcec4c2",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#the-benefits-of-fine-tuning) The Benefits of Fine-Tuning > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#reliable-performance) Reliable Performance\n\nEven in a hybrid setup that uses both fine-tuning and RAG, the fine-tuned model provides a reliable fallback. If the retrieval component fails to find the right information or returns incorrect data, the model‚Äôs built-in knowledge can still generate a useful answer. This guarantees more consistent and robust performance for your system.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "0f703f44-fd4e-4849-9a6f-294ddb76683d",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#common-misconceptions) Common Misconceptions\n\nDespite fine-tuning‚Äôs advantages, a few myths persist. Let‚Äôs address two of the most common misconceptions about fine-tuning:",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "32e7d4fa-dbdb-4416-ab00-5c87d02042c0",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#common-misconceptions) Common Misconceptions > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#does-fine-tuning-add-new-knowledge-to-a-model) Does Fine-Tuning Add New Knowledge to a Model?\n\n**Yes - it absolutely can.** A common myth suggests that fine-tuning doesn‚Äôt introduce new knowledge, but in reality it does. If your fine-tuning dataset contains new domain-specific information, the model will learn that content during training and incorporate it into its responses. In effect, fine-tuning _can and does_ teach the model new facts and patterns from scratch.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "d929d980-0568-4610-815a-8b6ae43f2b15",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#common-misconceptions) Common Misconceptions > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#is-rag-always-better-than-fine-tuning) Is RAG Always Better Than Fine-Tuning?\n\n**Not necessarily.** Many assume RAG will consistently outperform a fine-tuned model, but that‚Äôs not the case when fine-tuning is done properly. In fact, a well-tuned model often matches or even surpasses RAG-based systems on specialized tasks. Claims that ‚ÄúRAG is always better‚Äù usually stem from fine-tuning attempts that weren‚Äôt optimally configured - for example, using incorrect [LoRA parameters](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia) or insufficient training.  \nUnsloth takes care of these complexities by automatically selecting the best parameter configurations for you. All you need is a good-quality dataset, and you'll get a fine-tuned model that performs to its fullest potential.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "2d0e7a94-c468-432c-a488-6279ed69685d",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#common-misconceptions) Common Misconceptions > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#is-fine-tuning-expensive) Is Fine-Tuning Expensive?\n\n**Not at all!** While full fine-tuning or pretraining can be costly, these are not necessary (pretraining is especially not necessary). In most cases, LoRA or QLoRA fine-tuning can be done for minimal cost. In fact, with Unsloth‚Äôs [free notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) for Colab or Kaggle, you can fine-tune models without spending a dime. Better yet, you can even fine-tune locally on your own device.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "dc559a36-6634-4324-a091-871e3e4d4dbd",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#faq) FAQ: > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#why-you-should-combine-rag-and-fine-tuning) Why You Should Combine RAG & Fine-Tuning\n\nInstead of choosing between RAG and fine-tuning, consider using **both** together for the best results. Combining a retrieval system with a fine-tuned model brings out the strengths of each approach. Here‚Äôs why:  \n- **Task-Specific Expertise** ‚Äì Fine-tuning excels at specialized tasks or formats (making the model an expert in a specific area), while RAG keeps the model up-to-date with the latest external knowledge.  \n- **Better Adaptability** ‚Äì A fine-tuned model can still give useful answers even if the retrieval component fails or returns incomplete information. Meanwhile, RAG ensures the system stays current without requiring you to retrain the model for every new piece of data.  \n- **Efficiency** ‚Äì Fine-tuning provides a strong foundational knowledge base within the model, and RAG handles dynamic or quickly-changing details without the need for exhaustive re-training from scratch. This balance yields an efficient workflow and reduces overall compute costs.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "9e711c11-9cbe-4090-ae64-257b4ee7959c",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#faq) FAQ: > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#lora-vs.-qlora-which-one-to-use) LoRA vs. QLoRA: Which One to Use?\n\nWhen it comes to implementing fine-tuning, two popular techniques can dramatically cut down the compute and memory requirements: **LoRA** and **QLoRA**. Here‚Äôs a quick comparison of each:  \n- **LoRA (Low-Rank Adaptation)** ‚Äì Fine-tunes only a small set of additional ‚Äúadapter‚Äù weight matrices (in 16-bit precision), while leaving most of the original model unchanged. This significantly reduces the number of parameters that need updating during training.  \n- **QLoRA (Quantized LoRA)** ‚Äì Combines LoRA with 4-bit quantization of the model weights, enabling efficient fine-tuning of very large models on minimal hardware. By using 4-bit precision where possible, it dramatically lowers memory usage and compute overhead.  \nWe recommend starting with **QLoRA**, as it‚Äôs one of the most efficient and accessible methods available. Thanks to Unsloth‚Äôs [dynamic 4-bit](https://unsloth.ai/blog/dynamic-4bit) quants, the accuracy loss compared to standard 16-bit LoRA fine-tuning is now negligible.",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "ae64d2b2-2e61-47ba-83bc-6a316fc11df6",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-faq-+-is-fine-tuning-right-for-me.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#faq) FAQ: > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me#experimentation-is-key) Experimentation is Key\n\nThere‚Äôs no single ‚Äúbest‚Äù approach to fine-tuning - only best practices for different scenarios. It‚Äôs important to experiment with different methods and configurations to find what works best for your dataset and use case. A great starting point is **QLoRA (4-bit)**, which offers a very cost-effective, resource-friendly way to fine-tune models without heavy computational requirements.  \n[üß†LoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)  \n[PreviousUnsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements) [NextWhat Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use)  \nLast updated 2 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "FAQ + Is Fine-tuning Right For Me? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me"
      }
    },
    {
      "id": "4a6f7685-47e6-446e-ae4e-263e8ff5b3ee",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-lora-parameters-encyclopedia.md",
      "content": "---\ntitle: LoRA Parameters Encyclopedia | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia\n---",
      "metadata": {
        "title": "LoRA Parameters Encyclopedia | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"
      }
    },
    {
      "id": "0ae3ff5c-15fc-4626-888e-dbf278ac7a9c",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-lora-parameters-encyclopedia.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia#key-fine-tuning-parameters) Key Fine-tuning Parameters > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia#learning-rate) **Learning Rate**\n\nDefines how much the model‚Äôs weights adjust per training step.  \n- **Higher Learning Rates**: Faster training, risk of overfitting.  \n- **Lower Learning Rates**: More stable training, may require more epochs.  \n- **Typical Range**: 1e-4 (0.0001) to 5e-5 (0.00005).",
      "metadata": {
        "title": "LoRA Parameters Encyclopedia | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"
      }
    },
    {
      "id": "25b47eb1-cd85-4ceb-9b56-1f70c6168916",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-lora-parameters-encyclopedia.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia#key-fine-tuning-parameters) Key Fine-tuning Parameters > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia#epochs) **Epochs**\n\nNumber of times the model sees the full training dataset.  \n- **Recommended:** 1-3 epochs (anything more than 3 is generally not optimal unless you want your model to have much less hallucinations but also less creativity)  \n- **More Epochs**: Better learning, higher risk of overfitting.  \n- **Fewer Epochs**: May undertrain the model.",
      "metadata": {
        "title": "LoRA Parameters Encyclopedia | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"
      }
    },
    {
      "id": "0fee7f9f-cf53-45bd-a60f-2d4c025e3858",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-lora-parameters-encyclopedia.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia#advanced-parameters) **Advanced Parameters:**\n\nParameter  \nFunction  \nRecommended Settings  \n**LoRA Rank**  \nControls the number of low-rank factors used for adaptation.  \n16-32  \n**LoRA Alpha**  \nScaling factor for weight updates.  \nRatio of 1-2 with LoRA Rank  \n**LoRA Dropout**  \nDropout rate to prevent overfitting.  \n0.1-0.2  \n**Max Sequence Length**  \nMaximum number of tokens processed in one input.  \nAdjust based on dataset needs  \n**Warmup Steps**  \nGradually increases learning rate at the start of training.  \n5-10% of total steps  \n**Scheduler Type**  \nAdjusts learning rate dynamically during training.  \nLinear Decay, Cosine Annealing  \n**Seed**  \nEnsures reproducibility of results.  \nFixed number (e.g., 42)  \n**Batch Size**  \nNumber of samples processed per training step.  \nHigher values require more VRAM  \n**Quantization**  \nReduces precision of model weights for efficiency.  \nQ4_K_M for balance between performance & speed  \n**Weight Decay**  \nPenalizes large weight updates to prevent overfitting.  \nStart at 0.01, adjust as needed",
      "metadata": {
        "title": "LoRA Parameters Encyclopedia | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"
      }
    },
    {
      "id": "88176683-fa4f-402a-8cc7-da266dcd0eb7",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-lora-parameters-encyclopedia.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia#lora-configuration-parameters) **LoRA Configuration Parameters**\n\nFor more information, we would recommend you checking out our [tutorial](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-5.-parameters-for-finetuning). Written by [Sebastien](https://github.com/sebdg).  \nTuning these parameters helps balance model performance and efficiency:  \n- **r** (Rank of decomposition): Controls the finetuning process.  \n- Suggested: 8, 16, 32, 64, or 128.  \n- **Higher**: Better accuracy on hard tasks but increases memory and risk of overfitting.  \n- **Lower**: Faster, memory-efficient but may reduce accuracy.  \n- **lora_alpha** (Scaling factor): Determines the learning strength.  \n- Suggested: Equal to or double the rank ( `r`).  \n- **Higher**: Learns more but may overfit.  \n- **Lower**: Slower to learn, more generalizable.  \n- **lora_dropout** (Default: 0): Dropout probability for regularization.  \n- **Higher**: More regularization, slower training.  \n- **Lower (0)**: Faster training, minimal impact on overfitting.  \n- **target_modules**: Modules to fine-tune (default includes `\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"`).  \n- Fine-tuning all modules is recommended for best results.  \n- **bias** (Default: `\"none\"`): Controls bias term updates.  \n- **Set to none** for optimized, faster training.  \n- **use_gradient_checkpointing**: Reduces memory usage for long contexts.  \n- Use `\"unsloth\"` to reduce memory by an extra 30% by using our [gradient checkpointing](https://unsloth.ai/blog/gradient) algorithm which you can read about [here](https://unsloth.ai/blog/gradient).  \n- **random_state**: A seed for reproducible experiments.  \n- Suggested: Set to a fixed value like 3407.  \n- **use_rslora**: Enables Rank-Stabilized LoRA.  \n- **True**: Automatically adjusts `lora_alpha`.  \n- **loftq_config**: Applies quantization and advanced LoRA initialization.  \n- **None**: Default (no quantization).  \n- **Set**: Initializes LoRA using top singular vectors‚Äîimproves accuracy but increases memory usage.",
      "metadata": {
        "title": "LoRA Parameters Encyclopedia | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"
      }
    },
    {
      "id": "30747155-be0f-4133-b896-1c7ea112846a",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-lora-parameters-encyclopedia.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia#target-modules-explained) **Target Modules Explained**\n\nThese components transform inputs for attention mechanisms:  \n- **q_proj, k_proj, v_proj**: Handle queries, keys, and values.  \n- **o_proj**: Integrates attention results into the model.  \n- **gate_proj**: Manages flow in gated layers.  \n- **up_proj, down_proj**: Adjust dimensionality for efficiency.  \n[PreviousWhat Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use) [NextUnsloth Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)  \nLast updated 25 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "LoRA Parameters Encyclopedia | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"
      }
    },
    {
      "id": "101780ca-f621-4d4f-a7dc-c81b073c436b",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-unsloth-requirements.md",
      "content": "---\ntitle: Unsloth Requirements | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements\n---",
      "metadata": {
        "title": "Unsloth Requirements | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements"
      }
    },
    {
      "id": "35c285c5-1c43-4b22-b026-36e37603ce29",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-unsloth-requirements.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements#system-requirements) System Requirements\n\n- **Operating System**: Works on Linux and Windows.  \n- Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.  \n- If you have different versions of torch, transformers etc., `pip install unsloth ` will automatically install all the latest versions of those libraries so you don't need to worry about version compatibility.  \n- Your device must have `xformers`, `torch`, `BitsandBytes` and `triton` support.  \n- Unsloth only works if you have a NVIDIA GPU. Make sure you also have disk space to train & save your model",
      "metadata": {
        "title": "Unsloth Requirements | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements"
      }
    },
    {
      "id": "22cb212b-469d-4e76-b031-44396a76976c",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-unsloth-requirements.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements#fine-tuning-vram-requirements) Fine-tuning VRAM requirements:\n\nHow much GPU memory do I need for LLM fine-tuning using Unsloth?  \nA common issue when you OOM or run out of memory is because you set your batch size too high. Set it to 1, 2, or 3 to use less VRAM.  \n**For context length benchmarks, see** [**here**](https://docs.unsloth.ai/basics/unsloth-benchmarks#context-length-benchmarks) **.**  \nCheck this table for VRAM requirements sorted by model parameters and fine-tuning method. QLoRA uses 4-bit, LoRA uses 16-bit. Keep in mind that sometimes it may require more VRAM so these numbers are the absolute minimum:  \nModel parameters  \nQLoRA (4-bit) VRAM  \nLoRA (16-bit) VRAM  \n3B  \n3.5 GB  \n8 GB  \n7B  \n5 GB  \n19 GB  \n8B  \n6 GB  \n22 GB  \n9B  \n6.5 GB  \n24 GB  \n11B  \n7.5 GB  \n29 GB  \n14B  \n8.5 GB  \n33 GB  \n27B  \n22GB  \n64GB  \n32B  \n26 GB  \n76 GB  \n40B  \n30GB  \n96GB  \n70B  \n41 GB  \n164 GB  \n81B  \n48GB  \n192GB  \n90B  \n53GB  \n212GB  \n405B  \n237 GB  \n950 GB  \n[PreviousBeginner? Start here!](https://docs.unsloth.ai/get-started/beginner-start-here) [NextFAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me)  \nLast updated 2 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Unsloth Requirements | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements"
      }
    },
    {
      "id": "61b1164b-fed7-4466-b068-9d420f5bc797",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "---\ntitle: What Model Should I Use? | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\n---",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "0bc6b99c-52c1-4b58-beca-766a46429e61",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#llama-qwen-mistral-phi-or) Llama, Qwen, Mistral, Phi or?\n\nWhen preparing for fine-tuning, one of the first decisions you'll face is selecting the right model. Here's a step-by-step guide to help you choose:  \n1",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "4146acd8-ff6e-4a20-8db2-7f11d06d2c3e",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#llama-qwen-mistral-phi-or) Llama, Qwen, Mistral, Phi or? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#choose-a-model-that-aligns-with-your-usecase) Choose a model that aligns with your usecase\n\n- E.g. For image-based training, select a vision model such as _Llama 3.2 Vision_. For code datasets, opt for a specialized model like _Qwen Coder 2.5_.  \n- **Licensing and Requirements**: Different models may have specific licensing terms and [system requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements#system-requirements). Be sure to review these carefully to avoid compatibility issues.  \n2",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "760dd145-b552-4fe8-894f-09e9f6a2f0a5",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#llama-qwen-mistral-phi-or) Llama, Qwen, Mistral, Phi or? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#assess-your-storage-compute-capacity-and-dataset) **Assess your storage, compute capacity and dataset**\n\n- Use our [VRAM guideline](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements#approximate-vram-requirements-based-on-model-parameters) to determine the VRAM requirements for the model you‚Äôre considering.  \n- Your dataset will reflect the type of model you will use and amount of time it will take to train  \n3",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "a6ceddc1-8bc1-4c35-8ac3-737a238cddbf",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#llama-qwen-mistral-phi-or) Llama, Qwen, Mistral, Phi or? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#select-a-model-and-parameters) **Select a Model and Parameters**\n\n- We recommend using the latest model for the best performance and capabilities. For instance, as of January 2025, the leading 70B model is _Llama 3.3_.  \n- You can stay up to date by exploring our catalog of [model uploads](https://docs.unsloth.ai/get-started/all-our-models) to find the most recent and relevant options.  \n4",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "0df6b48e-b884-4984-a09b-2bf5f885fa16",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#llama-qwen-mistral-phi-or) Llama, Qwen, Mistral, Phi or? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#choose-between-base-and-instruct-models) **Choose Between Base and Instruct Models**\n\nFurther details below:",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "5f8200ac-f5ca-4d9f-ba89-27551a5bffb2",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#instruct-or-base-model) Instruct or Base Model?\n\nWhen preparing for fine-tuning, one of the first decisions you'll face is whether to use an instruct model or a base model.",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "638abd29-c1fb-4a71-be8e-6f6fa2cd9934",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#instruct-or-base-model) Instruct or Base Model? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#instruct-models) Instruct Models\n\nInstruct models are pre-trained with built-in instructions, making them ready to use without any fine-tuning. These models, including GGUFs and others commonly available, are optimized for direct usage and respond effectively to prompts right out of the box.",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "284e4454-63ba-44d2-978f-64a6bbeab74c",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#instruct-or-base-model) Instruct or Base Model? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#base-models) **Base Models**\n\nBase models, on the other hand, are the original pre-trained versions without instruction fine-tuning. These are specifically designed for customization through fine-tuning, allowing you to adapt them to your unique needs.",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "d15ea2b7-3138-4dc4-b8f6-f19064118d46",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#instruct-or-base-model) Instruct or Base Model? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#should-i-choose-instruct-or-base) Should I Choose Instruct or Base?\n\nThe decision often depends on the quantity, quality, and type of your data:  \n- **1,000+ Rows of Data**: If you have a large dataset with over 1,000 rows, it's generally best to fine-tune the base model.  \n- **300‚Äì1,000 Rows of High-Quality Data**: With a medium-sized, high-quality dataset, fine-tuning the base or instruct model are both viable options.  \n- **Less than 300 Rows**: For smaller datasets, the instruct model is typically the better choice. Fine-tuning the instruct model enables it to align with specific needs while preserving its built-in instructional capabilities. This ensures it can follow general instructions without additional input unless you intend to significantly alter its functionality.  \n- For information how how big your dataset should be, [see here](https://docs.unsloth.ai/basics/datasets-101#how-big-should-my-dataset-be)",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "ec640291-15f4-4ceb-90ec-93db9901c6c5",
      "source": "firecrawl\\docs\\get-started-beginner-start-here-what-model-should-i-use.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#instruct-or-base-model) Instruct or Base Model? > [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use#experimentation-is-key) Experimentation is Key\n\nWe recommend experimenting with both models when possible. Fine-tune each one and evaluate the outputs to see which aligns better with your goals.  \n[PreviousFAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me) [NextLoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)  \nLast updated 2 months ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "What Model Should I Use? | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use"
      }
    },
    {
      "id": "dae065f8-02f3-4491-a3b2-aa780c21dd6e",
      "source": "firecrawl\\docs\\get-started-beginner-start-here.md",
      "content": "---\ntitle: Beginner? Start here! | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/beginner-start-here\n---  \nIf you're a beginner, here might be the first questions you'll ask before your first fine-tune. You can also always ask our community by joining our [Discord server](https://discord.gg/unsloth).  \n[üß¨](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama) [Fine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide)  \nStep-by-step on how to fine-tune!  \nLearn the core basics of training.  \n[‚ùì](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use) [What Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use)  \nInstruct or Base Model?  \nHow big should my dataset be?  \n[üì•](https://docs.unsloth.ai/get-started/installing-+-updating) [Installing + Updating](https://docs.unsloth.ai/get-started/installing-+-updating)  \nHow do I install Unsloth locally?  \nHow to update Unsloth?  \n[ü§î](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me) [FAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me)  \nWhat can fine-tuning do for me?  \nRAG vs. Fine-tuning?  \nüìà [Datasets 101](https://docs.unsloth.ai/basics/datasets-101)  \nHow do I structure/prepare my dataset?  \nHow do I collect data?  \n[üí¨](https://docs.unsloth.ai/basics/chat-templates) [Chat Templates](https://docs.unsloth.ai/basics/chat-templates)  \nHow do I structure my Chat Template?  \nWhich Chat Template should I use for my model?  \n[üõ†Ô∏è](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements) [Unsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)  \nDoes Unsloth work on my GPU?  \nHow much VRAM will I need?  \n[üñ•Ô∏è](https://docs.unsloth.ai/basics/running-and-saving-models) [Running & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models)  \nHow do I save my model locally?  \nHow do I run my model via Ollama or vLLM?  \nüß† [LoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)  \nWhat happens when I change a parameter?  \nWhat parameters should I change?  \n!  \n[PreviousWelcome](https://docs.unsloth.ai/) [NextUnsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)  \nLast updated 25 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Beginner? Start here! | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/beginner-start-here"
      }
    },
    {
      "id": "2c567956-8113-4772-ac33-1da82735d9ca",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "---\ntitle: Fine-tuning Guide | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/fine-tuning-guide\n---",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "f5da7d80-1e22-4a1a-a386-f2c6090bb126",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-1.-understand-fine-tuning) 1. Understand Fine-tuning\n\nFine-tuning an LLM customizes its behavior, enhances + injects knowledge, and optimizes performance for domains/specific tasks. For example:  \n- **GPT-4** serves as a base model; however, OpenAI fine-tuned it to better comprehend instructions and prompts, leading to the creation of ChatGPT-4 which everyone uses today.  \n-  **DeepSeek-R1-Distill-Llama-8B** is a fine-tuned version of Llama-3.1-8B. DeepSeek utilized data generated by DeepSeek-R1, to fine-tune Llama-3.1-8B. This process, known as **distillation**, injects the data into the Llama model to learn reasoning capabilities.  \nWith [Unsloth](https://github.com/unslothai/unsloth), you can fine-tune for free on Colab, Kaggle, or locally with just 3GB VRAM by using our [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks). By fine-tuning a pre-trained model (e.g. Llama-3.1-8B) on a specialized dataset, you can:  \n- **Update Knowledge**: Introduce new domain-specific information.  \n- **Customize Behavior**: Adjust the model‚Äôs tone, personality, or response style.  \n- **Optimize for Tasks**: Improve accuracy and relevance for specific use cases.  \n**Example usecases**:  \n- Train LLM to predict if a headline impacts a company positively or negatively.  \n- Use historical customer interactions for more accurate and custom responses.  \n- Fine-tune LLM on legal texts for contract analysis, case law research, and compliance.  \nYou can think of a fine-tuned model as a specialized agent designed to do specific tasks more effectively and efficiently. **Fine-tuning can replicate all of RAG's capabilities**, but not vice versa. Read more FAQ here:  \n[ü§îFAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me)",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "1e3a2bd7-3e4f-4869-ab25-7f2748fb2ec2",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-2.-choose-the-right-model--method) 2. Choose the Right Model + Method\n\nIf you're a beginner, it is best to start with a small instruct model like Llama 3.1 (8B) and experiment from there. You'll also need to decide between QLoRA and LoRA training:  \n- **LoRA:** Fine-tunes small, trainable matrices in 16-bit without updating all model weights.  \n- **QLoRA:** Combines LoRA with 4-bit quantization to handle very large models with minimal resources.  \nYou can change the model name to whichever model you like by matching it with model's name on Hugging Face e.g. 'unsloth/llama-3.1-8b-bnb-4bit'.  \nThere are 3 other settings which you can toggle:  \n- `max_seq_length = 2048` ‚Äì Controls context length. While Llama-3 supports 8192, we recommend 2048 for testing. Unsloth enables 4√ó longer context fine-tuning.  \n- `dtype = None` ‚Äì Defaults to None; use `torch.float16` or `torch.bfloat16` for newer GPUs.  \n- `load_in_4bit = True` ‚Äì Enables 4-bit quantization, reducing memory use 4√ó for fine-tuning on 16GB GPUs. Disabling it on larger GPUs (e.g., H100) slightly improves accuracy (1‚Äì2%).  \nWe recommend starting with QLoRA, as it is one of the most accessible and effective methods for training models. Our [dynamic 4-bit](https://unsloth.ai/blog/phi4) quants, the accuracy loss for QLoRA compared to LoRA is now largely recovered.  \nYou can also do [reasoning (GRPO)](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl), [vision](https://docs.unsloth.ai/basics/vision-fine-tuning), [reward modelling](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto) (DPO, ORPO, KTO), [continued pretraining](https://docs.unsloth.ai/basics/continued-pretraining), text completion and other training methodologies with Unsloth.  \nRead our detailed guide on choosing the right model:  \n[‚ùìWhat Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use)",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "b7f65540-8b3c-49b7-a9b6-268e59df19c2",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-3.-your-dataset) 3. Your Dataset\n\nFor LLMs, datasets are collections of data that can be used to train our models. In order to be useful for training, text data needs to be in a format that can be tokenized.  \n- You will need to create a dataset usually with 2 columns - question and answer. The quality and amount will largely reflect the end result of your fine-tune so it's imperative to get this part right.  \n- You can synthetically generate data and structure your dataset (into QA pairs) using ChatGPT or local LLMs.  \n- Fine-tuning can permanently incorporate an existing repository of documents and continuously expand its knowledge base, but just dumping data alone won‚Äôt work as well. For optimal results, curate a well-structured dataset, ideally as question-answer pairs. This enhances learning, understanding, and response accuracy.  \n- But, that's not always the case, e.g. if you just dump all your code data for fine-tuning, your model can yield significant performance improvements, even without structured formatting. So it really depends on your use case.  \n_**Read more about creating your dataset:**_  \n[üìàDatasets 101](https://docs.unsloth.ai/basics/datasets-101)  \nFor most of our notebook examples, we utilize the [Alpaca dataset](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-6.-alpaca-dataset) however other notebooks like Vision will use different datasets which may need images in the answer ouput as well.",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "ebd82ae3-f943-4a73-94ed-f17bf2a149e6",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-4.-understand-model-parameters) 4. Understand Model Parameters\n\nThere are millions of hyperparameters combinations and choosing the right numbers are crucial to a good result. You can edit the parameters (numbers) below, but you can ignore it, since we already select quite reasonable numbers.  \n!  \nThe goal is to change these numbers to increase accuracy, but also **counteract over-fitting**. Over-fitting is when you make the language model memorize a dataset, and not be able to answer novel new questions. We want to a final model to answer unseen questions, and not do memorization. Here are the key parameters:",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "c21286e7-99c3-4ebc-acfe-030966997ab3",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-4.-understand-model-parameters) 4. Understand Model Parameters > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#learning-rate) **Learning Rate**\n\nDefines how much the model‚Äôs weights adjust per training step.  \n- **Higher Learning Rates**: Faster training, risk of overfitting.  \n- **Lower Learning Rates**: More stable training, may require more epochs.  \n- **Typical Range**: 1e-4 (0.0001) to 5e-5 (0.00005).",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "ffce3761-06c0-4d4c-933d-310464e3eea0",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-4.-understand-model-parameters) 4. Understand Model Parameters > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#epochs) **Epochs**\n\nNumber of times the model sees the full training dataset.  \n- **Recommended:** 1-3 epochs (anything more than 3 is generally not optimal unless you want your model to have much less hallucinations but also less creativity)  \n- **More Epochs**: Better learning, higher risk of overfitting.  \n- **Fewer Epochs**: May undertrain the model.  \n_**For a complete guide on how hyperparameters affect training, see:**_  \n[üß†LoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "8e27864b-cb5f-465a-b24e-3071cd96cb9f",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-4.-understand-model-parameters) 4. Understand Model Parameters > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#avoiding-overfitting-and-underfitting) **Avoiding Overfitting & Underfitting** > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#overfitting-too-specialized) **Overfitting** (Too Specialized)\n\nThe model memorizes training data, failing to generalize to unseen inputs. Solution:  \n- Reduce learning rate.  \n- Lower the number of training epochs.  \n- Combine your dataset with a generic dataset e.g. ShareGPT  \n- Increase dropout rate to introduce regularization.",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "18803285-150d-488e-8d48-d46f70eb2983",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-4.-understand-model-parameters) 4. Understand Model Parameters > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#avoiding-overfitting-and-underfitting) **Avoiding Overfitting & Underfitting** > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#underfitting-too-generic) **Underfitting** (Too Generic)\n\nThough quite rare, sometimes your model may fail to learn from training data, providing responses similar to the base model. Solution:  \n- Increase learning rate.  \n- Train for more epochs.  \n- Use a more domain-relevant dataset.  \nFine-tuning has no single \"best\" approach, only best practices. Experimentation is key to finding what works for your needs. Our notebooks auto-set optimal parameters based on evidence from research papers and past experiments.",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "9ede4a8a-d010-431f-8af5-83a9481bd563",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-5.-installing--requirements) 5. Installing + Requirements\n\nWe would recommend beginners to utilise our pre-made [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) first as it's the easiest way to get started with guided steps. However, if installing locally is a must, you can install and use Unsloth - just make sure you have all the right requirements necessary. Also depending on the model and quantization you're using, you'll need enough VRAM and resources. See all the details here:  \n[üõ†Ô∏èUnsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)  \nNext, you'll need to install Unsloth. Unsloth currently only supports Windows and Linux devices. Once you install Unsloth, you can copy and paste our notebooks and use them in your own local environment. We have many installation methods:  \n[üì•Installing + Updating](https://docs.unsloth.ai/get-started/installing-+-updating)",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "10bed074-db10-45ff-b5d4-8c2985d3bbee",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-6.-training--evaluation) 6. Training + Evaluation\n\nOnce you have everything set, it's time to train! If something's not working, remember you can always change hyperparameters, your dataset etc.  \nYou will see a log of some numbers whilst training! This is the training loss, and your job is to set parameters to make this go to as close to 0.5 as possible! If your finetune is not reaching 1, 0.8 or 0.5, you might have to adjust some numbers. If your loss goes to 0, that's probably not a good sign as well!  \n!  \nThe training loss will appear as numbers  \nWe generally recommend keeping the default settings unless you need longer training or larger batch sizes.  \n- `per_device_train_batch_size = 2` ‚Äì Increase for better GPU utilization but beware of slower training due to padding. Instead, increase `gradient_accumulation_steps` for smoother training.  \n- `gradient_accumulation_steps = 4` ‚Äì Simulates a larger batch size without increasing memory usage.  \n- `max_steps = 60` ‚Äì Speeds up training. For full runs, replace with `num_train_epochs = 1` (1‚Äì3 epochs recommended to avoid overfitting).  \n- `learning_rate = 2e-4` ‚Äì Lower for slower but more precise fine-tuning. Try values like `1e-4`, `5e-5`, or `2e-5`.",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "735b10d2-06f4-43f9-8efb-3bebad3dd98a",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-6.-training--evaluation) 6. Training + Evaluation > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#evaluation) Evaluation\n\nIn order to evaluate, you could do manually evaluation by just chatting with the model and see if it's to your liking. You can also enable evaluation for Unsloth, but keep in mind it can be time-consuming depending on the dataset size. To speed up evaluation you can: reduce the evaluation dataset size or set `evaluation_steps = 100`.  \nFor testing, you can also take 20% of your training data and use that for testing. If you already used all of the training data, then you have to manually evaluate it. You can also use automatic eval tools like EleutherAI‚Äôs [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness). Keep in mind that automated tools may not perfectly align with your evaluation criteria.",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "45b4abcc-3483-4c3e-828a-dfa2c6162d07",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-7.-running--saving-the-model) 7. Running + Saving the model\n\n!  \nNow let's run the model after we completed the training process! You can edit the yellow underlined part! In fact, because we created a multi turn chatbot, we can now also call the model as if it saw some conversations in the past like below:  \n!  \nReminder Unsloth itself provides **2x faster inference** natively as well, so always do not forget to call `FastLanguageModel.for_inference(model)`. If you want the model to output longer responses, set `max_new_tokens = 128` to some larger number like 256 or 1024. Notice you will have to wait longer for the result as well!",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "c354808e-1c95-4d9f-b96f-0063d3d34493",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-7.-running--saving-the-model) 7. Running + Saving the model > [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#saving-the-model) Saving the model\n\nFor saving and using your model in desired inference engines like Ollama, vLLM, Open WebUI, we can have more information here:  \n[üñ•Ô∏èRunning & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models)  \nWe can now save the finetuned model as a small 100MB file called a LoRA adapter like below. You can instead push to the Hugging Face hub as well if you want to upload your model! Remember to get a Hugging Face token via: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and add your token!  \n!  \nAfter saving the model, we can again use Unsloth to run the model itself! Use `FastLanguageModel` again to call it for inference!  \n!",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "e4ad5b14-1162-4b06-8b5a-32249ac27f74",
      "source": "firecrawl\\docs\\get-started-fine-tuning-guide.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-8.-were-done) 8. We're done!\n\nYou've successfully finetuned a language model and exported it to your desired inference engine with Unsloth!  \nTo learn more about finetuning tips and tricks, head over to our blogs which provide tremendous and educational value: [https://unsloth.ai/blog/](https://unsloth.ai/blog/)  \nIf you need any help on finetuning, you can also join our Discord server [here](https://discord.gg/unsloth). Thanks for reading and hopefully this was helpful!  \n!  \n[PreviousGoogle Colab](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab) [NextReasoning - GRPO & RL](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)  \nLast updated 1 day ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Fine-tuning Guide | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/fine-tuning-guide"
      }
    },
    {
      "id": "a26d6c35-657f-407f-bbe6-96c6edebb238",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-conda-install.md",
      "content": "---\ntitle: Conda Install | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/installing-+-updating/conda-install\n---  \nOnly use Conda if you have it. If not, use [Pip](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install).  \nSelect either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nconda create --name unsloth_env \\\npython=3.11 \\\npytorch-cuda=12.1 \\\npytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\n-y\nconda activate unsloth_env\n\npip install unsloth\n```  \nIf you're looking to install Conda in a Linux environment, [read here](https://docs.anaconda.com/miniconda/), or run the below:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```  \n[PreviousWindows Installation](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation) [NextGoogle Colab](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab)  \nLast updated 10 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Conda Install | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/conda-install"
      }
    },
    {
      "id": "30f88fe3-49c9-43b4-aad1-1e70150b9612",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-google-colab.md",
      "content": "---\ntitle: Google Colab | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/installing-+-updating/google-colab\n---  \n!  \nIf you have never used a Colab notebook, a quick primer on the notebook itself:  \n1. **Play Button at each \"cell\".** Click on this to run that cell's code. You must not skip any cells and you must run every cell in chronological order. If you encounter errors, simply rerun the cell you did not run. Another option is to click CTRL + ENTER if you don't want to click the play button.  \n2. **Runtime Button in the top toolbar.** You can also use this button and hit \"Run all\" to run the entire notebook in 1 go. This will skip all the customization steps, but is a good first try.  \n3. **Connect / Reconnect T4 button.** T4 is the free GPU Google is providing. It's quite powerful!  \nThe first installation cell looks like below: Remember to click the PLAY button in the brackets [ ]. We grab our open source Github package, and install some other packages.  \n!",
      "metadata": {
        "title": "Google Colab | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/google-colab"
      }
    },
    {
      "id": "f8df334a-d2db-4cdb-8d0a-cd1d81440923",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-google-colab.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab#undefined)\n\n[PreviousConda Install](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install) [NextFine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide)  \nLast updated 8 months ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Google Colab | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/google-colab"
      }
    },
    {
      "id": "c1fa6e70-1a52-47db-a226-14bcec501a17",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-pip-install.md",
      "content": "---\ntitle: Pip Install | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/installing-+-updating/pip-install\n---",
      "metadata": {
        "title": "Pip Install | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/pip-install"
      }
    },
    {
      "id": "aa083b7e-4984-4ba0-a6ae-ebf9c1842988",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-pip-install.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install#recommended-installation) **Recommended installation:**\n\n**Install with pip (recommended)** for Linux devices:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install unsloth\n```  \n* * *",
      "metadata": {
        "title": "Pip Install | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/pip-install"
      }
    },
    {
      "id": "799f3362-78f2-4a25-8dc4-93adfbaaf12f",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-pip-install.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install#advanced-pip-installation) Advanced Pip Installation\n\nDo **NOT** use this if you have [Conda](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install).  \nPip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.  \nFor other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.  \nFor example, if you have `torch 2.4` and `CUDA 12.1`, use:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install --upgrade pip\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n```  \nAnother example, if you have `torch 2.5` and `CUDA 12.4`, use:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install --upgrade pip\npip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```  \nAnd other examples:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```  \nOr, run the below in a terminal to get the **optimal** pip installation command:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nwget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n```  \nOr, run the below manually in a Python REPL:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ntry: import torch\nexcept: raise ImportError('Install torch via `pip install torch`')\nfrom packaging.version import Version as V\nv = V(torch.__version__)\ncuda = str(torch.version.cuda)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nif cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\": raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'): x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'): x = 'cu{}{}-torch212'\nelif v < V('2.3.0'): x = 'cu{}{}-torch220'\nelif v < V('2.4.0'): x = 'cu{}{}-torch230'\nelif v < V('2.5.0'): x = 'cu{}{}-torch240'\nelif v < V('2.6.0'): x = 'cu{}{}-torch250'\nelse: raise RuntimeError(f\"Torch = {v} too new!\")\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')\n```  \n[PreviousUpdating](https://docs.unsloth.ai/get-started/installing-+-updating/updating) [NextWindows Installation](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation)  \nLast updated 10 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Pip Install | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/pip-install"
      }
    },
    {
      "id": "c870a4cb-752d-42b0-bc1a-e183d930f822",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-updating.md",
      "content": "---\ntitle: Updating | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/installing-+-updating/updating\n---",
      "metadata": {
        "title": "Updating | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/updating"
      }
    },
    {
      "id": "47ba6a48-6c92-44ba-be11-1e9c60c70878",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-updating.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/updating#standard-updating) Standard Updating:\n\nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n```",
      "metadata": {
        "title": "Updating | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/updating"
      }
    },
    {
      "id": "7c71dbd0-5fb1-4b59-b253-2a8ca1d99c6c",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-updating.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/updating#standard-updating) Standard Updating: > [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/updating#updating-without-dependency-updates) Updating without dependency updates:\n\nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install --upgrade --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\npip install --upgrade --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth-zoo.git\n```",
      "metadata": {
        "title": "Updating | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/updating"
      }
    },
    {
      "id": "b1a4c505-ddf1-461b-ba73-cf6ce96689f2",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-updating.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/updating#to-use-an-old-version-of-unsloth) To use an old version of Unsloth:\n\nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install --force-reinstall --no-cache-dir --no-deps unsloth==2025.1.5\n```  \n'2025.1.5' is one of the previous old versions of Unsloth. Change it to a specific release listed on our [Github here](https://github.com/unslothai/unsloth/releases).  \n[PreviousInstalling + Updating](https://docs.unsloth.ai/get-started/installing-+-updating) [NextPip Install](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install)  \nLast updated 11 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Updating | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/updating"
      }
    },
    {
      "id": "345d383e-ca10-449f-a5eb-8a5cd44b4ec6",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "---\ntitle: Windows Installation | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\n---",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "979bdbeb-03ac-4077-93a2-073d51b76646",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#method-1-windows-directly) Method #1 - Windows directly:\n\nPython 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10.  \nNeed help or experiencing an error? Ask on our [GitHub Discussions](https://github.com/unslothai/unsloth/discussions/1849) thread for Windows support!  \n1  \n**Install NVIDIA Video Driver**  \nYou should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx)  \n2  \n**Install Visual Studio C++**  \nYou will need Visual Studio, with C++ installed. By default, C++ is not installed with Visual Studio, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK.  \n- Launch the Installer here: [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/community/)  \n- In the installer, navigate to individual components and select all the options listed here:  \n- **.NET Framework 4.8 SDK**  \n- **.NET Framework 4.7.2 targeting pack**  \n- **C# and Visual Basic Roslyn compilers**  \n- **MSBuild**  \n- **MSVC v143 - VS 2022 C++ x64/x86 build tools**  \n- **C++ 2022 Redistributable Update**  \n- **C++ CMake tools for Windows**  \n- **C++/CLI support for v143 build tools (Latest)**  \n- **MSBuild support for LLVM (clang-cl) toolset**  \n- **C++ Clang Compiler for Windows (19.1.1)**  \n- **Windows 11 SDK (10.0.22621.0)**  \n- **Windows Universal CRT SDK**  \n- **C++ 2022 Redistributable MSMs**  \n**Easier method:** Or you can open an elevated Command Prompt or PowerShell:  \n- Search for \"cmd\" or \"PowerShell\", right-click it, and choose \"Run as administrator.\"  \n- Paste and run this command (update the Visual Studio path if necessary):  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\n\"C:Program Files (x86)Microsoft Visual StudioInstallervs_installer.exe\" modify ^\n--installPath \"C:Program FilesMicrosoft Visual Studio2022Community\" ^\n--add Microsoft.Net.Component.4.8.SDK ^\n--add Microsoft.Net.Component.4.7.2.TargetingPack ^\n--add Microsoft.VisualStudio.Component.Roslyn.Compiler ^\n--add Microsoft.Component.MSBuild ^\n--add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 ^\n--add Microsoft.VisualStudio.Component.VC.Redist.14.Latest ^\n--add Microsoft.VisualStudio.Component.VC.CMake.Project ^\n--add Microsoft.VisualStudio.Component.VC.CLI.Support ^\n--add Microsoft.VisualStudio.Component.VC.Llvm.Clang ^\n--add Microsoft.VisualStudio.ComponentGroup.ClangCL ^\n--add Microsoft.VisualStudio.Component.Windows11SDK.22621 ^\n--add Microsoft.VisualStudio.Component.Windows10SDK.19041 ^\n--add Microsoft.VisualStudio.Component.UniversalCRT.SDK ^\n--add Microsoft.VisualStudio.Component.VC.Redist.MSM\n```  \n3  \n**Install Python and CUDA Toolkit**  \nFollow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).  \nThen install Miniconda (which has Python) here: [https://www.anaconda.com/docs/getting-started/miniconda/install](https://www.anaconda.com/docs/getting-started/miniconda/install#quickstart-install-instructions)  \n4  \n**Install PyTorch**  \nYou will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. [Install PyTorch](https://pytorch.org/get-started/locally/)  \n5  \n**Install Unsloth**  \nOpen Conda command prompt or your terminal with Python and run the command:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install \"unsloth[windows] @ git+https://github.com/unslothai/unsloth.git\"\n```  \nIf you're using GRPO or plan to use vLLM, currently vLLM does not support Windows directly but only via WSL or Linux.",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "38b501c7-3735-4052-bb9b-c19d62fa02b7",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#method-1-windows-directly) Method #1 - Windows directly: > [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#notes) **Notes**\n\nTo run Unsloth directly on Windows:  \n- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)  \n- In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ntrainer = SFTTrainer(\ndataset_num_proc=1,\n...\n)\n```",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "1db27f5c-f7e3-4276-85de-9d1e1713621a",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#method-1-windows-directly) Method #1 - Windows directly: > [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#advanced-troubleshooting) **Advanced/Troubleshooting**\n\nFor **advanced installation instructions** or if you see weird errors during installations:  \n1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`  \n2. Confirm if CUDA is installated correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.  \n3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs.  \n4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful.  \n5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`  \n* * *",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "a8b1b525-45c7-4774-bd42-23fe733b4d3d",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#method-2-windows-using-powershell) Method #2 - Windows using PowerShell: > [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#step-1-install-prerequisites) **Step 1: Install Prerequisites**\n\n1. **Install NVIDIA CUDA Toolkit**:  \n- Download and install the appropriate version of the **NVIDIA CUDA Toolkit** from [CUDA Downloads](https://developer.nvidia.com/cuda-downloads).  \n- Reboot your system after installation if prompted.  \n- **Note**: No additional setup is required after installation for Unsloth.  \n2. **Install Microsoft C++ Build Tools**:  \n- Download and install **Microsoft Build Tools for Visual Studio** from the [official website](https://visualstudio.microsoft.com/visual-cpp-build-tools/).  \n- During installation, select the **C++ build tools** workload.\nEnsure the **MSVC compiler toolset** is included.  \n3. **Set Environment Variables for the C++ Compiler**:  \n- Open the **System Properties** window (search for \"Environment Variables\" in the Start menu).  \n- Click **\"Environment Variables‚Ä¶\"**.  \n- Add or update the following under **System variables**:  \n- **CC**:\nPath to the `cl.exe` C++ compiler.\nExample (adjust if your version differs):  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nC:Program FilesMicrosoft Visual Studio2022BuildToolsVCToolsMSVC14.34.31933binHostx64x64cl.exe\n```  \n- **CXX**:\nSame path as `CC`.  \n- Click **OK** to save changes.  \n- Verify: Open a new terminal and type `cl`. It should show version info.  \n4. **Install Conda**  \n1. Download and install **Miniconda** from the [official website](https://docs.anaconda.com/miniconda/install/#quick-command-line-install)  \n2. Follow installation instruction from the website  \n3. To check whether `conda` is already installed, you can test it with `conda` in your PowerShell",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "f32899be-277f-450d-bc1b-aff12982c488",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#method-2-windows-using-powershell) Method #2 - Windows using PowerShell: > [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#step-2-run-the-unsloth-installation-script) **Step 2: Run the Unsloth Installation Script**\n\n1. **Download the** [**unsloth_windows.ps1**](https://github.com/unslothai/notebooks/blob/main/unsloth_windows.ps1) **PowerShell script by going through this link**.  \n2. **Open PowerShell as Administrator**:  \n- Right-click Start and select **\"Windows PowerShell (Admin)\"**.  \n3. **Navigate to the script‚Äôs location** using `cd`:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\ncd pathtoscriptfolder\n```  \n4. **Run the script**:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npowershell.exe -ExecutionPolicy Bypass -File .unsloth_windows.ps1\n```",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "f105ae79-f336-4cf3-a177-fa6de2ceaa68",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#method-2-windows-using-powershell) Method #2 - Windows using PowerShell: > [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#step-3-using-unsloth) **Step 3: Using Unsloth**\n\nActivate the environment after the installation completes:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nconda activate unsloth_env\n```  \n**Unsloth and its dependencies are now ready!**  \n* * *",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "f404b789-e06b-45ee-abf1-87f78a67f041",
      "source": "firecrawl\\docs\\get-started-installing-+-updating-windows-installation.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation#method-3-windows-via-wsl) Method #3 - Windows via WSL:\n\nWSL is Window's subsystem for Linux.  \n1. Install python though [Python's official site](https://www.python.org/downloads/windows/).  \n2. Start WSL (Should already be preinstalled). Open command prompt as admin then run:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nwsl -d ubuntu\n```  \nOptional: If WSL is not preinstalled, go to the Microsoft store and search \"Ubuntu\" and the app that says Ubuntu will be WSL. Install it and run it and continue from there.  \n1. Update WSL:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nsudo apt update && sudo apt upgrade -y\n```  \n1. Install pip:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\nsudo apt install python3-pip\n```  \n1. Install unsloth:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install unsloth\n```  \n1. Optional: Install Jupyter Notebook to run in a Colab like environment:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip3 install notebook\n```  \n1. Launch Jupyter Notebook:  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\njupyter notebook\n```  \n1. Download any Colab notebook from Unsloth, import it into your Jupyter Notebook, adjust the parameters as needed, and execute the script.  \n[PreviousPip Install](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install) [NextConda Install](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install)  \nLast updated 7 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Windows Installation | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"
      }
    },
    {
      "id": "e3bdd7d3-3f4d-40a5-a455-1aebd210b7be",
      "source": "firecrawl\\docs\\get-started-installing-+-updating.md",
      "content": "---\ntitle: Installing + Updating | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/installing-+-updating\n---  \nUnsloth works on Linux, Windows directly, Kaggle, Google Colab and more. See our [system requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements).  \n**Recommended installation method:**  \nCopy  \n```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]\npip install unsloth\n```  \n[Pip Install](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install)  \n[Windows Installation](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation)  \n[Updating](https://docs.unsloth.ai/get-started/installing-+-updating/updating)  \n[Conda Install](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install)  \n[Google Colab](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab)  \n[PreviousAll Our Models](https://docs.unsloth.ai/get-started/all-our-models) [NextUpdating](https://docs.unsloth.ai/get-started/installing-+-updating/updating)  \nLast updated 10 days ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Installing + Updating | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/installing-+-updating"
      }
    },
    {
      "id": "fcdb77f0-9ec5-44d1-87e5-82484fad9dcd",
      "source": "firecrawl\\docs\\get-started-unsloth-notebooks.md",
      "content": "---\ntitle: Unsloth Notebooks | Unsloth Documentation\nurl: https://docs.unsloth.ai/get-started/unsloth-notebooks\n---  \n‚Ä¢ Google Colab‚Ä¢ Kaggle",
      "metadata": {
        "title": "Unsloth Notebooks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/unsloth-notebooks"
      }
    },
    {
      "id": "e0904de1-0600-4255-b860-fd2eec6d223b",
      "source": "firecrawl\\docs\\get-started-unsloth-notebooks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks#main-notebooks) Main notebooks:\n\n- Gemma 3 (12B) - we're still working on it  \n- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb) - GRPO reasoning  \n- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb) - GRPO reasoning  \n- [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb) - GRPO reasoning  \n- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)  \n- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)  \n- [Llama 3.2 (1B + 3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)  \n- [Mistral v0.3 Instruct (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)  \n- [Gemma 2 (9B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)  \n- [Qwen 2.5 (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)",
      "metadata": {
        "title": "Unsloth Notebooks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/unsloth-notebooks"
      }
    },
    {
      "id": "a24eb808-eb95-4c15-8d23-d38364a80171",
      "source": "firecrawl\\docs\\get-started-unsloth-notebooks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-notebooks) Vision notebooks:\n\n- [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)  \n- [Qwen2-VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)  \n- [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)",
      "metadata": {
        "title": "Unsloth Notebooks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/unsloth-notebooks"
      }
    },
    {
      "id": "6fb89089-a3a1-468a-80f7-bac3f574a6c0",
      "source": "firecrawl\\docs\\get-started-unsloth-notebooks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) Other important notebooks:\n\n- [Ollama](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)  \n- [ORPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb)  \n- [Continued Pretraining](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb)  \n- [DPO Zephyr](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)  \n- [_**Inference only**_](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb)  \n- [Phi-3.5 (mini)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)  \n- [Llama 3 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb)  \n- [Phi-3 (medium)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3_Medium-Conversational.ipynb)  \n- [Mistral NeMo (12B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Nemo_(12B)-Alpaca.ipynb)",
      "metadata": {
        "title": "Unsloth Notebooks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/unsloth-notebooks"
      }
    },
    {
      "id": "4f4f02f9-18ea-4fa5-ab81-662f7a73a186",
      "source": "firecrawl\\docs\\get-started-unsloth-notebooks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks#specific-use-case-notebooks) Specific use-case notebooks:\n\n- [_**Inference chat UI**_](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)  \n- [Text Classification](https://github.com/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb) by Timotheeee  \n- [Multiple Datasets](https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing) by Flail  \n- [KTO](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing) by Jeffrey  \n- [Conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)  \n- [ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)  \n- [Text Completion](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)",
      "metadata": {
        "title": "Unsloth Notebooks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/unsloth-notebooks"
      }
    },
    {
      "id": "4f5e5bad-a73c-4339-8832-1fdefc3f3ad0",
      "source": "firecrawl\\docs\\get-started-unsloth-notebooks.md",
      "content": "[Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks#rest-of-notebooks) Rest of notebooks:\n\n- [Gemm](https://colab.research.google.com/drive/1weTpKOjBZxZJ5PQ-Ql8i6ptAY2x-FWVA?usp=sharing) [a 2 (2B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(2B)-Alpaca.ipynb)  \n- [Qwen 2.5 Coder (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb)  \n- [Mistral Small (22B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb)  \n- [TinyLlama](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/TinyLlama_(1.1B)-Alpaca.ipynb)  \n- [CodeGemma (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeGemma_(7B)-Conversational.ipynb)  \n- [Mistral v0.3 (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Alpaca.ipynb)  \n- [Qwen2 (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_(7B)-Alpaca.ipynb)  \n- [Llama 3.1 (8B)](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Phi_4_(14B)-GRPO.ipynb&accelerator=nvidiaTeslaT4) - GRPO reasoning  \n- [Phi-4 (14B)](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Phi_4_(14B)-GRPO.ipynb&accelerator=nvidiaTeslaT4) - GRPO reasoning  \n- [Qwen2.5 (3B)](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Qwen2.5_(3B)-GRPO.ipynb&accelerator=nvidiaTeslaT4) - GRPO reasoning  \n- [Phi-4 (14B)](https://www.kaggle.com/code/danielhanchen/phi-4-finetuning-unsloth-notebook)  \n- [Llama 3.1 (8B)](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook)  \n- [Llama 3.2 (1B + 3B)](https://www.kaggle.com/code/danielhanchen/fixed-kaggle-llama-3-2-1b-3b-conversation)  \n- [Llama 3.2 Vision](https://www.kaggle.com/code/danielhanchen/llama-3-2-vision-finetuning-unsloth-kaggle)  \n- [Mistral NeMo (12B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-nemo-12b-unsloth-notebook)  \n- [Qwen 2.5 (14B)](https://www.kaggle.com/code/danielhanchen/kaggle-qwen-2-5-conversational-unsloth)  \n- [Gemma 2 (9B)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma2-9b-unsloth-notebook)  \n- [Phi-3 (medium)](https://www.kaggle.com/code/danielhanchen/kaggle-phi-3-medium-unsloth-notebook)  \n- [Qwen2-VL (7B)](https://www.kaggle.com/code/danielhanchen/qwen2-vision-finetuning-unsloth-kaggle)  \n- [Qwen2.5-Coder (14B)](https://www.kaggle.com/code/danielhanchen/kaggle-qwen-2-5-coder-14b-conversational)  \n- [Llama 3 (8B)](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook)  \n- [Mistral v0.3 (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)  \n- [Qwen2 (7B)](https://docs.unsloth.ai/)  \nTo view a complete list of all our Kaggle notebooks, [click here](https://github.com/unslothai/notebooks#-kaggle-notebooks).  \nFeel free to contribute to the notebooks by visiting our [repo](https://github.com/unslothai/notebooks)!  \n[PreviousLoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia) [NextAll Our Models](https://docs.unsloth.ai/get-started/all-our-models)  \nLast updated 1 day ago  \nWas this helpful?  \n* * *",
      "metadata": {
        "title": "Unsloth Notebooks | Unsloth Documentation",
        "url": "https://docs.unsloth.ai/get-started/unsloth-notebooks"
      }
    }
  ]
}